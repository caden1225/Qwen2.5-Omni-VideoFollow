#!/usr/bin/env python3
"""
Qwen2.5-Omni vLLMæ¨¡å‹æœåŠ¡
ä½¿ç”¨vLLMå¯åŠ¨æ¨¡å‹å¹¶æä¾›APIæœåŠ¡ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡º
"""

import os
import time
import logging
import json
import base64
import uuid
import queue
from typing import List, Optional, Dict, Any, Union
from pathlib import Path

import torch
import numpy as np
import librosa
import cv2
from PIL import Image
import soundfile as sf
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import uvicorn
from dotenv import load_dotenv

# å¯¼å…¥qwen-omni-utils
from qwen_omni_utils import process_mm_info
from transformers import Qwen2_5OmniProcessor

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å®šä¹‰è¯·æ±‚å’Œå“åº”æ¨¡å‹
class MultimodalRequest(BaseModel):
    text: Optional[str] = Field(None, description="æ–‡æœ¬è¾“å…¥")
    system_prompt: str = Field("You are a helpful AI assistant.", description="ç³»ç»Ÿæç¤º")
    max_tokens: int = Field(512, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    extract_video_audio: bool = Field(False, description="æ˜¯å¦æå–è§†é¢‘éŸ³è½¨")
    extract_video_frame: bool = Field(False, description="æ˜¯å¦æå–è§†é¢‘æœ€åä¸€å¸§")
    using_mm_info_audio: bool = Field(False, description="æ˜¯å¦ä½¿ç”¨mm_infoæå–éŸ³é¢‘")
    enable_audio_output: bool = Field(False, description="æ˜¯å¦å¯ç”¨è¯­éŸ³è¾“å‡º")
    enable_streaming: bool = Field(False, description="æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º")
    
class MultimodalResponse(BaseModel):
    status: str = Field(..., description="å¤„ç†çŠ¶æ€")
    response_text: str = Field(..., description="ç”Ÿæˆçš„æ–‡æœ¬å›ç­”")
    extracted_audio: Optional[str] = Field(None, description="æå–çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    extracted_frame: Optional[str] = Field(None, description="æå–çš„å›¾åƒæ–‡ä»¶è·¯å¾„")
    generated_audio: Optional[str] = Field(None, description="ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    processing_time: float = Field(..., description="å¤„ç†æ—¶é—´")
    peak_memory: float = Field(..., description="å³°å€¼æ˜¾å­˜ä½¿ç”¨")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")

class ModelService:
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_path = os.getenv("MODEL_PATH", "/home/caden/models/Qwen2.5-Omni-3B")
        self.temp_dir = Path("temp_files")
        self.temp_dir.mkdir(exist_ok=True)
        
        # vLLMç›¸å…³é…ç½®
        self.use_vllm = os.getenv("USE_VLLM", "true").lower() == "true"
        self.vllm_model_path = os.getenv("VLLM_MODEL_PATH", self.model_path)
        self.vllm_tensor_parallel_size = int(os.getenv("VLLM_TENSOR_PARALLEL_SIZE", "1"))
        self.vllm_max_model_len = int(os.getenv("VLLM_MAX_MODEL_LEN", "8192"))
        self.vllm_gpu_memory_utilization = float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.9"))
        
        # å…¼å®¹æ€§å’Œå›é€€æœºåˆ¶æ ‡å¿—
        self.vllm_available = False
        self.fallback_mode = False
        self.compatibility_issues = []
        
    def _print_memory_usage(self, stage: str):
        """æ‰“å°ä¸åŒé˜¶æ®µçš„æ˜¾å­˜å ç”¨ç»Ÿè®¡"""
        if not torch.cuda.is_available():
            return
            
        print(f"\nğŸ” {stage} - æ˜¾å­˜å ç”¨ç»Ÿè®¡:")
        print(f"   å½“å‰æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"   æ˜¾å­˜ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
        print(f"   æ˜¾å­˜å³°å€¼: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
        
        # è·å–GPUä¿¡æ¯
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / 1024**3
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            free = total_memory - cached
            
            print(f"   GPU {i} ({props.name}):")
            print(f"     æ€»æ˜¾å­˜: {total_memory:.2f} GB")
            print(f"     å·²åˆ†é…: {allocated:.2f} GB")
            print(f"     å·²ç¼“å­˜: {cached:.2f} GB")
            print(f"     å¯ç”¨æ˜¾å­˜: {free:.2f} GB")
        print()
        
    def _check_vllm_compatibility(self):
        """æ£€æŸ¥vLLMå…¼å®¹æ€§"""
        try:
            import vllm
            vllm_version = getattr(vllm, '__version__', 'unknown')
            print(f"ğŸ” æ£€æµ‹åˆ°vLLMç‰ˆæœ¬: {vllm_version}")
            
            # æ£€æŸ¥å…³é”®æ¨¡å—
            required_modules = [
                'vllm.engine.omni_llm_engine',
                'vllm.engine.async_llm_engine', 
                'vllm.sampling_params',
                'vllm.inputs',
                'vllm.multimodal.processing_omni'
            ]
            
            missing_modules = []
            for module_name in required_modules:
                try:
                    __import__(module_name)
                except ImportError:
                    missing_modules.append(module_name)
            
            if missing_modules:
                print(f"âš ï¸ ç¼ºå¤±æ¨¡å—: {', '.join(missing_modules)}")
                return False
                
            return True
        except Exception as e:
            print(f"âš ï¸ vLLMå…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _create_optimized_engine_args(self):
        """åˆ›å»ºä¼˜åŒ–çš„å¼•æ“å‚æ•°"""
        try:
            from vllm.engine.async_llm_engine import AsyncEngineArgs
            
            # ä¿å®ˆçš„å‚æ•°é…ç½®ï¼Œä¼˜å…ˆç¨³å®šæ€§
            config = {
                'model': self.vllm_model_path,
                'trust_remote_code': True,
                'enforce_eager': True,  # å¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼é¿å…ç¼–è¯‘é—®é¢˜
                'distributed_executor_backend': 'mp',
                'enable_prefix_caching': False,  # ç¦ç”¨å‰ç¼€ç¼“å­˜é¿å…å…¼å®¹æ€§é—®é¢˜
                'gpu_memory_utilization': min(self.vllm_gpu_memory_utilization, 0.75),  # ä¿å®ˆçš„å†…å­˜ä½¿ç”¨
                'tensor_parallel_size': self.vllm_tensor_parallel_size,
                'max_model_len': min(self.vllm_max_model_len, 4096),  # é™åˆ¶åºåˆ—é•¿åº¦
                'max_num_seqs': min(8, 4),  # å‡å°‘å¹¶å‘æ•°
                'block_size': 16,
            }
            
            # åŠ¨æ€è°ƒæ•´å¤šæ¨¡æ€é™åˆ¶
            gpu_memory_gb = 0
            if torch.cuda.is_available():
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"ğŸ–¥ï¸ GPUæ˜¾å­˜: {gpu_memory_gb:.1f} GB")
            
            # æ ¹æ®æ˜¾å­˜åŠ¨æ€è°ƒæ•´å¤šæ¨¡æ€é™åˆ¶
            if gpu_memory_gb >= 24:  # 24GBä»¥ä¸Šæ˜¾å­˜
                mm_limits = {'audio': 16, 'image': 32, 'video': 8}
            elif gpu_memory_gb >= 16:  # 16-24GBæ˜¾å­˜
                mm_limits = {'audio': 12, 'image': 24, 'video': 6}
            elif gpu_memory_gb >= 8:   # 8-16GBæ˜¾å­˜
                mm_limits = {'audio': 8, 'image': 16, 'video': 4}
            else:  # 8GBä»¥ä¸‹æ˜¾å­˜
                mm_limits = {'audio': 4, 'image': 8, 'video': 2}
                config['max_model_len'] = min(config['max_model_len'], 2048)
                config['gpu_memory_utilization'] = min(config['gpu_memory_utilization'], 0.6)
            
            config['limit_mm_per_prompt'] = mm_limits
            print(f"ğŸ›ï¸ å¤šæ¨¡æ€é™åˆ¶: {mm_limits}")
            
            return AsyncEngineArgs(**config)
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå¼•æ“å‚æ•°å¤±è´¥: {e}")
            # è¿”å›æœ€åŸºç¡€çš„é…ç½®
            try:
                from vllm.engine.async_llm_engine import AsyncEngineArgs
                basic_config = {
                    'model': self.vllm_model_path,
                    'trust_remote_code': True,
                    'enforce_eager': True,
                    'gpu_memory_utilization': 0.6,
                    'tensor_parallel_size': 1,
                }
                print("ğŸ”„ ä½¿ç”¨åŸºç¡€é…ç½®é‡è¯•")
                return AsyncEngineArgs(**basic_config)
            except Exception as e2:
                print(f"âŒ åŸºç¡€é…ç½®ä¹Ÿå¤±è´¥: {e2}")
                return None

    def _check_model_files(self, model_path: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
        try:
            model_path = Path(model_path)
            if not model_path.exists():
                print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                return False
            
            # æ£€æŸ¥å…³é”®æ–‡ä»¶
            required_files = [
                'config.json',
                'tokenizer.json',
                'tokenizer_config.json'
            ]
            
            missing_files = []
            for file_name in required_files:
                if not (model_path / file_name).exists():
                    missing_files.append(file_name)
            
            if missing_files:
                print(f"âš ï¸ ç¼ºå¤±å…³é”®æ–‡ä»¶: {', '.join(missing_files)}")
                self.compatibility_issues.append(f"ç¼ºå¤±æ–‡ä»¶: {', '.join(missing_files)}")
                return False
            
            print("âœ… æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}")
            self.compatibility_issues.append(f"æ–‡ä»¶æ£€æŸ¥é”™è¯¯: {e}")
            return False

    def _try_fallback_mode(self):
        """å°è¯•å›é€€æ¨¡å¼ï¼ˆä½¿ç”¨æ ‡å‡†transformersï¼‰"""
        try:
            print("ğŸ”„ å¯åŠ¨å›é€€æ¨¡å¼ï¼Œä½¿ç”¨æ ‡å‡†transformersåŠ è½½...")
            from transformers import AutoModel, AutoTokenizer, AutoProcessor
            
            # åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            
            # å°è¯•åŠ è½½å¤„ç†å™¨
            try:
                self.processor = AutoProcessor.from_pretrained(
                    self.model_path,
                    trust_remote_code=True
                )
                print("âœ… å›é€€æ¨¡å¼å¤„ç†å™¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å›é€€æ¨¡å¼å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
                # ä½¿ç”¨åŸºç¡€çš„Qwen2å¤„ç†å™¨
                try:
                    from transformers import Qwen2Processor
                    self.processor = Qwen2Processor.from_pretrained(self.model_path)
                    print("âœ… ä½¿ç”¨åŸºç¡€Qwen2å¤„ç†å™¨")
                except Exception as e2:
                    print(f"âŒ åŸºç¡€å¤„ç†å™¨ä¹Ÿå¤±è´¥: {e2}")
                    return False
            
            self.fallback_mode = True
            print("âœ… å›é€€æ¨¡å¼å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ å›é€€æ¨¡å¼å¤±è´¥: {e}")
            self.compatibility_issues.append(f"å›é€€æ¨¡å¼é”™è¯¯: {e}")
            return False

    def _provide_compatibility_report(self):
        """æä¾›å…¼å®¹æ€§æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ” å…¼å®¹æ€§è¯Šæ–­æŠ¥å‘Š")
        print("="*60)
        
        diagnostic = self._diagnose_system_state()
        for key, value in diagnostic.items():
            print(f"  {key}: {value}")
        
        if self.compatibility_issues:
            print("\nâš ï¸ å‘ç°çš„é—®é¢˜:")
            for i, issue in enumerate(self.compatibility_issues, 1):
                print(f"  {i}. {issue}")
        
        print(f"\nğŸ¯ å½“å‰çŠ¶æ€:")
        print(f"  - vLLMå¯ç”¨: {self.vllm_available}")
        print(f"  - å›é€€æ¨¡å¼: {self.fallback_mode}")
        print(f"  - æ¨¡å‹å·²åŠ è½½: {hasattr(self, 'vllm_model') or self.fallback_mode}")
        
        if not self.vllm_available and not self.fallback_mode:
            print("\nğŸ’¡ å»ºè®®:")
            print("  1. æ£€æŸ¥vLLMå®‰è£…: pip install vllm")
            print("  2. æ›´æ–°transformers: pip install -U transformers")
            print("  3. æ£€æŸ¥CUDAç‰ˆæœ¬å…¼å®¹æ€§")
            print("  4. ç¡®è®¤æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§")
            print("  5. å°è¯•é™ä½GPUå†…å­˜ä½¿ç”¨ç‡")
        
        print("="*60)

    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
            if not self._check_model_files(self.vllm_model_path):
                print("âŒ æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å¤±è´¥")
                self._provide_compatibility_report()
                return False
            
            if self.use_vllm:
                print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨vLLMåŠ è½½æ¨¡å‹: {self.vllm_model_path}")
                
                # è®¾ç½®è§†é¢‘å¤„ç†ç›¸å…³çš„ç¯å¢ƒå˜é‡
                os.environ.setdefault('VIDEO_MAX_PIXELS', str(32000 * 28 * 28))
                print(f"ğŸ”§ è®¾ç½®VIDEO_MAX_PIXELS: {os.environ.get('VIDEO_MAX_PIXELS')}")
                
                # åº”ç”¨ä¿®å¤è¡¥ä¸
                try:
                    from vllm_fix_patch import apply_all_patches
                    if apply_all_patches():
                        print("âœ… vLLMä¿®å¤è¡¥ä¸åº”ç”¨æˆåŠŸ")
                    else:
                        print("âš ï¸ vLLMä¿®å¤è¡¥ä¸åº”ç”¨éƒ¨åˆ†å¤±è´¥ï¼Œç»§ç»­å°è¯•åŠ è½½")
                except ImportError:
                    print("âš ï¸ ä¿®å¤è¡¥ä¸æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œç»§ç»­å°è¯•åŠ è½½")
                except Exception as e:
                    print(f"âš ï¸ ä¿®å¤è¡¥ä¸åº”ç”¨å¤±è´¥: {e}ï¼Œç»§ç»­å°è¯•åŠ è½½")
                
                # å¯¼å…¥vLLMç›¸å…³æ¨¡å—
                try:
                    from vllm.engine.omni_llm_engine import OmniLLMEngine
                    from vllm.engine.async_llm_engine import AsyncEngineArgs
                    from vllm.sampling_params import SamplingParams
                    from vllm.inputs import TextPrompt
                    from vllm.multimodal.processing_omni import fetch_image, fetch_video
                except ImportError as e:
                    print(f"âŒ vLLMå¯¼å…¥å¤±è´¥: {e}")
                    print("è¯·æ£€æŸ¥vLLMå®‰è£…: pip install vllm")
                    return False
                
                # åˆ›å»ºä¼˜åŒ–çš„å¼•æ“å‚æ•°
                print("ğŸ”§ åˆ›å»ºä¼˜åŒ–çš„å¼•æ“é…ç½®...")
                thinker_engine_args = self._create_optimized_engine_args()
                if thinker_engine_args is None:
                    print("âŒ æ— æ³•åˆ›å»ºå¼•æ“å‚æ•°")
                    return False
                
                # å°è¯•åˆå§‹åŒ–OmniLLMEngineï¼Œä½¿ç”¨å¤šç§å›é€€ç­–ç•¥
                print("ğŸš€ åˆå§‹åŒ–OmniLLMEngine...")
                try:
                    self.vllm_model = OmniLLMEngine(
                        thinker_engine_args,
                        thinker_visible_devices=[0],
                    )
                    print("âœ… OmniLLMEngineåˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"âŒ OmniLLMEngineåˆå§‹åŒ–å¤±è´¥: {e}")
                    print("ğŸ”„ å°è¯•ä½¿ç”¨ç®€åŒ–é…ç½®...")
                    
                    # å°è¯•ç®€åŒ–é…ç½®
                    try:
                        simple_args = AsyncEngineArgs(
                            model=self.vllm_model_path,
                            trust_remote_code=True,
                            enforce_eager=True,
                            gpu_memory_utilization=0.5,
                            tensor_parallel_size=1,
                        )
                        self.vllm_model = OmniLLMEngine(
                            simple_args,
                            thinker_visible_devices=[0],
                        )
                        print("âœ… ä½¿ç”¨ç®€åŒ–é…ç½®åˆå§‹åŒ–æˆåŠŸ")
                    except Exception as e2:
                        print(f"âŒ ç®€åŒ–é…ç½®ä¹Ÿå¤±è´¥: {e2}")
                
                print("ğŸ“¦ vLLMæ¨¡å‹åŠ è½½å®Œæˆ")
                self.vllm_available = True
                
                # ç»Ÿè®¡æ˜¾å­˜å ç”¨
                if torch.cuda.is_available():
                    self._print_memory_usage("vLLMæ¨¡å‹åŠ è½½å®Œæˆå")
                
                # åŠ è½½å¤„ç†å™¨ï¼ˆç”¨äºå¤šæ¨¡æ€è¾“å…¥å¤„ç†ï¼‰
                try:
                    self.processor = Qwen2_5OmniProcessor.from_pretrained(self.vllm_model_path)
                    print("âœ… å¤„ç†å™¨åŠ è½½å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}ï¼Œä½†æ¨¡å‹å¼•æ“å·²åŠ è½½")
                    # å°è¯•ä»å¤‡ç”¨è·¯å¾„åŠ è½½
                    try:
                        from transformers import AutoProcessor
                        self.processor = AutoProcessor.from_pretrained(self.vllm_model_path)
                        print("âœ… ä½¿ç”¨å¤‡ç”¨æ–¹å¼åŠ è½½å¤„ç†å™¨æˆåŠŸ")
                    except Exception as e2:
                        print(f"âŒ å¤‡ç”¨å¤„ç†å™¨åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                        print("ğŸ”„ å¤„ç†å™¨åŠ è½½å¤±è´¥ï¼Œä½†vLLMå¼•æ“å¯ç”¨")
                        # å¦‚æœvLLMå¼•æ“æˆåŠŸä½†å¤„ç†å™¨å¤±è´¥ï¼Œä»ç„¶å¯ä»¥ä½¿ç”¨åŸºæœ¬åŠŸèƒ½
                
                return True
            else:
                print(f"âš ï¸ æœªå¯ç”¨vLLMï¼Œå°è¯•å›é€€æ¨¡å¼")
                
        except Exception as e:
            error_msg = self._handle_vllm_error(e, "æ¨¡å‹åŠ è½½")
            print(error_msg)
            import traceback
            traceback.print_exc()
            
            # æœ€åçš„å›é€€å°è¯•
            print("ğŸ”„ æœ€åå°è¯•å›é€€æ¨¡å¼...")
            if self._try_fallback_mode():
                return True
            
            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œæä¾›å®Œæ•´çš„è¯Šæ–­æŠ¥å‘Š
            self._provide_compatibility_report()
            return False

    def extract_video_features(self, video_path: str, extract_audio: bool = False, extract_frame: bool = False):
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘å’Œæœ€åä¸€å¸§"""
        features = {}
        
        if extract_audio:
            try:
                audio, sr = librosa.load(video_path, sr=16000)
                features['audio'] = audio
                print(f"æå–éŸ³é¢‘æˆåŠŸ: {len(audio)} samples at {sr}Hz")
            except Exception as e:
                print(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
        
        if extract_frame:
            try:
                cap = cv2.VideoCapture(video_path)
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count - 1)
                ret, frame = cap.read()
                
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    image = Image.fromarray(frame_rgb)
                    features['last_frame'] = image
                    print("æå–è§†é¢‘æœ€åä¸€å¸§æˆåŠŸ")
                else:
                    print("æå–å¸§å¤±è´¥")
                cap.release()
            except Exception as e:
                print(f"å¸§æå–å¤±è´¥: {e}")
        
        return features

    def save_temp_file(self, data: Union[np.ndarray, Image.Image, str], file_type: str, suffix: str = "") -> str:
        """ä¿å­˜ä¸´æ—¶æ–‡ä»¶å¹¶è¿”å›è·¯å¾„"""
        timestamp = int(time.time())
        filename = f"{file_type}_{timestamp}{suffix}"
        filepath = self.temp_dir / filename
        
        try:
            if file_type == "audio" and isinstance(data, np.ndarray):
                sf.write(str(filepath), data, 16000)
            elif file_type == "image" and isinstance(data, Image.Image):
                data.save(str(filepath))
            elif file_type == "video" and isinstance(data, str):
                # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œå¤åˆ¶åˆ°ä¸´æ—¶ç›®å½•
                import shutil
                shutil.copy2(data, str(filepath))
            else:
                print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {type(data)}")
                return ""
                
            print(f"âœ… ä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆåŠŸ: {filepath}")
            return str(filepath)
        except Exception as e:
            print(f"âŒ ä¸´æ—¶æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            return ""

    def _diagnose_system_state(self):
        """è¯Šæ–­ç³»ç»ŸçŠ¶æ€"""
        diagnostic_info = {
            'cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'model_loaded': hasattr(self, 'vllm_model'),
            'processor_loaded': hasattr(self, 'processor'),
        }
        
        if torch.cuda.is_available():
            try:
                for i in range(torch.cuda.device_count()):
                    props = torch.cuda.get_device_properties(i)
                    diagnostic_info[f'gpu_{i}_name'] = props.name
                    diagnostic_info[f'gpu_{i}_memory'] = f"{props.total_memory / 1024**3:.1f}GB"
                    diagnostic_info[f'gpu_{i}_allocated'] = f"{torch.cuda.memory_allocated(i) / 1024**3:.1f}GB"
            except Exception as e:
                diagnostic_info['gpu_error'] = str(e)
        
        return diagnostic_info

    def _handle_vllm_error(self, error: Exception, context: str) -> str:
        """å¤„ç†vLLMé”™è¯¯å¹¶æä¾›è¯Šæ–­ä¿¡æ¯"""
        error_str = str(error).lower()
        diagnostic = self._diagnose_system_state()
        
        # å¸¸è§é”™è¯¯çš„å¤„ç†å»ºè®®
        if 'out of memory' in error_str or 'cuda out of memory' in error_str:
            suggestion = (
                "ğŸ’¾ æ˜¾å­˜ä¸è¶³é”™è¯¯:\n"
                f"  - å½“å‰æ˜¾å­˜ä½¿ç”¨: {diagnostic.get('gpu_0_allocated', 'unknown')}\n"
                "  - å»ºè®®é™ä½gpu_memory_utilizationå‚æ•°\n"
                "  - æˆ–å‡å°‘max_model_lenå’Œmax_num_seqså‚æ•°"
            )
        elif 'import' in error_str or 'module' in error_str:
            suggestion = (
                "ğŸ“¦ æ¨¡å—å¯¼å…¥é”™è¯¯:\n"
                "  - æ£€æŸ¥vLLMæ˜¯å¦æ­£ç¡®å®‰è£…: pip install vllm\n"
                "  - ç¡®è®¤vLLMç‰ˆæœ¬æ”¯æŒQwen2.5-Omni\n"
                "  - æ£€æŸ¥Pythonç¯å¢ƒå’Œä¾èµ–"
            )
        elif 'omnillmengine' in error_str:
            suggestion = (
                "ğŸ¤– OmniLLMEngineé”™è¯¯:\n"
                "  - æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®\n"
                "  - ç¡®è®¤vLLMç‰ˆæœ¬æ”¯æŒOmniLLMEngine\n"
                "  - å°è¯•ä½¿ç”¨enforce_eager=True"
            )
        elif 'tokenizer' in error_str or 'processor' in error_str:
            suggestion = (
                "ğŸ”¤ å¤„ç†å™¨é”™è¯¯:\n"
                "  - æ£€æŸ¥æ¨¡å‹è·¯å¾„ä¸‹æ˜¯å¦æœ‰å®Œæ•´çš„tokenizeræ–‡ä»¶\n"
                "  - ç¡®è®¤trust_remote_code=True\n"
                "  - å°è¯•é‡æ–°ä¸‹è½½æ¨¡å‹æ–‡ä»¶"
            )
        else:
            suggestion = (
                "âš ï¸ é€šç”¨é”™è¯¯:\n"
                "  - æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæƒé™\n"
                "  - ç¡®è®¤æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…\n"
                "  - å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„é…ç½®å‚æ•°"
            )
        
        diagnostic_str = "\n".join([f"  {k}: {v}" for k, v in diagnostic.items()])
        
        return f"""
âŒ {context}å¤±è´¥:
é”™è¯¯ä¿¡æ¯: {error}

{suggestion}

ğŸ” ç³»ç»Ÿè¯Šæ–­:
{diagnostic_str}
"""

    def process_multimodal(self, 
                          text_input: Optional[str],
                          image_input: Optional[Image.Image],
                          audio_input: Optional[str],
                          video_input: Optional[str],
                          system_prompt: str,
                          max_tokens: int,
                          extract_video_audio: bool,
                          extract_video_frame: bool,
                          using_mm_info_audio: bool,
                          enable_audio_output: bool = False):
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        
        if not hasattr(self, 'vllm_model'):
            diagnostic = self._diagnose_system_state()
            error_msg = f"æ¨¡å‹æœªåŠ è½½ - è¯Šæ–­ä¿¡æ¯: {diagnostic}"
            return MultimodalResponse(
                status="âŒ æ¨¡å‹æœªåŠ è½½",
                response_text="",
                processing_time=0,
                peak_memory=0,
                error=error_msg
            )
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        generated_audio = None
        
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            ]
            
            user_content = []
            
            # æ·»åŠ æ–‡æœ¬
            if text_input and text_input.strip():
                user_content.append({"type": "text", "text": text_input.strip()})
            
            # å¤„ç†è§†é¢‘
            if video_input:
                if extract_video_audio or extract_video_frame:
                    print(f"ğŸ¬ å¼€å§‹æå–è§†é¢‘ç‰¹å¾...")
                    features = self.extract_video_features(
                        video_input, 
                        extract_audio=extract_video_audio, 
                        extract_frame=extract_video_frame
                    )
                    
                    if 'audio' in features:
                        # ä¿å­˜æå–çš„éŸ³é¢‘
                        temp_audio_path = self.save_temp_file(features['audio'], "audio", ".wav")
                        if temp_audio_path:
                            user_content.append({"type": "audio", "audio": features['audio']})
                            extracted_audio = temp_audio_path
                            print(f"âœ… éŸ³é¢‘å·²æå–å¹¶ä¿å­˜: {temp_audio_path}")
                    
                    if 'last_frame' in features:
                        # ä¿å­˜æå–çš„å›¾åƒ
                        temp_image_path = self.save_temp_file(features['last_frame'], "image", ".png")
                        if temp_image_path:
                            user_content.append({"type": "image", "image": features['last_frame']})
                            extracted_frame = temp_image_path
                            print(f"âœ… å›¾åƒå·²æå–å¹¶ä¿å­˜: {temp_image_path}")
                else:
                    user_content.append({"type": "video", "video": video_input, "using_mm_info_audio": using_mm_info_audio})
            
            # å¤„ç†å›¾åƒ
            if image_input:
                user_content.append({"type": "image", "image": image_input})
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    user_content.append({"type": "audio", "audio": audio_data})
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œä½¿ç”¨é»˜è®¤
            if not user_content:
                user_content.append({"type": "text", "text": "Hello"})
            
            messages.append({"role": "user", "content": user_content})
            
            print(f"ğŸ“ æ„å»ºçš„æ¶ˆæ¯åŒ…å« {len(user_content)} ä¸ªå†…å®¹é¡¹")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            print(f"ğŸ“„ ç”Ÿæˆçš„prompté•¿åº¦: {len(text_prompt)} å­—ç¬¦")
            
            # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ - ä½¿ç”¨vLLMçš„fetchå‡½æ•°
            audios, images, videos = [], [], []
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    audios.append(audio_data)
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # å¤„ç†å›¾åƒ
            if image_input:
                try:
                    # ä½¿ç”¨vLLMçš„fetch_image
                    from vllm.multimodal.processing_omni import fetch_image
                    image_data = fetch_image({'image': image_input})
                    images.append(image_data)
                except Exception as e:
                    print(f"å›¾åƒå¤„ç†å¤±è´¥: {e}")
            
            # å¤„ç†è§†é¢‘
            if video_input:
                try:
                    # ä½¿ç”¨vLLMçš„fetch_video
                    from vllm.multimodal.processing_omni import fetch_video
                    video_data = fetch_video({'video': video_input})
                    videos.append(video_data)
                except Exception as e:
                    print(f"è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # å¤„ç†ä»è§†é¢‘æå–çš„éŸ³é¢‘å’Œå›¾åƒ
            if extracted_audio:
                try:
                    audio_data, _ = librosa.load(extracted_audio, sr=16000)
                    audios.append(audio_data)
                except Exception as e:
                    print(f"æå–éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            if extracted_frame:
                try:
                    from vllm.multimodal.processing_omni import fetch_image
                    image_data = fetch_image({'image': extracted_frame})
                    images.append(image_data)
                except Exception as e:
                    print(f"æå–å›¾åƒå¤„ç†å¤±è´¥: {e}")
            
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†ç»“æœ: audios={len(audios)}, images={len(images)}, videos={len(videos)}")
            
            # ä½¿ç”¨OmniLLMEngineç”Ÿæˆ
            print("ğŸš€ å¼€å§‹ä½¿ç”¨OmniLLMEngineç”Ÿæˆå›ç­”...")
            
            # è®¾ç½®é‡‡æ ·å‚æ•°
            sampling_params = SamplingParams(
                temperature=0.0,
                top_k=-1,
                top_p=1.0,
                repetition_penalty=1.1,
                max_tokens=max_tokens,
                detokenize=True,
                seed=0
            )
            
            # æ„å»ºTextPromptè¾“å…¥
            from vllm.inputs import TextPrompt
            multi_modal_data = {}
            if audios:
                multi_modal_data["audio"] = audios
            if images:
                multi_modal_data["image"] = images
            if videos:
                multi_modal_data["video"] = videos
            multi_modal_data["use_audio_in_video"] = using_mm_info_audio
            
            prompt = TextPrompt(
                prompt=text_prompt,
                multi_modal_data=multi_modal_data,
            )
            
            # ç”Ÿæˆå›ç­”
            request_id = str(uuid.uuid4())
            output_queue = self.vllm_model.add_request(
                request_id,
                prompt,
                sampling_params
            )
            
            # è·å–è¾“å‡º
            response_text = ""
            try:
                while True:
                    output = output_queue.get(timeout=30)  # 30ç§’è¶…æ—¶
                    if output is None:
                        break
                    
                    if hasattr(output, 'outputs') and len(output.outputs) > 0:
                        if output.outputs[0].text:
                            response_text = output.outputs[0].text
                            print(f"ğŸ“¤ OmniLLMEngineç”Ÿæˆè¾“å‡º: {response_text[:100]}...")
                        else:
                            response_text = "OmniLLMEngineç”Ÿæˆå¤±è´¥"
                            print("âŒ OmniLLMEngineç”Ÿæˆå¤±è´¥")
                        break
            except queue.Empty:
                print("âš ï¸ è¾“å‡ºè·å–è¶…æ—¶")
                response_text = "OmniLLMEngineç”Ÿæˆè¶…æ—¶"
            
            # å¤„ç†éŸ³é¢‘è¾“å‡ºï¼ˆvLLMæ¨¡å¼ä¸‹æš‚ä¸æ”¯æŒéŸ³é¢‘ç”Ÿæˆï¼‰
            if enable_audio_output:
                print("âš ï¸ vLLMæ¨¡å¼ä¸‹éŸ³é¢‘è¾“å‡ºæš‚ä¸æ”¯æŒ")
                generated_audio = None
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
            print(f"âœ… å¤„ç†å®Œæˆï¼Œæ—¶é—´: {processing_time:.2f}ç§’ï¼Œå³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB")
            
            return MultimodalResponse(
                status=f"âœ… å¤„ç†å®Œæˆ - vLLMæ¨¡å¼",
                response_text=response_text,
                extracted_audio=extracted_audio,
                extracted_frame=extracted_frame,
                generated_audio=generated_audio,
                processing_time=processing_time,
                peak_memory=peak_memory
            )
            
        except Exception as e:
            error_msg = self._handle_vllm_error(e, "å¤šæ¨¡æ€å¤„ç†")
            print(error_msg)
            import traceback
            traceback.print_exc()
            
            return MultimodalResponse(
                status="âŒ å¤„ç†å¤±è´¥",
                response_text="",
                processing_time=time.time() - start_time,
                peak_memory=torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0,
                error=error_msg
            )

    def process_multimodal_streaming(self, 
                                   text_input: Optional[str],
                                   image_input: Optional[Image.Image],
                                   audio_input: Optional[str],
                                   video_input: Optional[str],
                                   system_prompt: str,
                                   max_tokens: int,
                                   extract_video_audio: bool,
                                   extract_video_frame: bool,
                                   using_mm_info_audio: bool,
                                   enable_audio_output: bool = False):
        """æµå¼å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        
        if not hasattr(self, 'vllm_model'):
            yield json.dumps({
                "status": "âŒ æ¨¡å‹æœªåŠ è½½",
                "response_text": "",
                "error": "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡"
            })
            return
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        
        try:
            # å‰æœŸå¤„ç†
            yield json.dumps({"status": "ğŸ”„ å¼€å§‹å¤„ç†...", "response_text": ""})
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            ]
            
            user_content = []
            
            # æ·»åŠ æ–‡æœ¬
            if text_input and text_input.strip():
                user_content.append({"type": "text", "text": text_input.strip()})
            
            # å¤„ç†è§†é¢‘
            if video_input:
                if extract_video_audio or extract_video_frame:
                    yield json.dumps({"status": "ğŸ¬ æå–è§†é¢‘ç‰¹å¾...", "response_text": ""})
                    
                    features = self.extract_video_features(
                        video_input, 
                        extract_audio=extract_video_audio, 
                        extract_frame=extract_video_frame
                    )
                    
                    if 'audio' in features:
                        temp_audio_path = self.save_temp_file(features['audio'], "audio", ".wav")
                        if temp_audio_path:
                            user_content.append({"type": "audio", "audio": features['audio']})
                            extracted_audio = temp_audio_path
                    
                    if 'last_frame' in features:
                        temp_image_path = self.save_temp_file(features['last_frame'], "image", ".png")
                        if temp_image_path:
                            user_content.append({"type": "image", "image": features['last_frame']})
                            extracted_frame = temp_image_path
                        
                    yield json.dumps({"status": "âœ… è§†é¢‘ç‰¹å¾æå–å®Œæˆ", "response_text": ""})
                else:
                    user_content.append({"type": "video", "video": video_input})
            
            # å¤„ç†å›¾åƒ
            if image_input:
                user_content.append({"type": "image", "image": image_input})
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    user_content.append({"type": "audio", "audio": audio_data})
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            if not user_content:
                user_content.append({"type": "text", "text": "Hello"})
            
            messages.append({"role": "user", "content": user_content})
            
            yield json.dumps({"status": "ğŸ“ æ„å»ºå¤šæ¨¡æ€è¾“å…¥...", "response_text": ""})
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            
            # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ - ä½¿ç”¨vLLMçš„fetchå‡½æ•°
            audios, images, videos = [], [], []
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    audios.append(audio_data)
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # å¤„ç†å›¾åƒ
            if image_input:
                try:
                    # ä½¿ç”¨vLLMçš„fetch_image
                    from vllm.multimodal.processing_omni import fetch_image
                    image_data = fetch_image({'image': image_input})
                    images.append(image_data)
                except Exception as e:
                    print(f"å›¾åƒå¤„ç†å¤±è´¥: {e}")
            
            # å¤„ç†è§†é¢‘
            if video_input:
                try:
                    # ä½¿ç”¨vLLMçš„fetch_video
                    from vllm.multimodal.processing_omni import fetch_video
                    video_data = fetch_video({'video': video_input})
                    videos.append(video_data)
                except Exception as e:
                    print(f"è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # å¤„ç†ä»è§†é¢‘æå–çš„éŸ³é¢‘å’Œå›¾åƒ
            if extracted_audio:
                try:
                    audio_data, _ = librosa.load(extracted_audio, sr=16000)
                    audios.append(audio_data)
                except Exception as e:
                    print(f"æå–éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            if extracted_frame:
                try:
                    from vllm.multimodal.processing_omni import fetch_image
                    image_data = fetch_image({'image': extracted_frame})
                    images.append(image_data)
                except Exception as e:
                    print(f"æå–å›¾åƒå¤„ç†å¤±è´¥: {e}")
            
            yield json.dumps({"status": "ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ...", "response_text": ""})
            
            # ä½¿ç”¨OmniLLMEngineæµå¼ç”Ÿæˆ
            sampling_params = SamplingParams(
                temperature=0.0,
                top_k=-1,
                top_p=1.0,
                repetition_penalty=1.1,
                max_tokens=max_tokens,
                detokenize=True,
                seed=0
            )
            
            # æ„å»ºTextPromptè¾“å…¥
            from vllm.inputs import TextPrompt
            multi_modal_data = {}
            if audios:
                multi_modal_data["audio"] = audios
            if images:
                multi_modal_data["image"] = images
            if videos:
                multi_modal_data["video"] = videos
            multi_modal_data["use_audio_in_video"] = using_mm_info_audio
            
            prompt = TextPrompt(
                prompt=text_prompt,
                multi_modal_data=multi_modal_data,
            )
            
            # æµå¼ç”Ÿæˆ
            response_text = ""
            request_id = str(uuid.uuid4())
            output_queue = self.vllm_model.add_request(
                request_id,
                prompt,
                sampling_params
            )
            
            # æµå¼è·å–è¾“å‡º
            while True:
                try:
                    output = output_queue.get(timeout=30)  # 30ç§’è¶…æ—¶
                    if output is None:
                        break
                    
                    if hasattr(output, 'outputs') and len(output.outputs) > 0:
                        if output.outputs[0].text:
                            new_text = output.outputs[0].text
                            if new_text.strip():
                                response_text += new_text
                                processing_time = time.time() - start_time
                                status = f"ğŸ“¡ æµå¼ç”Ÿæˆä¸­... ({processing_time:.1f}s)"
                                yield json.dumps({
                                    "status": status,
                                    "response_text": response_text,
                                    "extracted_audio": extracted_audio,
                                    "extracted_frame": extracted_frame
                                })
                        
                        if output.finished:
                            break
                except queue.Empty:
                    print("âš ï¸ æµå¼è¾“å‡ºè¶…æ—¶")
                    break
            
            # æœ€ç»ˆç»“æœ
            processing_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
            final_status = f"""âœ… æµå¼ç”Ÿæˆå®Œæˆ!
â±ï¸ æ€»æ—¶é—´: {processing_time:.2f}ç§’
ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB
ğŸ“ è¾“å‡ºé•¿åº¦: {len(response_text)} å­—ç¬¦"""
            
            yield json.dumps({
                "status": final_status,
                "response_text": response_text,
                "extracted_audio": extracted_audio,
                "extracted_frame": extracted_frame,
                "processing_time": processing_time,
                "peak_memory": peak_memory
            })
            
        except Exception as e:
            error_msg = self._handle_vllm_error(e, "æµå¼å¤šæ¨¡æ€å¤„ç†")
            print(error_msg)
            yield json.dumps({
                "status": "âŒ å¤„ç†å¤±è´¥",
                "response_text": "",
                "error": error_msg
            })

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Qwen2.5-Omni vLLM APIæœåŠ¡",
    description="åŸºäºvLLMçš„Qwen2.5-Omniå¤šæ¨¡æ€æ¨¡å‹APIæœåŠ¡",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆ›å»ºæ¨¡å‹æœåŠ¡å®ä¾‹
model_service = ModelService()

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    print("ğŸš€ æ­£åœ¨å¯åŠ¨Qwen2.5-Omni vLLM APIæœåŠ¡...")
    success = model_service.load_model()
    if success:
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒæœåŠ¡å¯åŠ¨å®Œæˆ")
    else:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼ŒæœåŠ¡å¯åŠ¨å¤±è´¥")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Qwen2.5-Omni vLLM APIæœåŠ¡",
        "status": "running",
        "model_loaded": hasattr(model_service, 'vllm_model')
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": hasattr(model_service, 'vllm_model'),
        "gpu_available": torch.cuda.is_available(),
        "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
    }

@app.post("/process", response_model=MultimodalResponse)
async def process_multimodal(
    text_input: Optional[str] = Form(None),
    system_prompt: str = Form("You are a helpful AI assistant."),
    max_tokens: int = Form(512),
    extract_video_audio: bool = Form(False),
    extract_video_frame: bool = Form(False),
    using_mm_info_audio: bool = Form(False),
    enable_audio_output: bool = Form(False),
    image_input: Optional[UploadFile] = File(None),
    audio_input: Optional[UploadFile] = File(None),
    video_input: Optional[UploadFile] = File(None)
):
    """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
    
    # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
    image_data = None
    audio_data = None
    video_data = None
    
    if image_input:
        try:
            image_data = Image.open(image_input.file)
            print(f"âœ… å›¾åƒä¸Šä¼ æˆåŠŸ: {image_input.filename}")
        except Exception as e:
            print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
    
    if audio_input:
        try:
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            temp_audio_path = model_service.temp_dir / f"upload_audio_{int(time.time())}.wav"
            with open(temp_audio_path, "wb") as f:
                f.write(audio_input.file.read())
            audio_data = str(temp_audio_path)
            print(f"âœ… éŸ³é¢‘ä¸Šä¼ æˆåŠŸ: {audio_input.filename}")
        except Exception as e:
            print(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
    
    if video_input:
        try:
            # ä¿å­˜è§†é¢‘æ–‡ä»¶
            temp_video_path = model_service.temp_dir / f"upload_video_{int(time.time())}.mp4"
            with open(temp_video_path, "wb") as f:
                f.write(video_input.file.read())
            video_data = str(temp_video_path)
            print(f"âœ… è§†é¢‘ä¸Šä¼ æˆåŠŸ: {video_input.filename}")
        except Exception as e:
            print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
    
    # è°ƒç”¨æ¨¡å‹æœåŠ¡
    result = model_service.process_multimodal(
        text_input=text_input,
        image_input=image_data,
        audio_input=audio_data,
        video_input=video_data,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        extract_video_audio=extract_video_audio,
        extract_video_frame=extract_video_frame,
        using_mm_info_audio=using_mm_info_audio,
        enable_audio_output=enable_audio_output
    )
    
    return result

@app.post("/process_streaming")
async def process_multimodal_streaming(
    text_input: Optional[str] = Form(None),
    system_prompt: str = Form("You are a helpful AI assistant."),
    max_tokens: int = Form(512),
    extract_video_audio: bool = Form(False),
    extract_video_frame: bool = Form(False),
    using_mm_info_audio: bool = Form(False),
    enable_audio_output: bool = Form(False),
    image_input: Optional[UploadFile] = File(None),
    audio_input: Optional[UploadFile] = File(None),
    video_input: Optional[UploadFile] = File(None)
):
    """æµå¼å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
    
    # å¤„ç†æ–‡ä»¶ä¸Šä¼ ï¼ˆä¸processå‡½æ•°ç›¸åŒï¼‰
    image_data = None
    audio_data = None
    video_data = None
    
    if image_input:
        try:
            image_data = Image.open(image_input.file)
        except Exception as e:
            print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
    
    if audio_input:
        try:
            temp_audio_path = model_service.temp_dir / f"upload_audio_{int(time.time())}.wav"
            with open(temp_audio_path, "wb") as f:
                f.write(audio_input.file.read())
            audio_data = str(temp_audio_path)
        except Exception as e:
            print(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
    
    if video_input:
        try:
            temp_video_path = model_service.temp_dir / f"upload_video_{int(time.time())}.mp4"
            with open(temp_video_path, "wb") as f:
                f.write(video_input.file.read())
            video_data = str(temp_video_path)
        except Exception as e:
            print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
    
    # è¿”å›æµå¼å“åº”
    def generate():
        for result in model_service.process_multimodal_streaming(
            text_input=text_input,
            image_input=image_data,
            audio_input=audio_data,
            video_input=video_data,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            extract_video_audio=extract_video_audio,
            extract_video_frame=extract_video_frame,
            using_mm_info_audio=using_mm_info_audio,
            enable_audio_output=enable_audio_output
        ):
            yield f"data: {result}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")

@app.get("/files/{file_type}/{filename}")
async def get_file(file_type: str, filename: str):
    """è·å–ä¸´æ—¶æ–‡ä»¶"""
    file_path = model_service.temp_dir / file_type / filename
    if file_path.exists():
        from fastapi.responses import FileResponse
        return FileResponse(str(file_path))
    else:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶æœªæ‰¾åˆ°")

if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    
    print(f"ğŸš€ å¯åŠ¨vLLM APIæœåŠ¡: http://{host}:{port}")
    print(f"ğŸ“š APIæ–‡æ¡£: http://{host}:{port}/docs")
    
    uvicorn.run(
        "vllm_model_service:app",
        host=host,
        port=port,
        reload=False,
        log_level="info"
    )
