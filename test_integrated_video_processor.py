#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•´åˆç‰ˆè§†é¢‘å¤„ç†ä¼˜åŒ–æ¨¡å—æµ‹è¯•è„šæœ¬
åŠŸèƒ½åŒ…æ‹¬ï¼š
1. å¤§è§†é¢‘æ–‡ä»¶å‹ç¼©ä¼˜åŒ–æµ‹è¯•
2. é€šç”¨åŠŸèƒ½éªŒè¯æµ‹è¯•
3. è‡ªå®šä¹‰é…ç½®æµ‹è¯•
4. æ€§èƒ½åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
"""

import torch
import gc
import os
import sys
import time
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from video_processor_optimizer import (
    VideoProcessorOptimizer, 
    VideoOptimizationConfig, 
    VideoOptimizationPresets,
    load_config_from_file,
    save_config_to_file
)

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniConfig

# ä»ç¯å¢ƒå˜é‡åŠ è½½æ¨¡å‹è·¯å¾„
MODEL_PATH = os.getenv('MODEL_PATH', "/home/caden/workplace/models/Qwen2.5-Omni-3B")

def print_gpu_memory_usage(stage=""):
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        print(f"[{stage}] GPUå†…å­˜ - å·²åˆ†é…: {allocated:.2f}GB, å·²é¢„ç•™: {reserved:.2f}GB")

def load_model():
    """åŠ è½½æ¨¡å‹"""
    print("=== åŠ è½½æ¨¡å‹ ===")
    
    try:
        config = Qwen2_5OmniConfig.from_pretrained(MODEL_PATH)
        config.enable_audio_output = False
        
        device_map = {
            "thinker.model": "cuda",
            "thinker.lm_head": "cuda",
            "thinker.visual": "cuda",
            "thinker.audio_tower": "cuda",
        }
        
        max_memory = {0: "8GB", "cpu": "16GB"}
        
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        print_gpu_memory_usage("åŠ è½½å‰")
        
        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            config=config,
            device_map=device_map,
            max_memory=max_memory,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        
        processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)
        
        print_gpu_memory_usage("æ¨¡å‹åŠ è½½å")
        print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_video_with_optimization(video_path: str, preset_name: str, model, processor, detailed_output=True):
    """ä½¿ç”¨æŒ‡å®šé¢„è®¾é…ç½®æµ‹è¯•è§†é¢‘"""
    print(f"\n{'='*60}")
    print(f"ğŸ¬ æµ‹è¯•è§†é¢‘: {os.path.basename(video_path)}")
    print(f"âš™ï¸ ä½¿ç”¨é¢„è®¾: {preset_name}")
    print(f"{'='*60}")
    
    try:
        # è·å–é¢„è®¾é…ç½®
        config = VideoOptimizationPresets.get_preset(preset_name)
        print(f"ğŸ“‹ é…ç½®è¯¦æƒ…:")
        print(f"  - å¸§æ•°: {config.nframes}")
        print(f"  - åˆ†è¾¨ç‡: {config.resized_width}x{config.resized_height}")
        print(f"  - æ—¶é—´èŒƒå›´: {config.video_start}s - {config.video_end}s")
        print(f"  - æœ€å¤§åƒç´ : {config.max_pixels:,}")
        print(f"  - åŠç²¾åº¦: {config.use_half_precision}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = VideoProcessorOptimizer(config)
        
        # åˆ›å»ºå¯¹è¯
        conversation = [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªè§†é¢‘åˆ†æåŠ©æ‰‹ï¼Œ è¯·æ ¹æ®å¾—åˆ°çš„ä¿¡æ¯å®Œæˆä»»åŠ¡ã€‚"}
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": video_path},
                    {"type": "text", "text": "å®Œæˆè§†é¢‘ä¸­çš„æŒ‡ä»¤ã€‚"},
                ],
            },
        ]
        
        # å¤„ç†è§†é¢‘
        print(f"\nğŸ”„ å¼€å§‹å¤„ç†è§†é¢‘...")
        start_time = time.time()
        
        success, video_tensor, info = optimizer.process_video(video_path, conversation)
        
        if not success:
            print("âŒ è§†é¢‘å¤„ç†å¤±è´¥")
            return False, {}, ""
        
        total_time = time.time() - start_time
        
        if detailed_output:
            print(f"\nâœ… è§†é¢‘å¤„ç†æˆåŠŸï¼")
            print(f"ğŸ“Š å¤„ç†ç»“æœ:")
            print(f"  - åŸå§‹æ–‡ä»¶å¤§å°: {info['file_size_mb']:.2f} MB")
            print(f"  - æœ€ç»ˆå¼ é‡å½¢çŠ¶: {info['final_shape']}")
            print(f"  - æœ€ç»ˆå†…å­˜å ç”¨: {info['final_memory_mb']:.2f} MB")
            print(f"  - å‹ç¼©æ¯”: {info['file_size_mb'] / info['final_memory_mb']:.0f}:1")
            print(f"  - å¤„ç†æ—¶é—´: {info['processing_time']:.2f}ç§’")
            print(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        else:
            print(f"âœ… è§†é¢‘å¤„ç†æˆåŠŸï¼")
        
        print_gpu_memory_usage("è§†é¢‘å¤„ç†å")
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        print(f"\nğŸ”„ åº”ç”¨èŠå¤©æ¨¡æ¿...")
        text = processor.apply_chat_template(conversation, add_generation_prompt=True)
        
        # å¤„ç†è¾“å…¥
        print(f"ğŸ”„ å¤„ç†æ¨¡å‹è¾“å…¥...")
        inputs = processor(
            text=[text],
            images=None,  # æ²¡æœ‰å›¾ç‰‡è¾“å…¥
            videos=[video_tensor],
            padding=True,
            return_tensors="pt",
        )
        
        # ç¡®ä¿å¼ é‡ç±»å‹æ­£ç¡®
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor):
                if key in ["input_ids", "image_grid_thw", "video_grid_thw"]:
                    inputs[key] = value.to(model.device).long()
                elif "visual" in key:
                    inputs[key] = value.to(model.device).to(model.dtype)
                else:
                    inputs[key] = value.to(model.device).to(model.dtype)
        
        print_gpu_memory_usage("è¾“å…¥å¤„ç†å")
        
        # ç”Ÿæˆå›å¤
        print(f"ğŸ”„ ç”ŸæˆAIå›å¤...")
        generation_start = time.time()
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=256 if detailed_output else 128,  # æ ¹æ®è¾“å‡ºæ¨¡å¼è°ƒæ•´tokenæ•°é‡
                do_sample=False,
                temperature=0.0,
                top_p=1.0,
            )
        
        generation_time = time.time() - generation_start
        print_gpu_memory_usage("ç”Ÿæˆå")
        
        # è§£ç è¾“å‡º
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        if detailed_output:
            print(f"\nğŸ‰ æ¨ç†æˆåŠŸï¼")
            print(f"ğŸ¤– AIå›å¤:")
            print(f"{response}")
            print(f"\nâ±ï¸ ç”Ÿæˆè€—æ—¶: {generation_time:.2f}ç§’")
        else:
            print(f"âœ… æ¨ç†æˆåŠŸï¼")
            print(f"AIå›å¤: {response}")
        
        # æ¸…ç†
        optimizer.cleanup()
        
        return True, info, response
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, {}, ""

def test_custom_config(video_path: str, model, processor):
    """æµ‹è¯•è‡ªå®šä¹‰é…ç½®"""
    print(f"\n{'='*60}")
    print(f"ğŸ”§ æµ‹è¯•è‡ªå®šä¹‰é…ç½®")
    print(f"{'='*60}")
    
    try:
        # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
        custom_config = VideoOptimizationConfig(
            nframes=6,                    # 6å¸§
            resized_height=168,           # 168x168åˆ†è¾¨ç‡
            resized_width=168,
            video_start=0.0,              # ä»å¼€å§‹
            video_end=4.0,                # å‰4ç§’
            max_pixels=256 * 28 * 28,     # åƒç´ é™åˆ¶
            use_half_precision=True,
            enable_audio=False
        )
        
        print(f"ğŸ“‹ è‡ªå®šä¹‰é…ç½®:")
        print(f"  - å¸§æ•°: {custom_config.nframes}")
        print(f"  - åˆ†è¾¨ç‡: {custom_config.resized_width}x{custom_config.resized_height}")
        print(f"  - æ—¶é—´èŒƒå›´: {custom_config.video_start}s - {custom_config.video_end}s")
        print(f"  - æœ€å¤§åƒç´ : {custom_config.max_pixels:,}")
        print(f"  - åŠç²¾åº¦: {custom_config.use_half_precision}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = VideoProcessorOptimizer(custom_config)
        
        # åˆ›å»ºå¯¹è¯
        conversation = [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªè§†é¢‘åˆ†æåŠ©æ‰‹ï¼Œ è¯·æ ¹æ®å¾—åˆ°çš„ä¿¡æ¯å®Œæˆä»»åŠ¡ã€‚"}
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": video_path},
                    {"type": "text", "text": "å®Œæˆè§†é¢‘ä¸­çš„æŒ‡ä»¤ã€‚"},
                ],
            },
        ]
        
        # å¤„ç†è§†é¢‘
        print(f"\nğŸ”„ å¼€å§‹å¤„ç†è§†é¢‘...")
        success, video_tensor, info = optimizer.process_video(video_path, conversation)
        
        if not success:
            print("âŒ è§†é¢‘å¤„ç†å¤±è´¥")
            return False
        
        print_gpu_memory_usage("è§†é¢‘å¤„ç†å")
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = processor.apply_chat_template(conversation, add_generation_prompt=True)
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=[text],
            images=None,  # æ²¡æœ‰å›¾ç‰‡è¾“å…¥
            videos=[video_tensor],
            padding=True,
            return_tensors="pt",
        )
        
        # ç¡®ä¿å¼ é‡ç±»å‹æ­£ç¡®
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor):
                if key in ["input_ids", "image_grid_thw", "video_grid_thw"]:
                    inputs[key] = value.to(model.device).long()
                elif "visual" in key:
                    inputs[key] = value.to(model.device).to(model.dtype)
                else:
                    inputs[key] = value.to(model.device).to(model.dtype)
        
        print_gpu_memory_usage("è¾“å…¥å¤„ç†å")
        
        # ç”Ÿæˆå›å¤
        print("ğŸ”„ ç”ŸæˆAIå›å¤...")
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=False,
                temperature=0.0,
                top_p=1.0,
            )
        
        print_gpu_memory_usage("ç”Ÿæˆå")
        
        # è§£ç è¾“å‡º
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"âœ… æ¨ç†æˆåŠŸï¼")
        print(f"AIå›å¤: {response}")
        print(f"å¤„ç†ä¿¡æ¯: {info}")
        
        # æ¸…ç†
        optimizer.cleanup()
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_all_presets_for_video(video_path: str, model, processor):
    """æµ‹è¯•æ‰€æœ‰é¢„è®¾é…ç½®å¯¹æŒ‡å®šè§†é¢‘çš„æ•ˆæœ"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª å…¨é¢æµ‹è¯•è§†é¢‘: {os.path.basename(video_path)}")
    print(f"{'='*80}")
    
    # è·å–æ‰€æœ‰é¢„è®¾é…ç½®
    presets = VideoOptimizationPresets.list_presets()
    
    results = {}
    
    for preset_name in presets:
        print(f"\n{'='*60}")
        print(f"ğŸ¯ æµ‹è¯•é¢„è®¾: {preset_name}")
        print(f"{'='*60}")
        
        success, info, response = test_video_with_optimization(video_path, preset_name, model, processor)
        
        if success:
            results[preset_name] = {
                'success': True,
                'info': info,
                'response': response
            }
            print(f"âœ… {preset_name} é¢„è®¾æµ‹è¯•æˆåŠŸ")
        else:
            results[preset_name] = {
                'success': False,
                'info': {},
                'response': ""
            }
            print(f"âŒ {preset_name} é¢„è®¾æµ‹è¯•å¤±è´¥")
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        torch.cuda.empty_cache()
        print_gpu_memory_usage("æ¸…ç†å")
        
        # ç­‰å¾…ä¸€ä¸‹å†æµ‹è¯•ä¸‹ä¸€ä¸ª
        time.sleep(2)
    
    return results

def test_basic_functionality(video_path: str, model, processor):
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼ˆä½¿ç”¨éƒ¨åˆ†é¢„è®¾é…ç½®ï¼‰"""
    print(f"\n{'='*60}")
    print(f"ğŸ” åŸºæœ¬åŠŸèƒ½æµ‹è¯•: {os.path.basename(video_path)}")
    print(f"{'='*60}")
    
    # é€‰æ‹©éƒ¨åˆ†é¢„è®¾é…ç½®è¿›è¡Œæµ‹è¯•
    presets_to_test = ['extreme_low_memory', 'low_memory', 'balanced']
    
    results = {}
    
    for preset_name in presets_to_test:
        print(f"\n{'='*40}")
        print(f"ğŸ¯ æµ‹è¯•é¢„è®¾: {preset_name}")
        print(f"{'='*40}")
        
        success, info, response = test_video_with_optimization(video_path, preset_name, model, processor, detailed_output=False)
        
        if success:
            results[preset_name] = {
                'success': True,
                'info': info,
                'response': response
            }
            print(f"âœ… {preset_name} é¢„è®¾æµ‹è¯•æˆåŠŸ")
        else:
            results[preset_name] = {
                'success': False,
                'info': {},
                'response': ""
            }
            print(f"âŒ {preset_name} é¢„è®¾æµ‹è¯•å¤±è´¥")
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        torch.cuda.empty_cache()
        print_gpu_memory_usage("æ¸…ç†å")
        
        # ç­‰å¾…ä¸€ä¸‹å†æµ‹è¯•ä¸‹ä¸€ä¸ª
        time.sleep(1)
    
    return results

def print_summary_report(video_path: str, results: dict):
    """æ‰“å°æµ‹è¯•æ€»ç»“æŠ¥å‘Š"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“æŠ¥å‘Š: {os.path.basename(video_path)}")
    print(f"{'='*80}")
    
    successful_presets = []
    failed_presets = []
    
    for preset_name, result in results.items():
        if result['success']:
            successful_presets.append(preset_name)
        else:
            failed_presets.append(preset_name)
    
    print(f"âœ… æˆåŠŸçš„é¢„è®¾é…ç½® ({len(successful_presets)}/{len(results)}):")
    for preset in successful_presets:
        info = results[preset]['info']
        compression_ratio = info['file_size_mb'] / info['final_memory_mb']
        print(f"  - {preset}: å‹ç¼©æ¯” {compression_ratio:.0f}:1, å†…å­˜ {info['final_memory_mb']:.2f}MB")
    
    if failed_presets:
        print(f"\nâŒ å¤±è´¥çš„é¢„è®¾é…ç½® ({len(failed_presets)}):")
        for preset in failed_presets:
            print(f"  - {preset}")
    
    # æ¨èæœ€ä½³é…ç½®
    if successful_presets:
        best_preset = min(successful_presets, 
                         key=lambda x: results[x]['info']['final_memory_mb'])
        best_info = results[best_preset]['info']
        print(f"\nğŸ† æ¨èé…ç½®: {best_preset}")
        print(f"  - å†…å­˜å ç”¨æœ€ä½: {best_info['final_memory_mb']:.2f} MB")
        print(f"  - å‹ç¼©æ¯”: {best_info['file_size_mb'] / best_info['final_memory_mb']:.0f}:1")
        print(f"  - å¤„ç†æ—¶é—´: {best_info['processing_time']:.2f}ç§’")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ•´åˆç‰ˆè§†é¢‘å¤„ç†ä¼˜åŒ–æµ‹è¯•è„šæœ¬")
    print("="*80)
    print("åŠŸèƒ½åŒ…æ‹¬ï¼š")
    print("1. ğŸ¬ å¤§è§†é¢‘æ–‡ä»¶å‹ç¼©ä¼˜åŒ–æµ‹è¯•")
    print("2. ğŸ” åŸºæœ¬åŠŸèƒ½éªŒè¯æµ‹è¯•")
    print("3. ğŸ”§ è‡ªå®šä¹‰é…ç½®æµ‹è¯•")
    print("4. ğŸ“Š æ€§èƒ½åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ")
    print("="*80)
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    video_paths = [
        "./math.mp4",           # 82.76 MB - ä¹‹å‰æ— æ³•è¿è¡Œçš„å¤§æ–‡ä»¶
        "./math_last_3s.mp4",   # 3ç§’ç‰‡æ®µ
        "./test_video.mp4"      # 32.82 MB - draw.mp4
    ]
    
    available_videos = []
    for path in video_paths:
        if os.path.exists(path):
            available_videos.append(path)
            file_size = os.path.getsize(path) / (1024 * 1024)
            print(f"ğŸ“ æ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {path} ({file_size:.2f} MB)")
        else:
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    if not available_videos:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„è§†é¢‘æ–‡ä»¶ï¼Œé€€å‡º")
        return
    
    try:
        # åŠ è½½æ¨¡å‹
        model, processor = load_model()
        
        if model is None or processor is None:
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
            return
        
        # é€‰æ‹©æµ‹è¯•æ¨¡å¼
        print(f"\n{'='*60}")
        print("ğŸ¯ è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
        print("1. ğŸ§ª å…¨é¢æµ‹è¯•æ¨¡å¼ - æµ‹è¯•æ‰€æœ‰é¢„è®¾é…ç½®ï¼ˆé€‚åˆå¤§æ–‡ä»¶æ€§èƒ½åˆ†æï¼‰")
        print("2. ğŸ” åŸºæœ¬æµ‹è¯•æ¨¡å¼ - æµ‹è¯•éƒ¨åˆ†é¢„è®¾é…ç½®ï¼ˆé€‚åˆåŠŸèƒ½éªŒè¯ï¼‰")
        print("3. ğŸ”§ è‡ªå®šä¹‰é…ç½®æµ‹è¯• - æµ‹è¯•è‡ªå®šä¹‰ä¼˜åŒ–å‚æ•°")
        print("4. ğŸš€ å…¨éƒ¨æµ‹è¯• - ä¾æ¬¡æ‰§è¡Œæ‰€æœ‰æµ‹è¯•")
        print("="*60)
        
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æµ‹è¯•æ¨¡å¼
        test_mode = "4"  # é»˜è®¤æ‰§è¡Œå…¨éƒ¨æµ‹è¯•
        
        if test_mode == "1" or test_mode == "4":
            # å…¨é¢æµ‹è¯•æ¨¡å¼
            for video_path in available_videos:
                print(f"\n{'='*80}")
                print(f"ğŸ¬ å¼€å§‹å…¨é¢æµ‹è¯•è§†é¢‘: {os.path.basename(video_path)}")
                print(f"{'='*80}")
                
                # æµ‹è¯•æ‰€æœ‰é¢„è®¾é…ç½®
                results = test_all_presets_for_video(video_path, model, processor)
                
                # æ‰“å°æ€»ç»“æŠ¥å‘Š
                print_summary_report(video_path, results)
                
                # ç­‰å¾…ä¸€ä¸‹å†æµ‹è¯•ä¸‹ä¸€ä¸ªè§†é¢‘
                if len(available_videos) > 1:
                    print(f"\nâ³ ç­‰å¾…5ç§’åæµ‹è¯•ä¸‹ä¸€ä¸ªè§†é¢‘...")
                    time.sleep(5)
        
        if test_mode == "2" or test_mode == "4":
            # åŸºæœ¬æµ‹è¯•æ¨¡å¼
            for video_path in available_videos:
                print(f"\n{'='*80}")
                print(f"ğŸ” å¼€å§‹åŸºæœ¬åŠŸèƒ½æµ‹è¯•: {os.path.basename(video_path)}")
                print(f"{'='*80}")
                
                # æµ‹è¯•éƒ¨åˆ†é¢„è®¾é…ç½®
                results = test_basic_functionality(video_path, model, processor)
                
                # æ‰“å°æ€»ç»“æŠ¥å‘Š
                print_summary_report(video_path, results)
                
                # ç­‰å¾…ä¸€ä¸‹å†æµ‹è¯•ä¸‹ä¸€ä¸ªè§†é¢‘
                if len(available_videos) > 1:
                    print(f"\nâ³ ç­‰å¾…3ç§’åæµ‹è¯•ä¸‹ä¸€ä¸ªè§†é¢‘...")
                    time.sleep(3)
        
        if test_mode == "3" or test_mode == "4":
            # è‡ªå®šä¹‰é…ç½®æµ‹è¯•
            print(f"\n{'='*80}")
            print(f"ğŸ”§ å¼€å§‹è‡ªå®šä¹‰é…ç½®æµ‹è¯•")
            print(f"{'='*80}")
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨è§†é¢‘è¿›è¡Œè‡ªå®šä¹‰é…ç½®æµ‹è¯•
            test_video = available_videos[0]
            success = test_custom_config(test_video, model, processor)
            
            if success:
                print("âœ… è‡ªå®šä¹‰é…ç½®æµ‹è¯•æˆåŠŸ")
            else:
                print("âŒ è‡ªå®šä¹‰é…ç½®æµ‹è¯•å¤±è´¥")
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"  - æµ‹è¯•è§†é¢‘æ•°é‡: {len(available_videos)}")
        print(f"  - å¯ç”¨é¢„è®¾é…ç½®: {len(VideoOptimizationPresets.list_presets())}")
        print(f"  - æˆåŠŸå¤„ç†å¤§æ–‡ä»¶ï¼Œå†…å­˜å ç”¨å¤§å¹…é™ä½ï¼")
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
