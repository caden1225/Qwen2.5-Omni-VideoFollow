#!/usr/bin/env python3
"""
Qwen2.5-Omni å¤šæ¨¡æ€Gradioç•Œé¢
æ”¯æŒè§†é¢‘ã€è¯­éŸ³ã€å›¾åƒã€æ–‡æœ¬ç­‰ä¸åŒæ¨¡æ€çš„ç»„åˆè¾“å…¥
"""

import os
import time
import logging
from typing import List, Optional, Tuple, Dict, Any

import torch
from PIL import Image
import gradio as gr
from dotenv import load_dotenv
import soundfile as sf
import numpy as np
import functools

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from qwen_omni_utils import process_mm_info
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

VOICE_LIST = ['Chelsie', 'Ethan']
DEFAULT_VOICE = 'Chelsie'

# æ—¶é—´ç»Ÿè®¡è£…é¥°å™¨å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
class TimeitContext:
    def __init__(self, description):
        self.description = description
        self.start = None
    
    def __enter__(self):
        self.start = time.perf_counter()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        end = time.perf_counter()
        print(f"{self.description} æ‰§è¡Œè€—æ—¶: {end - self.start:.6f} ç§’")

def timeit(func_or_description):
    """æ”¯æŒè£…é¥°å™¨å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸¤ç§ä½¿ç”¨æ–¹å¼"""
    if callable(func_or_description):
        # ä½œä¸ºè£…é¥°å™¨ä½¿ç”¨
        @functools.wraps(func_or_description)
        def wrapper(*args, **kwargs):
            start = time.perf_counter()
            result = func_or_description(*args, **kwargs)
            end = time.perf_counter()
            print(f"{func_or_description.__name__} æ‰§è¡Œè€—æ—¶: {end - start:.6f} ç§’")
            return result
        return wrapper
    else:
        # ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨
        return TimeitContext(func_or_description)

class QwenOmniInference:
    def __init__(self, checkpoint_path, cpu_only=False, flash_attn2=False):
        """
        åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨
        
        Args:
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            cpu_only: æ˜¯å¦åªä½¿ç”¨CPU
            flash_attn2: æ˜¯å¦å¯ç”¨flash attention 2
        """
        self.checkpoint_path = checkpoint_path
        self.cpu_only = cpu_only
        self.flash_attn2 = flash_attn2
        
        self.model = None
        self.processor = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        if self.cpu_only:
            device_map = 'cpu'
        else:
            device_map = 'auto'

        if self.flash_attn2:
            self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(self.checkpoint_path,
                                                        torch_dtype='auto',
                                                        attn_implementation='flash_attention_2',
                                                        device_map=device_map)
        else:
             self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(self.checkpoint_path, device_map=device_map, torch_dtype='auto')

        self.processor = Qwen2_5OmniProcessor.from_pretrained(self.checkpoint_path)
        print(f"æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.model.device}")
    
    
    def predict(self, formatted_messages, voice="Chelsie", save_audio_path=None):
        """
        æ‰§è¡Œæ¨ç†
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            voice: è¯­éŸ³é€‰æ‹© ('Chelsie' æˆ– 'Ethan')
            save_audio_path: éŸ³é¢‘ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            dict: åŒ…å«æ–‡æœ¬å’ŒéŸ³é¢‘çš„å­—å…¸
        """

        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = self.processor.apply_chat_template(
            formatted_messages, 
            add_generation_prompt=True, 
            tokenize=False
        )
        
        # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯
        audios, images, videos = process_mm_info(
            formatted_messages, 
            use_audio_in_video=True
        )
        
        # å‡†å¤‡è¾“å…¥
        inputs = self.processor(
            text=text, 
            audio=audios, 
            images=images, 
            videos=videos, 
            return_tensors="pt", 
            padding=True, 
            use_audio_in_video=True
        )
        inputs = inputs.to(self.model.device).to(self.model.dtype)
        
        print("ğŸš€ å¼€å§‹ç”Ÿæˆå›ç­”...")
        with timeit("ç”Ÿæˆå›å¤"):
            text_ids, audio_output = self.model.generate(
                **inputs, 
                speaker=voice, 
                use_audio_in_video=True
            )
        
        # è§£ç æ–‡æœ¬å›å¤
        response = self.processor.batch_decode(
            text_ids, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        response = response[0].split("\n")[-1]
        
        # å¤„ç†éŸ³é¢‘
        if audio_output is not None and save_audio_path:
            audio = np.array(audio_output * 32767).astype(np.int16)
            sf.write(save_audio_path, audio, samplerate=24000, format="WAV")
            print(f"éŸ³é¢‘å·²ä¿å­˜åˆ°: {save_audio_path}")
        
        return {
            "text": response,
            "audio": audio,
            "sample_rate": 24000
        }

class MultimodalProcessor:
    def __init__(self):
        self.inference_engine = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_path = os.getenv("MODEL_PATH", "/home/caden/models/Qwen2.5-Omni-3B")
        self.temp_files = []
        
    def _print_memory_usage(self, stage: str):
        """æ‰“å°ä¸åŒé˜¶æ®µçš„æ˜¾å­˜å ç”¨ç»Ÿè®¡"""
        if not torch.cuda.is_available():
            return
            
        print(f"\nğŸ” {stage} - æ˜¾å­˜å ç”¨ç»Ÿè®¡:")
        print(f"   å½“å‰æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"   æ˜¾å­˜ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
        print(f"   æ˜¾å­˜å³°å€¼: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
        
        # è·å–GPUä¿¡æ¯
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / 1024**3
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            free = total_memory - cached
            
            print(f"   GPU {i} ({props.name}):")
            print(f"     æ€»æ˜¾å­˜: {total_memory:.2f} GB")
            print(f"     å·²åˆ†é…: {allocated:.2f} GB")
            print(f"     å·²ç¼“å­˜: {cached:.2f} GB")
            print(f"     å¯ç”¨æ˜¾å­˜: {free:.2f} GB")
        print()
        
    def _analyze_module_memory_usage(self, stage: str):
        """åˆ†æQwen2.5-Omniæ¨¡å‹å„ä¸ªæ¨¡å—çš„æ˜¾å­˜/å†…å­˜å ç”¨"""
        if self.inference_engine is None or self.inference_engine.model is None:
            return
            
        print(f"\nğŸ” {stage} - æ¨¡å—çº§èµ„æºå ç”¨åˆ†æ:")
        
        # å®šä¹‰ä¸»è¦æ¨¡å—åŠå…¶è·¯å¾„
        main_modules = {
            "thinker": "thinker",
            "talker": "talker", 
            "token2wav": "token2wav",
            "thinker.model": "thinker.model",
            "thinker.visual": "thinker.visual",
            "thinker.audio_tower": "thinker.audio_tower",
            "thinker.lm_head": "thinker.lm_head"
        }
        
        total_params = 0
        total_trainable_params = 0
        
        for module_name, module_path in main_modules.items():
            try:
                # è·å–æ¨¡å—å¯¹è±¡
                module = self.inference_engine.model
                for attr in module_path.split('.'):
                    if hasattr(module, attr):
                        module = getattr(module, attr)
                    else:
                        module = None
                        break
                
                if module is not None:
                    # ç»Ÿè®¡å‚æ•°æ•°é‡
                    module_params = sum(p.numel() for p in module.parameters())
                    module_trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
                    
                    # ç»Ÿè®¡æ˜¾å­˜å ç”¨
                    module_gpu_memory = 0
                    module_cpu_memory = 0
                    
                    for param in module.parameters():
                        if param.device.type == 'cuda':
                            module_gpu_memory += param.numel() * param.element_size()
                        else:
                            module_cpu_memory += param.numel() * param.element_size()
                    
                    # ç»Ÿè®¡ç¼“å†²åŒºæ˜¾å­˜
                    for buffer in module.buffers():
                        if buffer.device.type == 'cuda':
                            module_gpu_memory += buffer.numel() * buffer.element_size()
                        else:
                            module_cpu_memory += buffer.numel() * buffer.element_size()
                    
                    total_params += module_params
                    total_trainable_params += module_trainable_params
                    
                    print(f"   ğŸ“Š {module_name}:")
                    print(f"      å‚æ•°æ•°é‡: {module_params:,} ({module_params/1e6:.2f}M)")
                    print(f"      å¯è®­ç»ƒå‚æ•°: {module_trainable_params:,} ({module_trainable_params/1e6:.2f}M)")
                    print(f"      GPUæ˜¾å­˜: {module_gpu_memory/1024**3:.3f} GB")
                    print(f"      CPUå†…å­˜: {module_cpu_memory/1024**3:.3f} GB")
                    
                    # æ˜¾ç¤ºè®¾å¤‡ä½ç½®
                    if hasattr(module, 'device'):
                        print(f"      è®¾å¤‡ä½ç½®: {module.device}")
                    else:
                        # å°è¯•ä»å‚æ•°æ¨æ–­è®¾å¤‡
                        device = next(module.parameters()).device if list(module.parameters()) else "unknown"
                        print(f"      è®¾å¤‡ä½ç½®: {device}")
                        
                else:
                    print(f"   âŒ {module_name}: æ¨¡å—ä¸å­˜åœ¨")
                    
            except Exception as e:
                print(f"   âŒ {module_name}: åˆ†æå¤±è´¥ - {e}")
        
        print(f"\n   ğŸ“ˆ æ€»è®¡:")
        print(f"      æ€»å‚æ•°: {total_params:,} ({total_params/1e6:.2f}M)")
        print(f"      å¯è®­ç»ƒå‚æ•°: {total_trainable_params:,} ({total_trainable_params/1e6:.2f}M)")
        
        # æ˜¾ç¤ºæ¨¡å‹æ€»æ˜¾å­˜å ç”¨
        if torch.cuda.is_available():
            model_gpu_memory = sum(p.numel() * p.element_size() for p in self.inference_engine.model.parameters() if p.device.type == 'cuda')
            model_gpu_memory += sum(b.numel() * b.element_size() for b in self.inference_engine.model.buffers() if b.device.type == 'cuda')
            print(f"      æ¨¡å‹æ€»GPUæ˜¾å­˜: {model_gpu_memory/1024**3:.3f} GB")
        
        print()
        
    def _monitor_inference_memory(self, modality_name: str):
        """ç›‘æ§æ¨ç†è¿‡ç¨‹ä¸­ç‰¹å®šæ¨¡æ€çš„æ˜¾å­˜å ç”¨"""
        class MemoryMonitor:
            def __init__(self, name, processor):
                self.name = name
                self.processor = processor
                self.start_memory = 0
                self.peak_memory = 0
                self.activation_memory = 0
                self.hooks = []
                
            def __enter__(self):
                if torch.cuda.is_available():
                    # è®°å½•å¼€å§‹æ—¶çš„æ˜¾å­˜çŠ¶æ€
                    torch.cuda.empty_cache()
                    self.start_memory = torch.cuda.memory_allocated()
                    self.peak_memory = self.start_memory
                    
                    # è®¾ç½®é’©å­æ¥ç›‘æ§æ¿€æ´»å€¼
                    self._setup_hooks()
                    
                    print(f"ğŸ” å¼€å§‹ç›‘æ§ {self.name} æ¨¡æ€æ¨ç†æ˜¾å­˜...")
                    print(f"   åˆå§‹æ˜¾å­˜: {self.start_memory / 1024**3:.3f} GB")
                
                return self
                
            def __exit__(self, exc_type, exc_val, exc_tb):
                if torch.cuda.is_available():
                    # ç§»é™¤é’©å­
                    self._remove_hooks()
                    
                    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
                    final_memory = torch.cuda.memory_allocated()
                    total_peak = torch.cuda.max_memory_allocated()
                    
                    # è®¡ç®—æ¿€æ´»æ˜¾å­˜ï¼ˆæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ˜¾å­˜ï¼‰
                    self.activation_memory = max(0, total_peak - self.start_memory)
                    
                    print(f"ğŸ” {self.name} æ¨¡æ€æ¨ç†å®Œæˆ - æ˜¾å­˜ç»Ÿè®¡:")
                    print(f"   åˆå§‹æ˜¾å­˜: {self.start_memory / 1024**3:.3f} GB")
                    print(f"   æœ€ç»ˆæ˜¾å­˜: {final_memory / 1024**3:.3f} GB")
                    print(f"   å³°å€¼æ˜¾å­˜: {total_peak / 1024**3:.3f} GB")
                    print(f"   æ¿€æ´»æ˜¾å­˜: {self.activation_memory / 1024**3:.3f} GB")
                    print(f"   æ¨ç†å¢é‡: {(final_memory - self.start_memory) / 1024**3:.3f} GB")
                    print()
                    
                    # é‡ç½®å³°å€¼ç»Ÿè®¡
                    torch.cuda.reset_peak_memory_stats()
                
            def _setup_hooks(self):
                """è®¾ç½®é’©å­æ¥ç›‘æ§æ¿€æ´»å€¼"""
                def forward_hook(module, input, output):
                    if torch.cuda.is_available():
                        current_memory = torch.cuda.memory_allocated()
                        self.peak_memory = max(self.peak_memory, current_memory)
                        
                        # è®¡ç®—æ¿€æ´»æ˜¾å­˜
                        if hasattr(output, 'numel'):
                            if hasattr(output, 'element_size'):
                                activation_size = output.numel() * output.element_size()
                            else:
                                activation_size = output.numel() * 4  # å‡è®¾float32
                            
                            if output.device.type == 'cuda':
                                self.activation_memory = max(self.activation_memory, activation_size)
                
                # ä¸ºå…³é”®æ¨¡å—æ·»åŠ é’©å­
                if hasattr(self.processor.inference_engine.model, 'thinker'):
                    if hasattr(self.processor.inference_engine.model.thinker, 'visual'):
                        self.hooks.append(self.processor.inference_engine.model.thinker.visual.register_forward_hook(forward_hook))
                    if hasattr(self.processor.inference_engine.model.thinker, 'audio_tower'):
                        self.hooks.append(self.processor.inference_engine.model.thinker.audio_tower.register_forward_hook(forward_hook))
                    if hasattr(self.processor.inference_engine.model.thinker, 'model'):
                        self.hooks.append(self.processor.inference_engine.model.thinker.model.register_forward_hook(forward_hook))
                
                if hasattr(self.processor.inference_engine.model, 'talker'):
                    self.hooks.append(self.processor.inference_engine.model.talker.register_forward_hook(forward_hook))
                
                if hasattr(self.processor.inference_engine.model, 'token2wav'):
                    self.hooks.append(self.processor.inference_engine.model.token2wav.register_forward_hook(forward_hook))
            
            def _remove_hooks(self):
                """ç§»é™¤æ‰€æœ‰é’©å­"""
                for hook in self.hooks:
                    hook.remove()
                self.hooks.clear()
        
        return MemoryMonitor(modality_name, self)
        
    def load_model(self, checkpoint_path=None, cpu_only=False, flash_attn2=False):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        try:
            if checkpoint_path is None:
                checkpoint_path = self.model_path
                
            print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {checkpoint_path}")
            print(f"   è®¾å¤‡: {'CPU' if cpu_only else 'GPU'}")
            print(f"   Flash Attention 2: {'å¯ç”¨' if flash_attn2 else 'ç¦ç”¨'}")
            
            # ä½¿ç”¨é‡å†™çš„æ¨ç†å¼•æ“
            self.inference_engine = QwenOmniInference(
                checkpoint_path=checkpoint_path,
                cpu_only=cpu_only,
                flash_attn2=flash_attn2
            )
            
            print("ğŸ“¦ æ¨¡å‹åŠ è½½å®Œæˆ")

            # ç»Ÿè®¡æ˜¾å­˜å ç”¨
            if torch.cuda.is_available():
                self._print_memory_usage("æ¨¡å‹åŠ è½½å®Œæˆå")
            
            # åˆ†æå„æ¨¡å—èµ„æºå ç”¨
            self._analyze_module_memory_usage("æ¨¡å‹åŠ è½½å®Œæˆå")
            
            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ"
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            print(error_msg)
            return error_msg

    def process_multimodal(self, 
                          text_input: str,
                          image_input: Optional[Image.Image],
                          audio_input: Optional[str],
                          video_input: Optional[str],
                          system_prompt: str,
                          max_tokens: int,
                          extract_video_audio: bool,
                          extract_video_frame: bool,
                          using_mm_info_audio: bool,
                          enable_streaming: bool = False,
                          enable_audio_output: bool = False):
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        
        if self.inference_engine is None:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "", None, None, 0, 0, None
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        generated_audio = None
        
        modality_types = []
        if text_input and text_input.strip():
            modality_types.append("æ–‡æœ¬")
        if image_input:
            modality_types.append("å›¾åƒ")
        if audio_input:
            modality_types.append("éŸ³é¢‘")
        if video_input:
            modality_types.append("è§†é¢‘")
        
        modality_name = "+".join(modality_types) if modality_types else "çº¯æ–‡æœ¬"

        # æ ¼å¼åŒ–æ¶ˆæ¯

        formatted_messages = []
        formatted_messages.append({
            "role": "system", 
            "content": [{"type": "text", "text": system_prompt}]
        })
        
        if text_input:
            formatted_messages.append({
                "role": "user",
                "content": [{"type": "text", "text": text_input}]
            })
        if image_input and image_input.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):
            formatted_messages.append({
                "role": "user",
                "content": [{"type": "image", "image": image_input}]
            })
        if audio_input and audio_input.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):
            formatted_messages.append({
                "role": "user",
                "content": [{"type": "audio", "audio": audio_input}]
            })
        if video_input and video_input.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
            formatted_messages.append({
                "role": "user",
                "content": [{"type": "video", "video": video_input}]
            })      
        
        # ç”Ÿæˆä¸´æ—¶éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        temp_audio_path = f"temp_generated_audio_{int(time.time())}.wav" if enable_audio_output else None
        
        with self._monitor_inference_memory(f"{modality_name}-æ¨ç†"):
            result = self.inference_engine.predict(
                formatted_messages=formatted_messages,
                voice=DEFAULT_VOICE,
                save_audio_path=temp_audio_path
            )
        
        response_text = result["text"]
        generated_audio = temp_audio_path if enable_audio_output and result.get("audio") is not None else None
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
        # æ„å»ºè¯¦ç»†çš„å¤„ç†ä¿¡æ¯
        status_info = f"""âœ… å¤„ç†å®Œæˆ
â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’
ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB"""

        return status_info, response_text, extracted_audio, extracted_frame, processing_time, peak_memory, generated_audio
            

# åˆ›å»ºå¤„ç†å™¨å®ä¾‹
processor = MultimodalProcessor()

# æ„å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="Qwen2.5-Omni å¤šæ¨¡æ€åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¤– Qwen2.5-Omni å¤šæ¨¡æ€æ™ºèƒ½åŠ©æ‰‹
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ›ï¸ æ¨¡å‹æ§åˆ¶")
                
                # æ¨¡å‹è·¯å¾„è¾“å…¥
                model_path = gr.Textbox(
                    label="ğŸ”— æ¨¡å‹è·¯å¾„",
                    value="/home/caden/models/Qwen2.5-Omni-3B",
                    placeholder="è¾“å…¥æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„",
                    info="Qwen2.5-Omniæ¨¡å‹çš„æœ¬åœ°è·¯å¾„"
                )
                
                # æ¨¡å‹é…ç½®é€‰é¡¹
                cpu_only = gr.Checkbox(
                    label="ğŸ–¥ï¸ ä»…ä½¿ç”¨CPU",
                    value=False,
                    info="å¼ºåˆ¶ä½¿ç”¨CPUè¿è¡Œæ¨¡å‹"
                )
                
                flash_attn2 = gr.Checkbox(
                    label="âš¡ Flash Attention 2",
                    value=False,
                    info="å¯ç”¨Flash Attention 2ä¼˜åŒ–"
                )
                
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                model_status = gr.Textbox(
                    label="æ¨¡å‹çŠ¶æ€", 
                    value="â³ æœªåŠ è½½", 
                    interactive=False
                )
                
                gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
                system_prompt = gr.Textbox(
                    label="ç³»ç»Ÿæç¤º",
                    value="'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'",
                    lines=2
                )
                max_tokens = gr.Slider(
                    minimum=50,
                    maximum=2048,
                    value=512,
                    step=50,
                    label="æœ€å¤§Tokenæ•°"
                )
                
                gr.Markdown("### ğŸ¬ è§†é¢‘å¤„ç†é€‰é¡¹")
                extract_video_audio = gr.Checkbox(
                    label="ğŸ“¢ æå–è§†é¢‘éŸ³è½¨",
                    value=False,
                    info="å°†è§†é¢‘éŸ³è½¨æå–ä¸ºéŸ³é¢‘è¾“å…¥"
                )
                extract_video_frame = gr.Checkbox(
                    label="ğŸ–¼ï¸ æå–è§†é¢‘æœ€åä¸€å¸§",
                    value=False,
                    info="å°†è§†é¢‘æœ€åä¸€å¸§æå–ä¸ºå›¾åƒè¾“å…¥"
                )
                using_mm_info_audio = gr.Checkbox(
                    label="ğŸµ ä½¿ç”¨mm_infoæå–éŸ³é¢‘",
                    value=False,
                    info="ä½¿ç”¨mm_infoæå–éŸ³é¢‘"
                )
                
                gr.Markdown("### âš¡ è¾“å‡ºæ¨¡å¼")
                enable_streaming = gr.Checkbox(
                    label="ğŸ“¡ å¯ç”¨æµå¼è¾“å‡º",
                    value=False,
                    info="å®æ—¶é€æ­¥æ˜¾ç¤ºç”Ÿæˆå†…å®¹ï¼Œæå‡äº¤äº’ä½“éªŒ"
                )
                enable_audio_output = gr.Checkbox(
                    label="ğŸµ å¯ç”¨è¯­éŸ³è¾“å‡º",
                    value=False,
                    info="ç”Ÿæˆè¯­éŸ³å›ç­”ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰"
                )
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ å¤šæ¨¡æ€è¾“å…¥")
                
                text_input = gr.Textbox(
                    label="ğŸ’¬ æ–‡æœ¬è¾“å…¥",
                    placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æŒ‡ä»¤...",
                    lines=3
                )
                
                with gr.Row():
                    with gr.Column():
                        image_input = gr.Image(
                            label="ğŸ–¼ï¸ å›¾åƒè¾“å…¥",
                            type="pil"
                        )
                        
                        audio_input = gr.Audio(
                            label="ğŸµ éŸ³é¢‘è¾“å…¥",
                            type="filepath"
                        )
                    
                    with gr.Column():
                        video_input = gr.Video(
                            label="ğŸ¬ è§†é¢‘è¾“å…¥"
                        )
                
                with gr.Row():
                    process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ ç”Ÿæˆç»“æœ")
                output_text = gr.Textbox(
                    label="AIå›ç­”",
                    lines=8,
                    placeholder="ç”Ÿæˆçš„å›ç­”å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                    interactive=False
                )
                
                # æ˜¾ç¤ºæå–çš„å†…å®¹
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸµ æå–çš„éŸ³é¢‘")
                        extracted_audio_display = gr.Audio(
                            label="ä»è§†é¢‘æå–çš„éŸ³é¢‘",
                            visible=True,
                            interactive=False
                        )
                    
                    with gr.Column():
                        gr.Markdown("### ğŸ–¼ï¸ æå–çš„å›¾åƒ")
                        extracted_image_display = gr.Image(
                            label="ä»è§†é¢‘æå–çš„æœ€åä¸€å¸§",
                            type="pil",
                            visible=True,
                            interactive=False
                        )
                
                # æ˜¾ç¤ºç”Ÿæˆçš„éŸ³é¢‘è¾“å‡º
                gr.Markdown("### ğŸ¤ ç”Ÿæˆçš„è¯­éŸ³å›ç­”")
                generated_audio_display = gr.Audio(
                    label="AIç”Ÿæˆçš„è¯­éŸ³å›ç­”",
                    visible=True,
                    interactive=False
                )
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š å¤„ç†ä¿¡æ¯")
                processing_info = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    lines=8,
                    interactive=False,
                    value="ç­‰å¾…å¤„ç†..."
                )
        
        # åŠ è½½æ¨¡å‹å‡½æ•°
        def load_model_with_config(model_path, cpu_only, flash_attn2):
            return processor.load_model(
                checkpoint_path=model_path,
                cpu_only=cpu_only,
                flash_attn2=flash_attn2
            )
        
        load_btn.click(
            fn=load_model_with_config,
            inputs=[model_path, cpu_only, flash_attn2],
            outputs=model_status
        )
        
        def handle_process(text_input, image_input, audio_input, video_input, system_prompt, max_tokens, 
                          extract_video_audio, extract_video_frame, using_mm_info_audio, enable_streaming, enable_audio_output):
            """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
            result = processor.process_multimodal(
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, enable_streaming, enable_audio_output
            )
            return result[0], result[1], result[2], result[3], result[6]  # status, text, audio, image, generated_audio
        
        # å¤„ç†æŒ‰é’®
        process_btn.click(
            fn=handle_process,
            inputs=[
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, enable_streaming, enable_audio_output
            ],
            outputs=[processing_info, output_text, extracted_audio_display, extracted_image_display, generated_audio_display]
        )
        
        def clear_all():
            return "", None, None, None, "", "ç­‰å¾…å¤„ç†...", None, None, None
        
        clear_btn.click(
            fn=clear_all,
            outputs=[text_input, image_input, audio_input, video_input, output_text, processing_info, extracted_audio_display, extracted_image_display, generated_audio_display]
        )
    
    return demo


if __name__ == "__main__":
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        debug=True,
        show_error=True
    )