#!/usr/bin/env python3
"""
Qwen2.5-Omni å¤šæ¨¡æ€Gradioç•Œé¢
æ”¯æŒè§†é¢‘ã€è¯­éŸ³ã€å›¾åƒã€æ–‡æœ¬ç­‰ä¸åŒæ¨¡æ€çš„ç»„åˆè¾“å…¥
"""

import os
import time
import logging
from typing import List, Optional, Tuple, Dict, Any
import tempfile

import torch
import numpy as np
import librosa
import cv2
from PIL import Image
import gradio as gr
from dotenv import load_dotenv
import soundfile as sf

# å¯¼å…¥qwen-omni-utils  
from qwen_omni_utils import process_mm_info
from transformers import Qwen2_5OmniProcessor
from transformers import Qwen2_5OmniForConditionalGeneration


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultimodalProcessor:
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_path = os.getenv("MODEL_PATH", "/home/caden/models/Qwen2.5-Omni-3B")
        self.temp_files = []
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        try:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
            # æ ‡å‡†æ¨¡å¼ä½†å°è¯•ä¼˜åŒ–æ˜¾å­˜
            self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                device_map="auto"
            )
            print("ğŸ“¦ æ¨¡å‹åŠ è½½å®Œæˆ")

            self.processor = Qwen2_5OmniProcessor.from_pretrained(self.model_path)
            print("å¤„ç†å™¨åŠ è½½å®Œæˆ")
            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ"
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            print(error_msg)
            return error_msg

    def extract_video_features(self, video_path: str, extract_audio: bool = False, extract_frame: bool = False):
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘å’Œæœ€åä¸€å¸§"""
        features = {}
        
        if extract_audio:
            try:
                audio, sr = librosa.load(video_path, sr=16000)
                features['audio'] = audio
                print(f"æå–éŸ³é¢‘æˆåŠŸ: {len(audio)} samples at {sr}Hz")
            except Exception as e:
                print(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
        
        if extract_frame:
            try:
                cap = cv2.VideoCapture(video_path)
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count - 1)
                ret, frame = cap.read()
                
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    image = Image.fromarray(frame_rgb)
                    features['last_frame'] = image
                    print("æå–è§†é¢‘æœ€åä¸€å¸§æˆåŠŸ")
                else:
                    print("æå–å¸§å¤±è´¥")
                cap.release()
            except Exception as e:
                print(f"å¸§æå–å¤±è´¥: {e}")
        
        return features

    def process_multimodal(self, 
                          text_input: str,
                          image_input: Optional[Image.Image],
                          audio_input: Optional[str],
                          video_input: Optional[str],
                          system_prompt: str,
                          max_tokens: int,
                          extract_video_audio: bool,
                          extract_video_frame: bool,
                          using_mm_info_audio: bool,
                          enable_streaming: bool = False,
                          enable_audio_output: bool = False):
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        
        if self.model is None:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "", None, None, 0, 0, None
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        generated_audio = None
        
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            ]
            
            user_content = []
            
            # æ·»åŠ æ–‡æœ¬
            if text_input and text_input.strip():
                user_content.append({"type": "text", "text": text_input.strip()})
            
            # å¤„ç†è§†é¢‘
            if video_input:
                if extract_video_audio or extract_video_frame:
                    print(f"ğŸ¬ å¼€å§‹æå–è§†é¢‘ç‰¹å¾...")
                    features = self.extract_video_features(
                        video_input, 
                        extract_audio=extract_video_audio, 
                        extract_frame=extract_video_frame
                    )
                    
                    if 'audio' in features:
                        user_content.append({"type": "audio", "audio": features['audio']})
                        # ä¿å­˜æå–çš„éŸ³é¢‘ä¾›æ˜¾ç¤º
                        temp_audio_path = f"temp_extracted_audio_{int(time.time())}.wav"
                        sf.write(temp_audio_path, features['audio'], 16000)
                        extracted_audio = temp_audio_path
                        print(f"âœ… éŸ³é¢‘å·²æå–å¹¶ä¿å­˜: {temp_audio_path}")
                    
                    if 'last_frame' in features:
                        user_content.append({"type": "image", "image": features['last_frame']})
                        # ä¿å­˜æå–çš„å›¾åƒä¾›æ˜¾ç¤º
                        extracted_frame = features['last_frame']
                        print(f"âœ… å›¾åƒå·²æå–")
                else:
                    # add args to qwen-mm-info-utils
                    user_content.append({"type": "video", "video": video_input, "using_mm_info_audio": using_mm_info_audio})
            
            # å¤„ç†å›¾åƒ
            if image_input:
                user_content.append({"type": "image", "image": image_input})
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    user_content.append({"type": "audio", "audio": audio_data})
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œä½¿ç”¨é»˜è®¤
            if not user_content:
                user_content.append({"type": "text", "text": "Hello"})
            
            messages.append({"role": "user", "content": user_content})
            
            print(f"ğŸ“ æ„å»ºçš„æ¶ˆæ¯åŒ…å« {len(user_content)} ä¸ªå†…å®¹é¡¹")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            print(f"ğŸ“„ ç”Ÿæˆçš„prompté•¿åº¦: {len(text_prompt)} å­—ç¬¦")
            
            # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯
            audios, images, videos = process_mm_info(messages, use_audio_in_video=using_mm_info_audio)
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†ç»“æœ: audios={len(audios) if audios else 0}, images={len(images) if images else 0}, videos={len(videos) if videos else 0}")
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_prompt, 
                audio=audios, 
                images=images, 
                videos=videos, 
                return_tensors="pt", 
                padding=True
            )
            
            print(f"ğŸ”§ è¾“å…¥tensorå½¢çŠ¶: {[(k, v.shape if hasattr(v, 'shape') else type(v)) for k, v in inputs.items()]}")
            
            # æ ‡å‡†æ¨¡å¼ï¼šè·å–æ¨¡å‹è®¾å¤‡
            device = next(self.model.parameters()).device
            inputs = inputs.to(device).to(self.model.dtype)
            
            print("ğŸš€ å¼€å§‹ç”Ÿæˆå›ç­”...")
            
            if enable_streaming:
                # æµå¼ç”Ÿæˆ
                print("ğŸ“¡ ä½¿ç”¨æµå¼è¾“å‡º...")
                response_text = ""
                
                # è·å–è¾“å…¥é•¿åº¦ç”¨äºåç»­è¿‡æ»¤
                input_length = inputs['input_ids'].shape[1]
                
                with torch.no_grad():
                    # ä½¿ç”¨æµå¼ç”Ÿæˆ
                    from transformers import TextIteratorStreamer
                    import threading
                    
                    streamer = TextIteratorStreamer(
                        self.processor.tokenizer, 
                        skip_prompt=True,
                        skip_special_tokens=True,
                        clean_up_tokenization_spaces=False
                    )
                    
                    generation_kwargs = dict(
                        inputs=inputs,
                        streamer=streamer,
                        max_new_tokens=max_tokens,
                        do_sample=False,
                        use_audio_in_video=True,
                        return_audio=enable_audio_output,
                        pad_token_id=self.processor.tokenizer.eos_token_id
                    )
                    
                    # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
                    thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
                    thread.start()
                    
                    # æµå¼è¯»å–ç”Ÿæˆçš„æ–‡æœ¬
                    for new_text in streamer:
                        response_text += new_text
                        # å®æ—¶æ›´æ–°å¯ä»¥åœ¨è¿™é‡Œå¤„ç†ï¼Œä½†gradioéœ€è¦ç‰¹æ®Šå¤„ç†
                    
                    thread.join()
                
                print(f"ğŸ“¡ æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(response_text)} å­—ç¬¦")
                
            else:
                # æ ‡å‡†ç”Ÿæˆ
                with torch.no_grad():
                    output = self.model.generate(
                        **inputs, 
                        max_new_tokens=max_tokens,
                        do_sample=False,  # ä½¿ç”¨è´ªå¿ƒè§£ç 
                        use_audio_in_video=True,
                        return_audio=enable_audio_output,  # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦è¿”å›éŸ³é¢‘
                        pad_token_id=self.processor.tokenizer.eos_token_id
                    )
                
                print(f"ğŸ“¤ ç”Ÿæˆè¾“å‡ºå½¢çŠ¶: {output.shape}")
                
                # å¤„ç†éŸ³é¢‘è¾“å‡º
                if enable_audio_output and hasattr(output, 'audio') and output.audio is not None:
                    try:
                        # ä¿å­˜ç”Ÿæˆçš„éŸ³é¢‘
                        audio_filename = f"generated_audio_{int(time.time())}.wav"
                        sf.write(audio_filename, output.audio.cpu().numpy(), 24000)  # Qwen2.5-Omniä½¿ç”¨24kHzé‡‡æ ·ç‡
                        generated_audio = audio_filename
                        print(f"ğŸµ éŸ³é¢‘å·²ç”Ÿæˆå¹¶ä¿å­˜: {audio_filename}")
                    except Exception as e:
                        print(f"éŸ³é¢‘ä¿å­˜å¤±è´¥: {e}")
                        generated_audio = None
                
                # è§£ç å“åº”
                response_text = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
                
                print(f"ğŸ“ åŸå§‹å›ç­”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                
                # æå–å®é™…çš„å›ç­”éƒ¨åˆ† (å»æ‰è¾“å…¥çš„prompt)
                # æ‰¾åˆ°assistantå›ç­”çš„å¼€å§‹ä½ç½®
                if "<|im_start|>assistant" in response_text:
                    response_text = response_text.split("<|im_start|>assistant")[-1].strip()
                elif "assistant\n" in response_text:
                    response_text = response_text.split("assistant\n")[-1].strip()
                
                # æ¸…ç†ç»“æŸç¬¦å·
                if response_text.endswith("<|im_end|>"):
                    response_text = response_text[:-10].strip()
            
            print(f"âœ… æ¸…ç†åå›ç­”: {response_text[:100]}...")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
            # æ„å»ºè¾“å‡ºä¿¡æ¯
            info_text = f"""
ğŸ“Š **å¤„ç†ç»Ÿè®¡**
- â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’
- ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB
- ğŸ”¤ æœ€å¤§Tokenæ•°: {max_tokens}
- ğŸ“ è¾“å‡ºTokenæ•°: {len(output[0]) - len(inputs['input_ids'][0])}
- ğŸ¯ ç³»ç»Ÿæç¤º: {system_prompt[:50]}...

ğŸ“‹ **è¾“å…¥å†…å®¹**
- æ–‡æœ¬è¾“å…¥: {'âœ…' if text_input else 'âŒ'}
- å›¾åƒè¾“å…¥: {'âœ…' if image_input else 'âŒ'}  
- éŸ³é¢‘è¾“å…¥: {'âœ…' if audio_input else 'âŒ'}
- è§†é¢‘è¾“å…¥: {'âœ…' if video_input else 'âŒ'}
- æå–éŸ³è½¨: {'âœ…' if extract_video_audio and extracted_audio else 'âŒ'}
- æå–å¸§: {'âœ…' if extract_video_frame and extracted_frame else 'âŒ'}
            """
            
            # æ„å»ºè¯¦ç»†çš„å¤„ç†ä¿¡æ¯
            status_info = f"""âœ… å¤„ç†å®Œæˆ - {'æµå¼' if enable_streaming else 'æ ‡å‡†'}æ¨¡å¼
â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’
ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB"""

            return status_info, response_text, extracted_audio, extracted_frame, processing_time, peak_memory, generated_audio
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg, "", None, None, 0, 0, None

    def process_multimodal_streaming(self, 
                                   text_input: str,
                                   image_input: Optional[Image.Image],
                                   audio_input: Optional[str],
                                   video_input: Optional[str],
                                   system_prompt: str,
                                   max_tokens: int,
                                   extract_video_audio: bool,
                                   extract_video_frame: bool,
                                   using_mm_info_audio: bool,
                                   enable_audio_output: bool = False):
        """æµå¼å¤„ç†å¤šæ¨¡æ€è¾“å…¥ - ä½¿ç”¨ç”Ÿæˆå™¨è¿”å›é€æ­¥æ›´æ–°"""
        
        if self.model is None:
            yield "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "", None, None, 0, 0, None
            return
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        generated_audio = None
        
        try:
            # å‰æœŸå¤„ç† - å’Œæ™®é€šå¤„ç†ç›¸åŒ
            yield "ğŸ”„ å¼€å§‹å¤„ç†...", "", None, None, 0, 0, None
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            ]
            
            user_content = []
            
            # æ·»åŠ æ–‡æœ¬
            if text_input and text_input.strip():
                user_content.append({"type": "text", "text": text_input.strip()})
            
            # å¤„ç†è§†é¢‘
            if video_input:
                if extract_video_audio or extract_video_frame:
                    yield "ğŸ¬ æå–è§†é¢‘ç‰¹å¾...", "", None, None, 0, 0, None
                    
                    features = self.extract_video_features(
                        video_input, 
                        extract_audio=extract_video_audio, 
                        extract_frame=extract_video_frame
                    )
                    
                    if 'audio' in features:
                        user_content.append({"type": "audio", "audio": features['audio']})
                        temp_audio_path = f"temp_extracted_audio_{int(time.time())}.wav"
                        sf.write(temp_audio_path, features['audio'], 16000)
                        extracted_audio = temp_audio_path
                    
                    if 'last_frame' in features:
                        user_content.append({"type": "image", "image": features['last_frame']})
                        extracted_frame = features['last_frame']
                        
                    yield "âœ… è§†é¢‘ç‰¹å¾æå–å®Œæˆ", "", extracted_audio, extracted_frame, 0, 0, None
                else:
                    user_content.append({"type": "video", "video": video_input})
            
            # å¤„ç†å›¾åƒ
            if image_input:
                user_content.append({"type": "image", "image": image_input})
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    user_content.append({"type": "audio", "audio": audio_data})
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            if not user_content:
                user_content.append({"type": "text", "text": "Hello"})
            
            messages.append({"role": "user", "content": user_content})
            
            yield "ğŸ“ æ„å»ºå¤šæ¨¡æ€è¾“å…¥...", "", extracted_audio, extracted_frame, 0, 0, None
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            
            # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯
            audios, images, videos = process_mm_info(messages, use_audio_in_video=using_mm_info_audio)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_prompt, 
                audio=audios, 
                images=images, 
                videos=videos, 
                return_tensors="pt", 
                padding=True
            )
            
            device = next(self.model.parameters()).device
            inputs = inputs.to(device).to(self.model.dtype)
            
            yield "ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ...", "", extracted_audio, extracted_frame, 0, 0, None
            
            # æµå¼ç”Ÿæˆ
            from transformers import TextIteratorStreamer
            import threading
            
            streamer = TextIteratorStreamer(
                self.processor.tokenizer, 
                skip_prompt=True,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )
            
            generation_kwargs = dict(
                **inputs,
                streamer=streamer,
                max_new_tokens=max_tokens,
                do_sample=False,
                use_audio_in_video=True,
                return_audio=enable_audio_output,
                pad_token_id=self.processor.tokenizer.eos_token_id
            )
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
            thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # æµå¼è¾“å‡º
            response_text = ""
            for new_text in streamer:
                if new_text.strip():  # å¿½ç•¥ç©ºç™½token
                    response_text += new_text
                    processing_time = time.time() - start_time
                    status = f"ğŸ“¡ æµå¼ç”Ÿæˆä¸­... ({processing_time:.1f}s)"
                    yield status, response_text, extracted_audio, extracted_frame, processing_time, 0, None
            
            thread.join()
            
            # å¤„ç†éŸ³é¢‘è¾“å‡ºï¼ˆæµå¼æ¨¡å¼ä¸‹éŸ³é¢‘åœ¨æœ€åç”Ÿæˆï¼‰
            if enable_audio_output:
                try:
                    # è¿™é‡Œéœ€è¦é‡æ–°ç”Ÿæˆä¸€æ¬¡æ¥è·å–éŸ³é¢‘ï¼Œæˆ–è€…ä¿®æ”¹æµå¼é€»è¾‘
                    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶åœ¨æµå¼æ¨¡å¼ä¸‹ä¸è¿”å›éŸ³é¢‘
                    generated_audio = None
                    print("ğŸ“¡ æµå¼æ¨¡å¼ä¸‹éŸ³é¢‘è¾“å‡ºæš‚ä¸æ”¯æŒ")
                except Exception as e:
                    print(f"æµå¼éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
                    generated_audio = None
            
            # æœ€ç»ˆç»“æœ
            processing_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
            final_status = f"""âœ… æµå¼ç”Ÿæˆå®Œæˆ!
â±ï¸ æ€»æ—¶é—´: {processing_time:.2f}ç§’
ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB
ğŸ“ è¾“å‡ºé•¿åº¦: {len(response_text)} å­—ç¬¦"""
            
            yield final_status, response_text, extracted_audio, extracted_frame, processing_time, peak_memory, generated_audio
            
        except Exception as e:
            error_msg = f"âŒ æµå¼å¤„ç†å¤±è´¥: {str(e)}"
            yield error_msg, "", extracted_audio, extracted_frame, 0, 0, None


# åˆ›å»ºå¤„ç†å™¨å®ä¾‹
processor = MultimodalProcessor()

# æ„å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="Qwen2.5-Omni å¤šæ¨¡æ€åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¤– Qwen2.5-Omni å¤šæ¨¡æ€æ™ºèƒ½åŠ©æ‰‹
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ›ï¸ æ¨¡å‹æ§åˆ¶")
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                model_status = gr.Textbox(
                    label="æ¨¡å‹çŠ¶æ€", 
                    value="â³ æœªåŠ è½½", 
                    interactive=False
                )
                
                gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
                system_prompt = gr.Textbox(
                    label="ç³»ç»Ÿæç¤º",
                    value="You are a helpful AI assistant.",
                    lines=2
                )
                max_tokens = gr.Slider(
                    minimum=50,
                    maximum=2048,
                    value=512,
                    step=50,
                    label="æœ€å¤§Tokenæ•°"
                )
                
                gr.Markdown("### ğŸ¬ è§†é¢‘å¤„ç†é€‰é¡¹")
                extract_video_audio = gr.Checkbox(
                    label="ğŸ“¢ æå–è§†é¢‘éŸ³è½¨",
                    value=False,
                    info="å°†è§†é¢‘éŸ³è½¨æå–ä¸ºéŸ³é¢‘è¾“å…¥"
                )
                extract_video_frame = gr.Checkbox(
                    label="ğŸ–¼ï¸ æå–è§†é¢‘æœ€åä¸€å¸§",
                    value=False,
                    info="å°†è§†é¢‘æœ€åä¸€å¸§æå–ä¸ºå›¾åƒè¾“å…¥"
                )
                using_mm_info_audio = gr.Checkbox(
                    label="ğŸµ ä½¿ç”¨mm_infoæå–éŸ³é¢‘",
                    value=False,
                    info="ä½¿ç”¨mm_infoæå–éŸ³é¢‘"
                )
                
                gr.Markdown("### âš¡ è¾“å‡ºæ¨¡å¼")
                enable_streaming = gr.Checkbox(
                    label="ğŸ“¡ å¯ç”¨æµå¼è¾“å‡º",
                    value=False,
                    info="å®æ—¶é€æ­¥æ˜¾ç¤ºç”Ÿæˆå†…å®¹ï¼Œæå‡äº¤äº’ä½“éªŒ"
                )
                enable_audio_output = gr.Checkbox(
                    label="ğŸµ å¯ç”¨è¯­éŸ³è¾“å‡º",
                    value=False,
                    info="ç”Ÿæˆè¯­éŸ³å›ç­”ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰"
                )
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ å¤šæ¨¡æ€è¾“å…¥")
                
                text_input = gr.Textbox(
                    label="ğŸ’¬ æ–‡æœ¬è¾“å…¥",
                    placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æŒ‡ä»¤...",
                    lines=3
                )
                
                with gr.Row():
                    with gr.Column():
                        image_input = gr.Image(
                            label="ğŸ–¼ï¸ å›¾åƒè¾“å…¥",
                            type="pil"
                        )
                        
                        audio_input = gr.Audio(
                            label="ğŸµ éŸ³é¢‘è¾“å…¥",
                            type="filepath"
                        )
                    
                    with gr.Column():
                        video_input = gr.Video(
                            label="ğŸ¬ è§†é¢‘è¾“å…¥"
                        )
                
                with gr.Row():
                    process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                    stream_btn = gr.Button("ğŸ“¡ æµå¼å¤„ç†", variant="secondary", size="lg", visible=False)
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ ç”Ÿæˆç»“æœ")
                output_text = gr.Textbox(
                    label="AIå›ç­”",
                    lines=8,
                    placeholder="ç”Ÿæˆçš„å›ç­”å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                    interactive=False
                )
                
                # æ˜¾ç¤ºæå–çš„å†…å®¹
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸµ æå–çš„éŸ³é¢‘")
                        extracted_audio_display = gr.Audio(
                            label="ä»è§†é¢‘æå–çš„éŸ³é¢‘",
                            visible=True,
                            interactive=False
                        )
                    
                    with gr.Column():
                        gr.Markdown("### ğŸ–¼ï¸ æå–çš„å›¾åƒ")
                        extracted_image_display = gr.Image(
                            label="ä»è§†é¢‘æå–çš„æœ€åä¸€å¸§",
                            type="pil",
                            visible=True,
                            interactive=False
                        )
                
                # æ˜¾ç¤ºç”Ÿæˆçš„éŸ³é¢‘è¾“å‡º
                gr.Markdown("### ğŸ¤ ç”Ÿæˆçš„è¯­éŸ³å›ç­”")
                generated_audio_display = gr.Audio(
                    label="AIç”Ÿæˆçš„è¯­éŸ³å›ç­”",
                    visible=True,
                    interactive=False
                )
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š å¤„ç†ä¿¡æ¯")
                processing_info = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    lines=8,
                    interactive=False,
                    value="ç­‰å¾…å¤„ç†..."
                )
        
        
        # äº‹ä»¶ç»‘å®š
        load_btn.click(
            fn=processor.load_model,
            outputs=model_status
        )
        
        def handle_process_standard(text_input, image_input, audio_input, video_input, system_prompt, max_tokens, 
                                   extract_video_audio, extract_video_frame, using_mm_info_audio, enable_streaming, enable_audio_output):
            """æ ‡å‡†å¤„ç†å‡½æ•°"""
            if enable_streaming:
                # å¦‚æœå¯ç”¨æµå¼ï¼Œç»™å‡ºæç¤º
                return "ğŸ“¡ æµå¼æ¨¡å¼ï¼šè¯·ç‚¹å‡»ä¸‹é¢çš„æµå¼å¤„ç†æŒ‰é’®", "", None, None, None
            else:
                # ä½¿ç”¨æ ‡å‡†å¤„ç†
                result = processor.process_multimodal(
                    text_input, image_input, audio_input, video_input,
                    system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, False, enable_audio_output
                )
                return result[0], result[1], result[2], result[3], result[6]  # status, text, audio, image, generated_audio
        
        def handle_process_streaming(text_input, image_input, audio_input, video_input, system_prompt, max_tokens, 
                                   extract_video_audio, extract_video_frame, using_mm_info_audio, enable_audio_output):
            """æµå¼å¤„ç†å‡½æ•°"""
            for status, text, audio, image, time, memory, generated_audio in processor.process_multimodal_streaming(
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, enable_audio_output
            ):
                yield status, text, audio, image, generated_audio
        
        # æ ‡å‡†å¤„ç†æŒ‰é’®
        process_btn.click(
            fn=handle_process_standard,
            inputs=[
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, enable_streaming, enable_audio_output
            ],
            outputs=[processing_info, output_text, extracted_audio_display, extracted_image_display, generated_audio_display]
        )
        
        # æµå¼æŒ‰é’®å·²åœ¨ä¸Šé¢å®šä¹‰
        
        stream_btn.click(
            fn=handle_process_streaming,
            inputs=[
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, enable_audio_output
            ],
            outputs=[processing_info, output_text, extracted_audio_display, extracted_image_display, generated_audio_display]
        )
        
        # æ ¹æ®æµå¼å¼€å…³æ§åˆ¶æŒ‰é’®æ˜¾ç¤º
        def update_buttons(enable_streaming):
            if enable_streaming:
                return gr.update(value="ğŸš€ æ ‡å‡†å¤„ç†"), gr.update(visible=True)
            else:
                return gr.update(value="ğŸš€ å¼€å§‹å¤„ç†"), gr.update(visible=False)
        
        enable_streaming.change(
            fn=update_buttons,
            inputs=[enable_streaming],
            outputs=[process_btn, stream_btn]
        )
        
        def clear_all():
            return "", None, None, None, "", "ç­‰å¾…å¤„ç†...", None, None, False, None
        
        clear_btn.click(
            fn=clear_all,
            outputs=[text_input, image_input, audio_input, video_input, output_text, processing_info, extracted_audio_display, extracted_image_display, enable_streaming, generated_audio_display]
        )
    
    return demo


if __name__ == "__main__":
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        debug=True,
        show_error=True
    )