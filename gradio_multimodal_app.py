#!/usr/bin/env python3
"""
Qwen2.5-Omni å¤šæ¨¡æ€Gradioç•Œé¢
æ”¯æŒè§†é¢‘ã€è¯­éŸ³ã€å›¾åƒã€æ–‡æœ¬ç­‰ä¸åŒæ¨¡æ€çš„ç»„åˆè¾“å…¥
æ”¯æŒGPTQé‡åŒ–ç‰ˆæœ¬ä»¥é™ä½æ˜¾å­˜å ç”¨
"""

import os
import time
import logging
from typing import List, Optional, Tuple, Dict, Any

import torch
import numpy as np
import librosa
import cv2
from PIL import Image
import gradio as gr
from dotenv import load_dotenv
import soundfile as sf

# å¯¼å…¥qwen-omni-utils  
from qwen_omni_utils import process_mm_info
from transformers import Qwen2_5OmniProcessor
from transformers import Qwen2_5OmniForConditionalGeneration

# GPTQç›¸å…³å¯¼å…¥
from transformers.utils.hub import cached_file
from gptqmodel import GPTQModel
from gptqmodel.models.base import BaseGPTQModel
from gptqmodel.models.auto import MODEL_MAP, SUPPORTED_MODELS
from gptqmodel.models._const import CPU
from huggingface_hub import snapshot_download

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å®šä¹‰GPTQæ¨¡å‹ç±»
class Qwen25OmniThinkerGPTQ(BaseGPTQModel):
    loader = Qwen2_5OmniForConditionalGeneration
    base_modules = [
        "thinker.model.embed_tokens", 
        "thinker.model.norm", 
        "token2wav", 
        "thinker.audio_tower", 
        "thinker.model.rotary_emb",
        "thinker.visual", 
        "talker"
    ]
    pre_lm_head_norm_module = "thinker.model.norm"
    require_monkeypatch = False
    layers_node = "thinker.model.layers"
    layer_type = "Qwen2_5OmniDecoderLayer"
    layer_modules = [
        ["self_attn.k_proj", "self_attn.v_proj", "self_attn.q_proj"],
        ["self_attn.o_proj"],
        ["mlp.up_proj", "mlp.gate_proj"],
        ["mlp.down_proj"],
    ]
   
    def pre_quantize_generate_hook_start(self):
        # å®šä¹‰move_toå‡½æ•°
        def move_to(tensor, device):
            if hasattr(tensor, 'to'):
                return tensor.to(device)
            return tensor
        
        self.thinker.visual = move_to(self.thinker.visual, device=self.quantize_config.device)
        self.thinker.audio_tower = move_to(self.thinker.audio_tower, device=self.quantize_config.device)

    def pre_quantize_generate_hook_end(self):
        # å®šä¹‰move_toå‡½æ•°
        def move_to(tensor, device):
            if hasattr(tensor, 'to'):
                return tensor.to(device)
            return tensor
        
        self.thinker.visual = move_to(self.thinker.visual, device=CPU)
        self.thinker.audio_tower = move_to(self.thinker.audio_tower, device=CPU)

    def preprocess_dataset(self, sample: Dict) -> Dict:
        return sample

# æ³¨å†ŒGPTQæ¨¡å‹
MODEL_MAP["qwen2_5_omni"] = Qwen25OmniThinkerGPTQ
SUPPORTED_MODELS.extend(["qwen2_5_omni"])

# å®šä¹‰speakerå­—å…¸åŠ è½½è¡¥ä¸
@classmethod
def patched_from_config(cls, config, *args, **kwargs):
    kwargs.pop("trust_remote_code", None)
    
    model = cls._from_config(config, **kwargs)
    
    # å°è¯•åŠ è½½speakerå­—å…¸
    try:
        spk_path = cached_file(
            config._name_or_path if hasattr(config, '_name_or_path') else "Qwen/Qwen2.5-Omni",
            "spk_dict.pt",
            subfolder=kwargs.pop("subfolder", None),
            cache_dir=kwargs.pop("cache_dir", None),
            force_download=kwargs.pop("force_download", False),
            proxies=kwargs.pop("proxies", None),
            resume_download=kwargs.pop("resume_download", None),
            local_files_only=kwargs.pop("local_files_only", False),
            token=kwargs.pop("use_auth_token", None),
            revision=kwargs.pop("revision", None),
        )
        if spk_path is not None:
            model.load_speakers(spk_path)
            print("âœ… Speakerå­—å…¸åŠ è½½æˆåŠŸ")
        else:
            print("âš ï¸ Speakerå­—å…¸æœªæ‰¾åˆ°ï¼Œè·³è¿‡åŠ è½½")
    except Exception as e:
        print(f"âš ï¸ Speakerå­—å…¸åŠ è½½å¤±è´¥: {e}")
    
    return model

# åº”ç”¨è¡¥ä¸
Qwen2_5OmniForConditionalGeneration.from_config = patched_from_config


class MultimodalProcessor:
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_path = os.getenv("MODEL_PATH", "/home/caden/models/Qwen2.5-Omni-3B")
        self.temp_files = []
        self.use_gptq = os.getenv("USE_GPTQ", "false").lower() == "true"
        self.gptq_model_path = os.getenv("GPTQ_MODEL_PATH", "")
        
    def _print_memory_usage(self, stage: str):
        """æ‰“å°ä¸åŒé˜¶æ®µçš„æ˜¾å­˜å ç”¨ç»Ÿè®¡"""
        if not torch.cuda.is_available():
            return
            
        print(f"\nğŸ” {stage} - æ˜¾å­˜å ç”¨ç»Ÿè®¡:")
        print(f"   å½“å‰æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"   æ˜¾å­˜ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
        print(f"   æ˜¾å­˜å³°å€¼: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
        
        # è·å–GPUä¿¡æ¯
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / 1024**3
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            free = total_memory - cached
            
            print(f"   GPU {i} ({props.name}):")
            print(f"     æ€»æ˜¾å­˜: {total_memory:.2f} GB")
            print(f"     å·²åˆ†é…: {allocated:.2f} GB")
            print(f"     å·²ç¼“å­˜: {cached:.2f} GB")
            print(f"     å¯ç”¨æ˜¾å­˜: {free:.2f} GB")
        print()
        
    def _analyze_module_memory_usage(self, stage: str):
        """åˆ†æQwen2.5-Omniæ¨¡å‹å„ä¸ªæ¨¡å—çš„æ˜¾å­˜/å†…å­˜å ç”¨"""
        if self.model is None:
            return
            
        print(f"\nğŸ” {stage} - æ¨¡å—çº§èµ„æºå ç”¨åˆ†æ:")
        
        # å®šä¹‰ä¸»è¦æ¨¡å—åŠå…¶è·¯å¾„
        main_modules = {
            "thinker": "thinker",
            "talker": "talker", 
            "token2wav": "token2wav",
            "thinker.model": "thinker.model",
            "thinker.visual": "thinker.visual",
            "thinker.audio_tower": "thinker.audio_tower",
            "thinker.lm_head": "thinker.lm_head"
        }
        
        total_params = 0
        total_trainable_params = 0
        
        for module_name, module_path in main_modules.items():
            try:
                # è·å–æ¨¡å—å¯¹è±¡
                module = self.model
                for attr in module_path.split('.'):
                    if hasattr(module, attr):
                        module = getattr(module, attr)
                    else:
                        module = None
                        break
                
                if module is not None:
                    # ç»Ÿè®¡å‚æ•°æ•°é‡
                    module_params = sum(p.numel() for p in module.parameters())
                    module_trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
                    
                    # ç»Ÿè®¡æ˜¾å­˜å ç”¨
                    module_gpu_memory = 0
                    module_cpu_memory = 0
                    
                    for param in module.parameters():
                        if param.device.type == 'cuda':
                            module_gpu_memory += param.numel() * param.element_size()
                        else:
                            module_cpu_memory += param.numel() * param.element_size()
                    
                    # ç»Ÿè®¡ç¼“å†²åŒºæ˜¾å­˜
                    for buffer in module.buffers():
                        if buffer.device.type == 'cuda':
                            module_gpu_memory += buffer.numel() * buffer.element_size()
                        else:
                            module_cpu_memory += buffer.numel() * buffer.element_size()
                    
                    total_params += module_params
                    total_trainable_params += module_trainable_params
                    
                    print(f"   ğŸ“Š {module_name}:")
                    print(f"      å‚æ•°æ•°é‡: {module_params:,} ({module_params/1e6:.2f}M)")
                    print(f"      å¯è®­ç»ƒå‚æ•°: {module_trainable_params:,} ({module_trainable_params/1e6:.2f}M)")
                    print(f"      GPUæ˜¾å­˜: {module_gpu_memory/1024**3:.3f} GB")
                    print(f"      CPUå†…å­˜: {module_cpu_memory/1024**3:.3f} GB")
                    
                    # æ˜¾ç¤ºè®¾å¤‡ä½ç½®
                    if hasattr(module, 'device'):
                        print(f"      è®¾å¤‡ä½ç½®: {module.device}")
                    else:
                        # å°è¯•ä»å‚æ•°æ¨æ–­è®¾å¤‡
                        device = next(module.parameters()).device if list(module.parameters()) else "unknown"
                        print(f"      è®¾å¤‡ä½ç½®: {device}")
                        
                else:
                    print(f"   âŒ {module_name}: æ¨¡å—ä¸å­˜åœ¨")
                    
            except Exception as e:
                print(f"   âŒ {module_name}: åˆ†æå¤±è´¥ - {e}")
        
        print(f"\n   ğŸ“ˆ æ€»è®¡:")
        print(f"      æ€»å‚æ•°: {total_params:,} ({total_params/1e6:.2f}M)")
        print(f"      å¯è®­ç»ƒå‚æ•°: {total_trainable_params:,} ({total_trainable_params/1e6:.2f}M)")
        
        # æ˜¾ç¤ºæ¨¡å‹æ€»æ˜¾å­˜å ç”¨
        if torch.cuda.is_available():
            model_gpu_memory = sum(p.numel() * p.element_size() for p in self.model.parameters() if p.device.type == 'cuda')
            model_gpu_memory += sum(b.numel() * b.element_size() for b in self.model.buffers() if b.device.type == 'cuda')
            print(f"      æ¨¡å‹æ€»GPUæ˜¾å­˜: {model_gpu_memory/1024**3:.3f} GB")
        
        print()
        
    def _monitor_inference_memory(self, modality_name: str):
        """ç›‘æ§æ¨ç†è¿‡ç¨‹ä¸­ç‰¹å®šæ¨¡æ€çš„æ˜¾å­˜å ç”¨"""
        class MemoryMonitor:
            def __init__(self, name, processor):
                self.name = name
                self.processor = processor
                self.start_memory = 0
                self.peak_memory = 0
                self.activation_memory = 0
                self.hooks = []
                
            def __enter__(self):
                if torch.cuda.is_available():
                    # è®°å½•å¼€å§‹æ—¶çš„æ˜¾å­˜çŠ¶æ€
                    torch.cuda.empty_cache()
                    self.start_memory = torch.cuda.memory_allocated()
                    self.peak_memory = self.start_memory
                    
                    # è®¾ç½®é’©å­æ¥ç›‘æ§æ¿€æ´»å€¼
                    self._setup_hooks()
                    
                    print(f"ğŸ” å¼€å§‹ç›‘æ§ {self.name} æ¨¡æ€æ¨ç†æ˜¾å­˜...")
                    print(f"   åˆå§‹æ˜¾å­˜: {self.start_memory / 1024**3:.3f} GB")
                
                return self
                
            def __exit__(self, exc_type, exc_val, exc_tb):
                if torch.cuda.is_available():
                    # ç§»é™¤é’©å­
                    self._remove_hooks()
                    
                    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
                    final_memory = torch.cuda.memory_allocated()
                    total_peak = torch.cuda.max_memory_allocated()
                    
                    # è®¡ç®—æ¿€æ´»æ˜¾å­˜ï¼ˆæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ˜¾å­˜ï¼‰
                    self.activation_memory = max(0, total_peak - self.start_memory)
                    
                    print(f"ğŸ” {self.name} æ¨¡æ€æ¨ç†å®Œæˆ - æ˜¾å­˜ç»Ÿè®¡:")
                    print(f"   åˆå§‹æ˜¾å­˜: {self.start_memory / 1024**3:.3f} GB")
                    print(f"   æœ€ç»ˆæ˜¾å­˜: {final_memory / 1024**3:.3f} GB")
                    print(f"   å³°å€¼æ˜¾å­˜: {total_peak / 1024**3:.3f} GB")
                    print(f"   æ¿€æ´»æ˜¾å­˜: {self.activation_memory / 1024**3:.3f} GB")
                    print(f"   æ¨ç†å¢é‡: {(final_memory - self.start_memory) / 1024**3:.3f} GB")
                    print()
                    
                    # é‡ç½®å³°å€¼ç»Ÿè®¡
                    torch.cuda.reset_peak_memory_stats()
                
            def _setup_hooks(self):
                """è®¾ç½®é’©å­æ¥ç›‘æ§æ¿€æ´»å€¼"""
                def forward_hook(module, input, output):
                    if torch.cuda.is_available():
                        current_memory = torch.cuda.memory_allocated()
                        self.peak_memory = max(self.peak_memory, current_memory)
                        
                        # è®¡ç®—æ¿€æ´»æ˜¾å­˜
                        if hasattr(output, 'numel'):
                            if hasattr(output, 'element_size'):
                                activation_size = output.numel() * output.element_size()
                            else:
                                activation_size = output.numel() * 4  # å‡è®¾float32
                            
                            if output.device.type == 'cuda':
                                self.activation_memory = max(self.activation_memory, activation_size)
                
                # ä¸ºå…³é”®æ¨¡å—æ·»åŠ é’©å­
                if hasattr(self.processor.model, 'thinker'):
                    if hasattr(self.processor.model.thinker, 'visual'):
                        self.hooks.append(self.processor.model.thinker.visual.register_forward_hook(forward_hook))
                    if hasattr(self.processor.model.thinker, 'audio_tower'):
                        self.hooks.append(self.processor.model.thinker.audio_tower.register_forward_hook(forward_hook))
                    if hasattr(self.processor.model.thinker, 'model'):
                        self.hooks.append(self.processor.model.thinker.model.register_forward_hook(forward_hook))
                
                if hasattr(self.processor.model, 'talker'):
                    self.hooks.append(self.processor.model.talker.register_forward_hook(forward_hook))
                
                if hasattr(self.processor.model, 'token2wav'):
                    self.hooks.append(self.processor.model.token2wav.register_forward_hook(forward_hook))
            
            def _remove_hooks(self):
                """ç§»é™¤æ‰€æœ‰é’©å­"""
                for hook in self.hooks:
                    hook.remove()
                self.hooks.clear()
        
        return MemoryMonitor(modality_name, self)
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        try:
            if self.use_gptq:
                print(f"ğŸš€ æ­£åœ¨åŠ è½½GPTQé‡åŒ–æ¨¡å‹: {self.gptq_model_path}")
                
                # å®šä¹‰è®¾å¤‡æ˜ å°„
                device_map = {
                    "thinker.model": "cuda", 
                    "thinker.lm_head": "cuda", 
                    "thinker.visual": "cpu",  
                    "thinker.audio_tower": "cpu",  
                    "talker": "cuda",  
                    "token2wav": "cuda",  
                }
                
                # åŠ è½½GPTQæ¨¡å‹
                self.model = GPTQModel.load(
                    self.gptq_model_path, 
                    device_map=device_map, 
                    torch_dtype=torch.float16,   
                    attn_implementation="flash_attention_2"
                )
                print("ğŸ“¦ GPTQæ¨¡å‹åŠ è½½å®Œæˆ")
                
                # ç»Ÿè®¡æ˜¾å­˜å ç”¨
                if torch.cuda.is_available():
                    self._print_memory_usage("GPTQæ¨¡å‹åŠ è½½å®Œæˆå")
                
                # åˆ†æå„æ¨¡å—èµ„æºå ç”¨
                self._analyze_module_memory_usage("GPTQæ¨¡å‹åŠ è½½å®Œæˆå")
                
                # åŠ è½½å¤„ç†å™¨
                self.processor = Qwen2_5OmniProcessor.from_pretrained(self.gptq_model_path)
                print("å¤„ç†å™¨åŠ è½½å®Œæˆ")
                
                return "âœ… GPTQé‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸ"
            else:
                print(f"æ­£åœ¨åŠ è½½æ ‡å‡†æ¨¡å‹: {self.model_path}")
                # æ ‡å‡†æ¨¡å¼ä½†å°è¯•ä¼˜åŒ–æ˜¾å­˜
                self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    device_map="auto"
                )
                print("ğŸ“¦ æ¨¡å‹åŠ è½½å®Œæˆ")

                # ç»Ÿè®¡æ˜¾å­˜å ç”¨
                if torch.cuda.is_available():
                    self._print_memory_usage("æ ‡å‡†æ¨¡å‹åŠ è½½å®Œæˆå")
                
                # åˆ†æå„æ¨¡å—èµ„æºå ç”¨
                self._analyze_module_memory_usage("æ ‡å‡†æ¨¡å‹åŠ è½½å®Œæˆå")
                
                self.processor = Qwen2_5OmniProcessor.from_pretrained(self.model_path)
                print("å¤„ç†å™¨åŠ è½½å®Œæˆ")
                return "âœ… æ ‡å‡†æ¨¡å‹åŠ è½½æˆåŠŸ"
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            print(error_msg)
            return error_msg

    def extract_video_features(self, video_path: str, extract_audio: bool = False, extract_frame: bool = False):
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘å’Œæœ€åä¸€å¸§"""
        features = {}
        
        if extract_audio:
            try:
                audio, sr = librosa.load(video_path, sr=16000)
                features['audio'] = audio
                print(f"æå–éŸ³é¢‘æˆåŠŸ: {len(audio)} samples at {sr}Hz")
            except Exception as e:
                print(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
        
        if extract_frame:
            try:
                cap = cv2.VideoCapture(video_path)
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count - 1)
                ret, frame = cap.read()
                
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    image = Image.fromarray(frame_rgb)
                    features['last_frame'] = image
                    print("æå–è§†é¢‘æœ€åä¸€å¸§æˆåŠŸ")
                else:
                    print("æå–å¸§å¤±è´¥")
                cap.release()
            except Exception as e:
                print(f"å¸§æå–å¤±è´¥: {e}")
        
        return features

    def process_multimodal(self, 
                          text_input: str,
                          image_input: Optional[Image.Image],
                          audio_input: Optional[str],
                          video_input: Optional[str],
                          system_prompt: str,
                          max_tokens: int,
                          extract_video_audio: bool,
                          extract_video_frame: bool,
                          using_mm_info_audio: bool,
                          enable_streaming: bool = False,
                          enable_audio_output: bool = False):
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        
        if self.model is None:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "", None, None, 0, 0, None
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        generated_audio = None
        
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            ]
            
            user_content = []
            
            # æ·»åŠ æ–‡æœ¬
            if text_input and text_input.strip():
                user_content.append({"type": "text", "text": text_input.strip()})
            
            # å¤„ç†è§†é¢‘
            if video_input:
                if extract_video_audio or extract_video_frame:
                    print(f"ğŸ¬ å¼€å§‹æå–è§†é¢‘ç‰¹å¾...")
                    features = self.extract_video_features(
                        video_input, 
                        extract_audio=extract_video_audio, 
                        extract_frame=extract_video_frame
                    )
                    
                    if 'audio' in features:
                        user_content.append({"type": "audio", "audio": features['audio']})
                        # ä¿å­˜æå–çš„éŸ³é¢‘ä¾›æ˜¾ç¤º
                        temp_audio_path = f"temp_extracted_audio_{int(time.time())}.wav"
                        sf.write(temp_audio_path, features['audio'], 16000)
                        extracted_audio = temp_audio_path
                        print(f"âœ… éŸ³é¢‘å·²æå–å¹¶ä¿å­˜: {temp_audio_path}")
                    
                    if 'last_frame' in features:
                        user_content.append({"type": "image", "image": features['last_frame']})
                        # ä¿å­˜æå–çš„å›¾åƒä¾›æ˜¾ç¤º
                        extracted_frame = features['last_frame']
                        print(f"âœ… å›¾åƒå·²æå–")
                else:
                    # add args to qwen-mm-info-utils
                    user_content.append({"type": "video", "video": video_input, "using_mm_info_audio": using_mm_info_audio})
            
            # å¤„ç†å›¾åƒ
            if image_input:
                user_content.append({"type": "image", "image": image_input})
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    user_content.append({"type": "audio", "audio": audio_data})
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œä½¿ç”¨é»˜è®¤
            if not user_content:
                user_content.append({"type": "text", "text": "Hello"})
            
            messages.append({"role": "user", "content": user_content})
            
            print(f"ğŸ“ æ„å»ºçš„æ¶ˆæ¯åŒ…å« {len(user_content)} ä¸ªå†…å®¹é¡¹")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            print(f"ğŸ“„ ç”Ÿæˆçš„prompté•¿åº¦: {len(text_prompt)} å­—ç¬¦")
            
            # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯
            audios, images, videos = process_mm_info(messages, use_audio_in_video=using_mm_info_audio)
            if audios and not audio_input:
                print("å¼€å¯ä½¿ç”¨mm_info_audioæå–çš„éŸ³é¢‘ä¸”æ²¡æœ‰éŸ³é¢‘è¾“å…¥ï¼Œå°†ä½¿ç”¨mm_info_audioçš„éŸ³é¢‘")
                messages.append({"role": "user", "content": [{"type": "audio", "audio": audios}]})
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†ç»“æœ: audios={len(audios) if audios else 0}, images={len(images) if images else 0}, videos={len(videos) if videos else 0}")
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_prompt, 
                audio=audios, 
                images=images, 
                videos=videos, 
                return_tensors="pt", 
                use_audio_in_video= not audios.is_empty(),
                padding=True
            )
            
            print(f"ğŸ”§ è¾“å…¥tensorå½¢çŠ¶: {[(k, v.shape if hasattr(v, 'shape') else type(v)) for k, v in inputs.items()]}")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†è¾“å…¥
            if self.use_gptq:
                # GPTQæ¨¡å‹ï¼šç›´æ¥å‘é€åˆ°CUDAå¹¶è½¬æ¢ä¸ºfloat16
                inputs = inputs.to('cuda').to(torch.float16)
                print("ğŸ”§ GPTQæ¨¡å‹è¾“å…¥å·²å‘é€åˆ°CUDAå¹¶è½¬æ¢ä¸ºfloat16")
            else:
                # æ ‡å‡†æ¨¡å‹ï¼šè·å–æ¨¡å‹è®¾å¤‡
                device = next(self.model.parameters()).device
                inputs = inputs.to(device).to(self.model.dtype)
                print(f"ğŸ”§ æ ‡å‡†æ¨¡å‹è¾“å…¥å·²å‘é€åˆ°è®¾å¤‡: {device}")
            
            print("ğŸš€ å¼€å§‹ç”Ÿæˆå›ç­”...")
            
            # æ ¹æ®è¾“å…¥æ¨¡æ€ç±»å‹ç¡®å®šç›‘æ§åç§°
            modality_types = []
            if text_input and text_input.strip():
                modality_types.append("æ–‡æœ¬")
            if image_input:
                modality_types.append("å›¾åƒ")
            if audio_input:
                modality_types.append("éŸ³é¢‘")
            if video_input:
                modality_types.append("è§†é¢‘")
            
            modality_name = "+".join(modality_types) if modality_types else "çº¯æ–‡æœ¬"
            
            if enable_streaming:
                # æµå¼ç”Ÿæˆ
                print("ğŸ“¡ ä½¿ç”¨æµå¼è¾“å‡º...")
                response_text = ""
                
                # è·å–è¾“å…¥é•¿åº¦ç”¨äºåç»­è¿‡æ»¤
                input_length = inputs['input_ids'].shape[1]
                
                with torch.no_grad():
                    # ä½¿ç”¨æµå¼ç”Ÿæˆ
                    from transformers import TextIteratorStreamer
                    import threading
                    
                    streamer = TextIteratorStreamer(
                        self.processor.tokenizer, 
                        skip_prompt=True,
                        skip_special_tokens=True,
                        clean_up_tokenization_spaces=False
                    )
                    
                    generation_kwargs = dict(
                        inputs=inputs,
                        streamer=streamer,
                        max_new_tokens=max_tokens,
                        do_sample=False,
                        use_audio_in_video=True,
                        return_audio=enable_audio_output,
                        pad_token_id=self.processor.tokenizer.eos_token_id
                    )
                    
                    # ä½¿ç”¨æ˜¾å­˜ç›‘æ§å™¨ç›‘æ§æ¨ç†è¿‡ç¨‹
                    with self._monitor_inference_memory(f"{modality_name}-æµå¼æ¨ç†"):
                        # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
                        thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
                        thread.start()
                        
                        # æµå¼è¯»å–ç”Ÿæˆçš„æ–‡æœ¬
                        for new_text in streamer:
                            response_text += new_text
                            # å®æ—¶æ›´æ–°å¯ä»¥åœ¨è¿™é‡Œå¤„ç†ï¼Œä½†gradioéœ€è¦ç‰¹æ®Šå¤„ç†
                        
                        thread.join()
                
                print(f"ğŸ“¡ æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(response_text)} å­—ç¬¦")
                
                # æµå¼æ¨¡å¼ä¸‹éŸ³é¢‘è¾“å‡ºå¤„ç†
                if enable_audio_output and self.use_gptq:
                    print("âš ï¸ GPTQæ¨¡å‹åœ¨æµå¼æ¨¡å¼ä¸‹éŸ³é¢‘è¾“å‡ºæš‚ä¸æ”¯æŒï¼Œè¯·ä½¿ç”¨æ ‡å‡†æ¨¡å¼")
                    generated_audio = None
                
            else:
                # æ ‡å‡†ç”Ÿæˆ
                with torch.no_grad():
                    # ä½¿ç”¨æ˜¾å­˜ç›‘æ§å™¨ç›‘æ§æ¨ç†è¿‡ç¨‹
                    with self._monitor_inference_memory(f"{modality_name}-æ ‡å‡†æ¨ç†"):
                        output = self.model.generate(
                            **inputs, 
                            max_new_tokens=max_tokens,
                            use_audio_in_video=True,
                            return_audio=enable_audio_output,  # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦è¿”å›éŸ³é¢‘
                            pad_token_id=self.processor.tokenizer.eos_token_id
                        )
                
                print(f"ğŸ“¤ ç”Ÿæˆè¾“å‡ºå½¢çŠ¶: {output[0].shape if enable_audio_output else output.shape}")
                
                # å¤„ç†éŸ³é¢‘è¾“å‡º - æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†
                if enable_audio_output:
                    try:
                        if self.use_gptq:
                            # GPTQæ¨¡å‹ï¼šæ£€æŸ¥output[2]æ˜¯å¦ä¸ºéŸ³é¢‘
                            if len(output) > 1 and output[1] is not None:
                                audio_data = output[1]
                                if hasattr(audio_data, 'reshape'):
                                    audio_data = audio_data.reshape(-1).detach().cpu().numpy()
                                else:
                                    audio_data = audio_data.cpu().numpy()
                                
                                audio_filename = f"generated_audio_gptq_{int(time.time())}.wav"
                                sf.write(audio_filename, audio_data, 24000)
                                generated_audio = audio_filename
                                print(f"ğŸµ GPTQéŸ³é¢‘å·²ç”Ÿæˆå¹¶ä¿å­˜: {audio_filename}")
                            else:
                                print("âš ï¸ GPTQæ¨¡å‹æœªè¿”å›éŸ³é¢‘")
                                generated_audio = None
                        else:
                            # æ ‡å‡†æ¨¡å‹ï¼šæ£€æŸ¥output.audio
                            if hasattr(output, 'audio') and output.audio is not None:
                                audio_filename = f"generated_audio_{int(time.time())}.wav"
                                sf.write(audio_filename, output.audio.cpu().numpy(), 24000)
                                generated_audio = audio_filename
                                print(f"ğŸµ æ ‡å‡†æ¨¡å‹éŸ³é¢‘å·²ç”Ÿæˆå¹¶ä¿å­˜: {audio_filename}")
                            else:
                                print("âš ï¸ æ ‡å‡†æ¨¡å‹æœªè¿”å›éŸ³é¢‘")
                                generated_audio = None
                    except Exception as e:
                        print(f"éŸ³é¢‘ä¿å­˜å¤±è´¥: {e}")
                        generated_audio = None
                
                # è§£ç å“åº” - æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†
                if self.use_gptq:
                    # GPTQæ¨¡å‹ï¼šoutput[0]æ˜¯æ–‡æœ¬è¾“å‡º
                    if len(output) > 1:
                        response_text = self.processor.batch_decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
                    elif len(output) == 1:
                        response_text = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
                    else:
                        response_text = "GPTQæ¨¡å‹è¾“å‡ºæ ¼å¼å¼‚å¸¸"
                else:
                    # æ ‡å‡†æ¨¡å‹ï¼šç›´æ¥è§£ç 
                    response_text = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
                
                print(f"ğŸ“ åŸå§‹å›ç­”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                
                # æå–å®é™…çš„å›ç­”éƒ¨åˆ† (å»æ‰è¾“å…¥çš„prompt)
                # æ‰¾åˆ°assistantå›ç­”çš„å¼€å§‹ä½ç½®
                if "<|im_start|>assistant" in response_text:
                    response_text = response_text.split("<|im_start|>assistant")[-1].strip()
                elif "assistant\n" in response_text:
                    response_text = response_text.split("assistant\n")[-1].strip()
                
                # æ¸…ç†ç»“æŸç¬¦å·
                if response_text.endswith("<|im_end|>"):
                    response_text = response_text[:-10].strip()
            
            print(f"âœ… æ¸…ç†åå›ç­”: {response_text[:100]}...")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
            # æ„å»ºè¾“å‡ºä¿¡æ¯
            info_text = f"""
ğŸ“Š **å¤„ç†ç»Ÿè®¡**
- â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’
- ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB
- ğŸ”¤ æœ€å¤§Tokenæ•°: {max_tokens}
- ğŸ“ è¾“å‡ºTokenæ•°: {len(output[0]) - len(inputs['input_ids'][0])}
- ğŸ¯ ç³»ç»Ÿæç¤º: {system_prompt[:50]}...

ğŸ“‹ **è¾“å…¥å†…å®¹**
- æ–‡æœ¬è¾“å…¥: {'âœ…' if text_input else 'âŒ'}
- å›¾åƒè¾“å…¥: {'âœ…' if image_input else 'âŒ'}  
- éŸ³é¢‘è¾“å…¥: {'âœ…' if audio_input else 'âŒ'}
- è§†é¢‘è¾“å…¥: {'âœ…' if video_input else 'âŒ'}
- æå–éŸ³è½¨: {'âœ…' if extract_video_audio and extracted_audio else 'âŒ'}
- æå–å¸§: {'âœ…' if extract_video_frame and extracted_frame else 'âŒ'}
            """
            
            # æ„å»ºè¯¦ç»†çš„å¤„ç†ä¿¡æ¯
            status_info = f"""âœ… å¤„ç†å®Œæˆ - {'æµå¼' if enable_streaming else 'æ ‡å‡†'}æ¨¡å¼
â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’
ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB"""

            return status_info, response_text, extracted_audio, extracted_frame, processing_time, peak_memory, generated_audio
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg, "", None, None, 0, 0, None

    def process_multimodal_streaming(self, 
                                   text_input: str,
                                   image_input: Optional[Image.Image],
                                   audio_input: Optional[str],
                                   video_input: Optional[str],
                                   system_prompt: str,
                                   max_tokens: int,
                                   extract_video_audio: bool,
                                   extract_video_frame: bool,
                                   using_mm_info_audio: bool,
                                   enable_audio_output: bool = False):
        """æµå¼å¤„ç†å¤šæ¨¡æ€è¾“å…¥ - ä½¿ç”¨ç”Ÿæˆå™¨è¿”å›é€æ­¥æ›´æ–°"""
        
        if self.model is None:
            yield "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "", None, None, 0, 0, None
            return
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        generated_audio = None
        
        try:
            # å‰æœŸå¤„ç† - å’Œæ™®é€šå¤„ç†ç›¸åŒ
            yield "ğŸ”„ å¼€å§‹å¤„ç†...", "", None, None, 0, 0, None
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            ]
            
            user_content = []
            
            # æ·»åŠ æ–‡æœ¬
            if text_input and text_input.strip():
                user_content.append({"type": "text", "text": text_input.strip()})
            
            # å¤„ç†è§†é¢‘
            if video_input:
                if extract_video_audio or extract_video_frame:
                    yield "ğŸ¬ æå–è§†é¢‘ç‰¹å¾...", "", None, None, 0, 0, None
                    
                    features = self.extract_video_features(
                        video_input, 
                        extract_audio=extract_video_audio, 
                        extract_frame=extract_video_frame
                    )
                    
                    if 'audio' in features:
                        user_content.append({"type": "audio", "audio": features['audio']})
                        temp_audio_path = f"temp_extracted_audio_{int(time.time())}.wav"
                        sf.write(temp_audio_path, features['audio'], 16000)
                        extracted_audio = temp_audio_path
                    
                    if 'last_frame' in features:
                        user_content.append({"type": "image", "image": features['last_frame']})
                        extracted_frame = features['last_frame']
                        
                    yield "âœ… è§†é¢‘ç‰¹å¾æå–å®Œæˆ", "", extracted_audio, extracted_frame, 0, 0, None
                else:
                    user_content.append({"type": "video", "video": video_input})
            
            # å¤„ç†å›¾åƒ
            if image_input:
                user_content.append({"type": "image", "image": image_input})
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    user_content.append({"type": "audio", "audio": audio_data})
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            if not user_content:
                user_content.append({"type": "text", "text": "Hello"})
            
            messages.append({"role": "user", "content": user_content})
            
            yield "ğŸ“ æ„å»ºå¤šæ¨¡æ€è¾“å…¥...", "", extracted_audio, extracted_frame, 0, 0, None
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            
            # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯
            audios, images, videos = process_mm_info(messages, use_audio_in_video=using_mm_info_audio)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_prompt, 
                audio=audios, 
                images=images, 
                videos=videos, 
                return_tensors="pt", 
                padding=True
            )
            
            device = next(self.model.parameters()).device
            inputs = inputs.to(device).to(self.model.dtype)
            
            yield "ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ...", "", extracted_audio, extracted_frame, 0, 0, None
            
            # æµå¼ç”Ÿæˆ
            from transformers import TextIteratorStreamer
            import threading
            
            streamer = TextIteratorStreamer(
                self.processor.tokenizer, 
                skip_prompt=True,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )
            
            generation_kwargs = dict(
                **inputs,
                streamer=streamer,
                max_new_tokens=max_tokens,
                do_sample=False,
                use_audio_in_video=True,
                return_audio=enable_audio_output,
                pad_token_id=self.processor.tokenizer.eos_token_id
            )
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
            thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # æµå¼è¾“å‡º
            response_text = ""
            for new_text in streamer:
                if new_text.strip():  # å¿½ç•¥ç©ºç™½token
                    response_text += new_text
                    processing_time = time.time() - start_time
                    status = f"ğŸ“¡ æµå¼ç”Ÿæˆä¸­... ({processing_time:.1f}s)"
                    yield status, response_text, extracted_audio, extracted_frame, processing_time, 0, None
            
            thread.join()
            
            # å¤„ç†éŸ³é¢‘è¾“å‡ºï¼ˆæµå¼æ¨¡å¼ä¸‹éŸ³é¢‘åœ¨æœ€åç”Ÿæˆï¼‰
            if enable_audio_output:
                try:
                    # è¿™é‡Œéœ€è¦é‡æ–°ç”Ÿæˆä¸€æ¬¡æ¥è·å–éŸ³é¢‘ï¼Œæˆ–è€…ä¿®æ”¹æµå¼é€»è¾‘
                    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶åœ¨æµå¼æ¨¡å¼ä¸‹ä¸è¿”å›éŸ³é¢‘
                    generated_audio = None
                    print("ğŸ“¡ æµå¼æ¨¡å¼ä¸‹éŸ³é¢‘è¾“å‡ºæš‚ä¸æ”¯æŒ")
                except Exception as e:
                    print(f"æµå¼éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
                    generated_audio = None
            
            # æœ€ç»ˆç»“æœ
            processing_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
            final_status = f"""âœ… æµå¼ç”Ÿæˆå®Œæˆ!
â±ï¸ æ€»æ—¶é—´: {processing_time:.2f}ç§’
ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB
ğŸ“ è¾“å‡ºé•¿åº¦: {len(response_text)} å­—ç¬¦"""
            
            yield final_status, response_text, extracted_audio, extracted_frame, processing_time, peak_memory, generated_audio
            
        except Exception as e:
            error_msg = f"âŒ æµå¼å¤„ç†å¤±è´¥: {str(e)}"
            yield error_msg, "", extracted_audio, extracted_frame, 0, 0, None


# åˆ›å»ºå¤„ç†å™¨å®ä¾‹
processor = MultimodalProcessor()

# æ„å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="Qwen2.5-Omni å¤šæ¨¡æ€åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¤– Qwen2.5-Omni å¤šæ¨¡æ€æ™ºèƒ½åŠ©æ‰‹
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ›ï¸ æ¨¡å‹æ§åˆ¶")
                
                # æ·»åŠ æ¨¡å‹ç±»å‹é€‰æ‹©
                model_type = gr.Radio(
                    choices=["æ ‡å‡†æ¨¡å‹", "GPTQé‡åŒ–æ¨¡å‹"],
                    value="æ ‡å‡†æ¨¡å‹",
                    label="ğŸš€ æ¨¡å‹ç±»å‹",
                    info="é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ç±»å‹"
                )
                
                # æ·»åŠ GPTQæ¨¡å‹è·¯å¾„è¾“å…¥
                gptq_model_path = gr.Textbox(
                    label="ğŸ”— GPTQæ¨¡å‹è·¯å¾„",
                    value="/home/caden/workplace/models/Qwen2.5-Omni-7B-GPTQ-Int4",
                    placeholder="è¾“å…¥GPTQæ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹ID",
                    visible=False,
                    info="GPTQé‡åŒ–æ¨¡å‹çš„è·¯å¾„æˆ–HuggingFaceæ¨¡å‹ID"
                )
                
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                model_status = gr.Textbox(
                    label="æ¨¡å‹çŠ¶æ€", 
                    value="â³ æœªåŠ è½½", 
                    interactive=False
                )
                
                gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
                system_prompt = gr.Textbox(
                    label="ç³»ç»Ÿæç¤º",
                    value="You are a helpful AI assistant.",
                    lines=2
                )
                max_tokens = gr.Slider(
                    minimum=50,
                    maximum=2048,
                    value=512,
                    step=50,
                    label="æœ€å¤§Tokenæ•°"
                )
                
                gr.Markdown("### ğŸ¬ è§†é¢‘å¤„ç†é€‰é¡¹")
                extract_video_audio = gr.Checkbox(
                    label="ğŸ“¢ æå–è§†é¢‘éŸ³è½¨",
                    value=False,
                    info="å°†è§†é¢‘éŸ³è½¨æå–ä¸ºéŸ³é¢‘è¾“å…¥"
                )
                extract_video_frame = gr.Checkbox(
                    label="ğŸ–¼ï¸ æå–è§†é¢‘æœ€åä¸€å¸§",
                    value=False,
                    info="å°†è§†é¢‘æœ€åä¸€å¸§æå–ä¸ºå›¾åƒè¾“å…¥"
                )
                using_mm_info_audio = gr.Checkbox(
                    label="ğŸµ ä½¿ç”¨mm_infoæå–éŸ³é¢‘",
                    value=False,
                    info="ä½¿ç”¨mm_infoæå–éŸ³é¢‘"
                )
                
                gr.Markdown("### âš¡ è¾“å‡ºæ¨¡å¼")
                enable_streaming = gr.Checkbox(
                    label="ğŸ“¡ å¯ç”¨æµå¼è¾“å‡º",
                    value=False,
                    info="å®æ—¶é€æ­¥æ˜¾ç¤ºç”Ÿæˆå†…å®¹ï¼Œæå‡äº¤äº’ä½“éªŒ"
                )
                enable_audio_output = gr.Checkbox(
                    label="ğŸµ å¯ç”¨è¯­éŸ³è¾“å‡º",
                    value=False,
                    info="ç”Ÿæˆè¯­éŸ³å›ç­”ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰"
                )
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ å¤šæ¨¡æ€è¾“å…¥")
                
                text_input = gr.Textbox(
                    label="ğŸ’¬ æ–‡æœ¬è¾“å…¥",
                    placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æŒ‡ä»¤...",
                    lines=3
                )
                
                with gr.Row():
                    with gr.Column():
                        image_input = gr.Image(
                            label="ğŸ–¼ï¸ å›¾åƒè¾“å…¥",
                            type="pil"
                        )
                        
                        audio_input = gr.Audio(
                            label="ğŸµ éŸ³é¢‘è¾“å…¥",
                            type="filepath"
                        )
                    
                    with gr.Column():
                        video_input = gr.Video(
                            label="ğŸ¬ è§†é¢‘è¾“å…¥"
                        )
                
                with gr.Row():
                    process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                    stream_btn = gr.Button("ğŸ“¡ æµå¼å¤„ç†", variant="secondary", size="lg", visible=False)
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ ç”Ÿæˆç»“æœ")
                output_text = gr.Textbox(
                    label="AIå›ç­”",
                    lines=8,
                    placeholder="ç”Ÿæˆçš„å›ç­”å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                    interactive=False
                )
                
                # æ˜¾ç¤ºæå–çš„å†…å®¹
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸµ æå–çš„éŸ³é¢‘")
                        extracted_audio_display = gr.Audio(
                            label="ä»è§†é¢‘æå–çš„éŸ³é¢‘",
                            visible=True,
                            interactive=False
                        )
                    
                    with gr.Column():
                        gr.Markdown("### ğŸ–¼ï¸ æå–çš„å›¾åƒ")
                        extracted_image_display = gr.Image(
                            label="ä»è§†é¢‘æå–çš„æœ€åä¸€å¸§",
                            type="pil",
                            visible=True,
                            interactive=False
                        )
                
                # æ˜¾ç¤ºç”Ÿæˆçš„éŸ³é¢‘è¾“å‡º
                gr.Markdown("### ğŸ¤ ç”Ÿæˆçš„è¯­éŸ³å›ç­”")
                generated_audio_display = gr.Audio(
                    label="AIç”Ÿæˆçš„è¯­éŸ³å›ç­”",
                    visible=True,
                    interactive=False
                )
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š å¤„ç†ä¿¡æ¯")
                processing_info = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    lines=8,
                    interactive=False,
                    value="ç­‰å¾…å¤„ç†..."
                )
        
        
        # äº‹ä»¶ç»‘å®š
        
        # æ¨¡å‹ç±»å‹é€‰æ‹©æ§åˆ¶GPTQæ¨¡å‹è·¯å¾„æ˜¾ç¤º
        def update_gptq_path_visibility(model_type):
            if model_type == "GPTQé‡åŒ–æ¨¡å‹":
                return gr.update(visible=True)
            else:
                return gr.update(visible=False)
        
        model_type.change(
            fn=update_gptq_path_visibility,
            inputs=[model_type],
            outputs=[gptq_model_path]
        )
        
        # ä¿®æ”¹åŠ è½½æ¨¡å‹å‡½æ•°ä»¥æ”¯æŒæ¨¡å‹ç±»å‹é€‰æ‹©
        def load_model_with_type(model_type, gptq_path):
            if model_type == "GPTQé‡åŒ–æ¨¡å‹":
                # è®¾ç½®GPTQæ¨¡å‹è·¯å¾„
                processor.gptq_model_path = gptq_path
                processor.use_gptq = True
                print(f"ğŸš€ å‡†å¤‡åŠ è½½GPTQæ¨¡å‹: {gptq_path}")
            else:
                # ä½¿ç”¨æ ‡å‡†æ¨¡å‹
                processor.use_gptq = False
                print("ğŸš€ å‡†å¤‡åŠ è½½æ ‡å‡†æ¨¡å‹")
            
            return processor.load_model()
        
        load_btn.click(
            fn=load_model_with_type,
            inputs=[model_type, gptq_model_path],
            outputs=model_status
        )
        
        def handle_process_standard(text_input, image_input, audio_input, video_input, system_prompt, max_tokens, 
                                   extract_video_audio, extract_video_frame, using_mm_info_audio, enable_streaming, enable_audio_output):
            """æ ‡å‡†å¤„ç†å‡½æ•°"""
            if enable_streaming:
                # å¦‚æœå¯ç”¨æµå¼ï¼Œç»™å‡ºæç¤º
                return "ğŸ“¡ æµå¼æ¨¡å¼ï¼šè¯·ç‚¹å‡»ä¸‹é¢çš„æµå¼å¤„ç†æŒ‰é’®", "", None, None, None
            else:
                # ä½¿ç”¨æ ‡å‡†å¤„ç†
                result = processor.process_multimodal(
                    text_input, image_input, audio_input, video_input,
                    system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, False, enable_audio_output
                )
                return result[0], result[1], result[2], result[3], result[6]  # status, text, audio, image, generated_audio
        
        def handle_process_streaming(text_input, image_input, audio_input, video_input, system_prompt, max_tokens, 
                                   extract_video_audio, extract_video_frame, using_mm_info_audio, enable_audio_output):
            """æµå¼å¤„ç†å‡½æ•°"""
            for status, text, audio, image, time, memory, generated_audio in processor.process_multimodal_streaming(
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, enable_audio_output
            ):
                yield status, text, audio, image, generated_audio
        
        # æ ‡å‡†å¤„ç†æŒ‰é’®
        process_btn.click(
            fn=handle_process_standard,
            inputs=[
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, enable_streaming, enable_audio_output
            ],
            outputs=[processing_info, output_text, extracted_audio_display, extracted_image_display, generated_audio_display]
        )
        
        # æµå¼æŒ‰é’®å·²åœ¨ä¸Šé¢å®šä¹‰
        
        stream_btn.click(
            fn=handle_process_streaming,
            inputs=[
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, using_mm_info_audio, enable_audio_output
            ],
            outputs=[processing_info, output_text, extracted_audio_display, extracted_image_display, generated_audio_display]
        )
        
        # æ ¹æ®æµå¼å¼€å…³æ§åˆ¶æŒ‰é’®æ˜¾ç¤º
        def update_buttons(enable_streaming):
            if enable_streaming:
                return gr.update(value="ğŸš€ æ ‡å‡†å¤„ç†"), gr.update(visible=True)
            else:
                return gr.update(value="ğŸš€ å¼€å§‹å¤„ç†"), gr.update(visible=False)
        
        enable_streaming.change(
            fn=update_buttons,
            inputs=[enable_streaming],
            outputs=[process_btn, stream_btn]
        )
        
        def clear_all():
            return "", None, None, None, "", "ç­‰å¾…å¤„ç†...", None, None, False, None
        
        clear_btn.click(
            fn=clear_all,
            outputs=[text_input, image_input, audio_input, video_input, output_text, processing_info, extracted_audio_display, extracted_image_display, enable_streaming, generated_audio_display]
        )
    
    return demo


if __name__ == "__main__":
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        debug=True,
        show_error=True
    )