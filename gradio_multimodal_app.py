#!/usr/bin/env python3
"""
Qwen2.5-Omni å¤šæ¨¡æ€Gradioç•Œé¢
æ”¯æŒè§†é¢‘ã€è¯­éŸ³ã€å›¾åƒã€æ–‡æœ¬ç­‰ä¸åŒæ¨¡æ€çš„ç»„åˆè¾“å…¥
"""

import os
import time
import logging
from typing import List, Optional, Tuple, Dict, Any
import tempfile

import torch
import numpy as np
import librosa
import cv2
from PIL import Image
import gradio as gr
from dotenv import load_dotenv
import soundfile as sf

# å¯¼å…¥qwen-omni-utils  
from qwen_omni_utils import process_mm_info
from transformers import Qwen2_5OmniProcessor


try:
    from transformers import Qwen2_5OmniForConditionalGeneration
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
    raise

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultimodalProcessor:
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_path = os.getenv("MODEL_PATH", "/home/caden/models/Qwen2.5-Omni-3B")
        self.temp_files = []
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        try:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
            # æ ‡å‡†æ¨¡å¼ä½†å°è¯•ä¼˜åŒ–æ˜¾å­˜
            self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                device_map="auto"
            )
            print("ğŸ“¦ æ¨¡å‹åŠ è½½å®Œæˆ")

            self.processor = Qwen2_5OmniProcessor.from_pretrained(self.model_path)
            print("å¤„ç†å™¨åŠ è½½å®Œæˆ")
            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ"
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            print(error_msg)
            return error_msg

    def extract_video_features(self, video_path: str, extract_audio: bool = False, extract_frame: bool = False):
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘å’Œæœ€åä¸€å¸§"""
        features = {}
        
        if extract_audio:
            try:
                audio, sr = librosa.load(video_path, sr=16000)
                features['audio'] = audio
                print(f"æå–éŸ³é¢‘æˆåŠŸ: {len(audio)} samples at {sr}Hz")
            except Exception as e:
                print(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
        
        if extract_frame:
            try:
                cap = cv2.VideoCapture(video_path)
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count - 1)
                ret, frame = cap.read()
                
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    image = Image.fromarray(frame_rgb)
                    features['last_frame'] = image
                    print("æå–è§†é¢‘æœ€åä¸€å¸§æˆåŠŸ")
                else:
                    print("æå–å¸§å¤±è´¥")
                cap.release()
            except Exception as e:
                print(f"å¸§æå–å¤±è´¥: {e}")
        
        return features

    def process_multimodal(self, 
                          text_input: str,
                          image_input: Optional[Image.Image],
                          audio_input: Optional[str],
                          video_input: Optional[str],
                          system_prompt: str,
                          max_tokens: int,
                          extract_video_audio: bool,
                          extract_video_frame: bool,
                          enable_streaming: bool = False):
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        
        if self.model is None:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "", None, None, 0, 0
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            ]
            
            user_content = []
            
            # æ·»åŠ æ–‡æœ¬
            if text_input and text_input.strip():
                user_content.append({"type": "text", "text": text_input.strip()})
            
            # å¤„ç†è§†é¢‘
            if video_input:
                if extract_video_audio or extract_video_frame:
                    print(f"ğŸ¬ å¼€å§‹æå–è§†é¢‘ç‰¹å¾...")
                    features = self.extract_video_features(
                        video_input, 
                        extract_audio=extract_video_audio, 
                        extract_frame=extract_video_frame
                    )
                    
                    if 'audio' in features:
                        user_content.append({"type": "audio", "audio": features['audio']})
                        # ä¿å­˜æå–çš„éŸ³é¢‘ä¾›æ˜¾ç¤º
                        temp_audio_path = f"temp_extracted_audio_{int(time.time())}.wav"
                        sf.write(temp_audio_path, features['audio'], 16000)
                        extracted_audio = temp_audio_path
                        print(f"âœ… éŸ³é¢‘å·²æå–å¹¶ä¿å­˜: {temp_audio_path}")
                    
                    if 'last_frame' in features:
                        user_content.append({"type": "image", "image": features['last_frame']})
                        # ä¿å­˜æå–çš„å›¾åƒä¾›æ˜¾ç¤º
                        extracted_frame = features['last_frame']
                        print(f"âœ… å›¾åƒå·²æå–")
                else:
                    user_content.append({"type": "video", "video": video_input})
            
            # å¤„ç†å›¾åƒ
            if image_input:
                user_content.append({"type": "image", "image": image_input})
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    user_content.append({"type": "audio", "audio": audio_data})
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œä½¿ç”¨é»˜è®¤
            if not user_content:
                user_content.append({"type": "text", "text": "Hello"})
            
            messages.append({"role": "user", "content": user_content})
            
            print(f"ğŸ“ æ„å»ºçš„æ¶ˆæ¯åŒ…å« {len(user_content)} ä¸ªå†…å®¹é¡¹")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            print(f"ğŸ“„ ç”Ÿæˆçš„prompté•¿åº¦: {len(text_prompt)} å­—ç¬¦")
            
            # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯
            audios, images, videos = process_mm_info(messages, use_audio_in_video=True)
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†ç»“æœ: audios={len(audios) if audios else 0}, images={len(images) if images else 0}, videos={len(videos) if videos else 0}")
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_prompt, 
                audio=audios, 
                images=images, 
                videos=videos, 
                return_tensors="pt", 
                padding=True
            )
            
            print(f"ğŸ”§ è¾“å…¥tensorå½¢çŠ¶: {[(k, v.shape if hasattr(v, 'shape') else type(v)) for k, v in inputs.items()]}")
            
            # æ ‡å‡†æ¨¡å¼ï¼šè·å–æ¨¡å‹è®¾å¤‡
            device = next(self.model.parameters()).device
            inputs = inputs.to(device).to(self.model.dtype)
            
            print("ğŸš€ å¼€å§‹ç”Ÿæˆå›ç­”...")
            
            if enable_streaming:
                # æµå¼ç”Ÿæˆ
                print("ğŸ“¡ ä½¿ç”¨æµå¼è¾“å‡º...")
                response_text = ""
                
                # è·å–è¾“å…¥é•¿åº¦ç”¨äºåç»­è¿‡æ»¤
                input_length = inputs['input_ids'].shape[1]
                
                with torch.no_grad():
                    # ä½¿ç”¨æµå¼ç”Ÿæˆ
                    from transformers import TextIteratorStreamer
                    import threading
                    
                    streamer = TextIteratorStreamer(
                        self.processor.tokenizer, 
                        skip_prompt=True,
                        skip_special_tokens=True,
                        clean_up_tokenization_spaces=False
                    )
                    
                    generation_kwargs = dict(
                        inputs=inputs,
                        streamer=streamer,
                        max_new_tokens=max_tokens,
                        do_sample=False,
                        use_audio_in_video=True,
                        return_audio=False,
                        pad_token_id=self.processor.tokenizer.eos_token_id
                    )
                    
                    # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
                    thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
                    thread.start()
                    
                    # æµå¼è¯»å–ç”Ÿæˆçš„æ–‡æœ¬
                    for new_text in streamer:
                        response_text += new_text
                        # å®æ—¶æ›´æ–°å¯ä»¥åœ¨è¿™é‡Œå¤„ç†ï¼Œä½†gradioéœ€è¦ç‰¹æ®Šå¤„ç†
                    
                    thread.join()
                
                print(f"ğŸ“¡ æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(response_text)} å­—ç¬¦")
                
            else:
                # æ ‡å‡†ç”Ÿæˆ
                with torch.no_grad():
                    output = self.model.generate(
                        **inputs, 
                        max_new_tokens=max_tokens,
                        do_sample=False,  # ä½¿ç”¨è´ªå¿ƒè§£ç 
                        use_audio_in_video=True,
                        return_audio=False,  # ç¦ç”¨éŸ³é¢‘è¾“å‡º
                        pad_token_id=self.processor.tokenizer.eos_token_id
                    )
                
                print(f"ğŸ“¤ ç”Ÿæˆè¾“å‡ºå½¢çŠ¶: {output.shape}")
                
                # è§£ç å“åº”
                response_text = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
                
                print(f"ğŸ“ åŸå§‹å›ç­”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                
                # æå–å®é™…çš„å›ç­”éƒ¨åˆ† (å»æ‰è¾“å…¥çš„prompt)
                # æ‰¾åˆ°assistantå›ç­”çš„å¼€å§‹ä½ç½®
                if "<|im_start|>assistant" in response_text:
                    response_text = response_text.split("<|im_start|>assistant")[-1].strip()
                elif "assistant\n" in response_text:
                    response_text = response_text.split("assistant\n")[-1].strip()
                
                # æ¸…ç†ç»“æŸç¬¦å·
                if response_text.endswith("<|im_end|>"):
                    response_text = response_text[:-10].strip()
            
            print(f"âœ… æ¸…ç†åå›ç­”: {response_text[:100]}...")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
            # æ„å»ºè¾“å‡ºä¿¡æ¯
            info_text = f"""
ğŸ“Š **å¤„ç†ç»Ÿè®¡**
- â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’
- ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB
- ğŸ”¤ æœ€å¤§Tokenæ•°: {max_tokens}
- ğŸ“ è¾“å‡ºTokenæ•°: {len(output[0]) - len(inputs['input_ids'][0])}
- ğŸ¯ ç³»ç»Ÿæç¤º: {system_prompt[:50]}...

ğŸ“‹ **è¾“å…¥å†…å®¹**
- æ–‡æœ¬è¾“å…¥: {'âœ…' if text_input else 'âŒ'}
- å›¾åƒè¾“å…¥: {'âœ…' if image_input else 'âŒ'}  
- éŸ³é¢‘è¾“å…¥: {'âœ…' if audio_input else 'âŒ'}
- è§†é¢‘è¾“å…¥: {'âœ…' if video_input else 'âŒ'}
- æå–éŸ³è½¨: {'âœ…' if extract_video_audio and extracted_audio else 'âŒ'}
- æå–å¸§: {'âœ…' if extract_video_frame and extracted_frame else 'âŒ'}
            """
            
            # æ„å»ºè¯¦ç»†çš„å¤„ç†ä¿¡æ¯
            status_info = f"""âœ… å¤„ç†å®Œæˆ - {'æµå¼' if enable_streaming else 'æ ‡å‡†'}æ¨¡å¼
â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’
ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB"""

            return status_info, response_text, extracted_audio, extracted_frame, processing_time, peak_memory
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg, "", None, None, 0, 0

    def process_multimodal_streaming(self, 
                                   text_input: str,
                                   image_input: Optional[Image.Image],
                                   audio_input: Optional[str],
                                   video_input: Optional[str],
                                   system_prompt: str,
                                   max_tokens: int,
                                   extract_video_audio: bool,
                                   extract_video_frame: bool):
        """æµå¼å¤„ç†å¤šæ¨¡æ€è¾“å…¥ - ä½¿ç”¨ç”Ÿæˆå™¨è¿”å›é€æ­¥æ›´æ–°"""
        
        if self.model is None:
            yield "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "", None, None
            return
        
        start_time = time.time()
        torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None
        
        extracted_audio = None
        extracted_frame = None
        
        try:
            # å‰æœŸå¤„ç† - å’Œæ™®é€šå¤„ç†ç›¸åŒ
            yield "ğŸ”„ å¼€å§‹å¤„ç†...", "", None, None
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            ]
            
            user_content = []
            
            # æ·»åŠ æ–‡æœ¬
            if text_input and text_input.strip():
                user_content.append({"type": "text", "text": text_input.strip()})
            
            # å¤„ç†è§†é¢‘
            if video_input:
                if extract_video_audio or extract_video_frame:
                    yield "ğŸ¬ æå–è§†é¢‘ç‰¹å¾...", "", None, None
                    
                    features = self.extract_video_features(
                        video_input, 
                        extract_audio=extract_video_audio, 
                        extract_frame=extract_video_frame
                    )
                    
                    if 'audio' in features:
                        user_content.append({"type": "audio", "audio": features['audio']})
                        temp_audio_path = f"temp_extracted_audio_{int(time.time())}.wav"
                        sf.write(temp_audio_path, features['audio'], 16000)
                        extracted_audio = temp_audio_path
                    
                    if 'last_frame' in features:
                        user_content.append({"type": "image", "image": features['last_frame']})
                        extracted_frame = features['last_frame']
                        
                    yield "âœ… è§†é¢‘ç‰¹å¾æå–å®Œæˆ", "", extracted_audio, extracted_frame
                else:
                    user_content.append({"type": "video", "video": video_input})
            
            # å¤„ç†å›¾åƒ
            if image_input:
                user_content.append({"type": "image", "image": image_input})
            
            # å¤„ç†éŸ³é¢‘
            if audio_input:
                try:
                    audio_data, _ = librosa.load(audio_input, sr=16000)
                    user_content.append({"type": "audio", "audio": audio_data})
                except Exception as e:
                    print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            
            if not user_content:
                user_content.append({"type": "text", "text": "Hello"})
            
            messages.append({"role": "user", "content": user_content})
            
            yield "ğŸ“ æ„å»ºå¤šæ¨¡æ€è¾“å…¥...", "", extracted_audio, extracted_frame
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            
            # å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯
            audios, images, videos = process_mm_info(messages, use_audio_in_video=True)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_prompt, 
                audio=audios, 
                images=images, 
                videos=videos, 
                return_tensors="pt", 
                padding=True
            )
            
            device = next(self.model.parameters()).device
            inputs = inputs.to(device).to(self.model.dtype)
            
            yield "ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ...", "", extracted_audio, extracted_frame
            
            # æµå¼ç”Ÿæˆ
            from transformers import TextIteratorStreamer
            import threading
            
            streamer = TextIteratorStreamer(
                self.processor.tokenizer, 
                skip_prompt=True,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )
            
            generation_kwargs = dict(
                **inputs,
                streamer=streamer,
                max_new_tokens=max_tokens,
                do_sample=False,
                use_audio_in_video=True,
                return_audio=False,
                pad_token_id=self.processor.tokenizer.eos_token_id
            )
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
            thread = threading.Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # æµå¼è¾“å‡º
            response_text = ""
            for new_text in streamer:
                if new_text.strip():  # å¿½ç•¥ç©ºç™½token
                    response_text += new_text
                    processing_time = time.time() - start_time
                    status = f"ğŸ“¡ æµå¼ç”Ÿæˆä¸­... ({processing_time:.1f}s)"
                    yield status, response_text, extracted_audio, extracted_frame
            
            thread.join()
            
            # æœ€ç»ˆç»“æœ
            processing_time = time.time() - start_time
            peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0
            
            final_status = f"""âœ… æµå¼ç”Ÿæˆå®Œæˆ!
â±ï¸ æ€»æ—¶é—´: {processing_time:.2f}ç§’
ğŸ’¾ å³°å€¼æ˜¾å­˜: {peak_memory:.1f}MB
ğŸ“ è¾“å‡ºé•¿åº¦: {len(response_text)} å­—ç¬¦"""
            
            yield final_status, response_text, extracted_audio, extracted_frame
            
        except Exception as e:
            error_msg = f"âŒ æµå¼å¤„ç†å¤±è´¥: {str(e)}"
            yield error_msg, "", extracted_audio, extracted_frame


# åˆ›å»ºå¤„ç†å™¨å®ä¾‹
processor = MultimodalProcessor()

# æ„å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="Qwen2.5-Omni å¤šæ¨¡æ€åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¤– Qwen2.5-Omni å¤šæ¨¡æ€æ™ºèƒ½åŠ©æ‰‹
        
        æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘çš„å¤šç§ç»„åˆè¾“å…¥ï¼Œæ™ºèƒ½ç”Ÿæˆå›ç­”ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ›ï¸ æ¨¡å‹æ§åˆ¶")
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                model_status = gr.Textbox(
                    label="æ¨¡å‹çŠ¶æ€", 
                    value="â³ æœªåŠ è½½", 
                    interactive=False
                )
                
                gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
                system_prompt = gr.Textbox(
                    label="ç³»ç»Ÿæç¤º",
                    value="You are a helpful AI assistant.",
                    lines=2
                )
                max_tokens = gr.Slider(
                    minimum=50,
                    maximum=2048,
                    value=512,
                    step=50,
                    label="æœ€å¤§Tokenæ•°"
                )
                
                gr.Markdown("### ğŸ¬ è§†é¢‘å¤„ç†é€‰é¡¹")
                extract_video_audio = gr.Checkbox(
                    label="ğŸ“¢ æå–è§†é¢‘éŸ³è½¨",
                    value=False,
                    info="å°†è§†é¢‘éŸ³è½¨æå–ä¸ºéŸ³é¢‘è¾“å…¥"
                )
                extract_video_frame = gr.Checkbox(
                    label="ğŸ–¼ï¸ æå–è§†é¢‘æœ€åä¸€å¸§",
                    value=False,
                    info="å°†è§†é¢‘æœ€åä¸€å¸§æå–ä¸ºå›¾åƒè¾“å…¥"
                )
                
                gr.Markdown("### âš¡ è¾“å‡ºæ¨¡å¼")
                enable_streaming = gr.Checkbox(
                    label="ğŸ“¡ å¯ç”¨æµå¼è¾“å‡º",
                    value=False,
                    info="å®æ—¶é€æ­¥æ˜¾ç¤ºç”Ÿæˆå†…å®¹ï¼Œæå‡äº¤äº’ä½“éªŒ"
                )
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ å¤šæ¨¡æ€è¾“å…¥")
                
                text_input = gr.Textbox(
                    label="ğŸ’¬ æ–‡æœ¬è¾“å…¥",
                    placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æŒ‡ä»¤...",
                    lines=3
                )
                
                with gr.Row():
                    with gr.Column():
                        image_input = gr.Image(
                            label="ğŸ–¼ï¸ å›¾åƒè¾“å…¥",
                            type="pil"
                        )
                        
                        audio_input = gr.Audio(
                            label="ğŸµ éŸ³é¢‘è¾“å…¥",
                            type="filepath"
                        )
                    
                    with gr.Column():
                        video_input = gr.Video(
                            label="ğŸ¬ è§†é¢‘è¾“å…¥"
                        )
                
                with gr.Row():
                    process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                    stream_btn = gr.Button("ğŸ“¡ æµå¼å¤„ç†", variant="secondary", size="lg", visible=False)
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ ç”Ÿæˆç»“æœ")
                output_text = gr.Textbox(
                    label="AIå›ç­”",
                    lines=8,
                    placeholder="ç”Ÿæˆçš„å›ç­”å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",
                    interactive=False
                )
                
                # æ˜¾ç¤ºæå–çš„å†…å®¹
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸµ æå–çš„éŸ³é¢‘")
                        extracted_audio_display = gr.Audio(
                            label="ä»è§†é¢‘æå–çš„éŸ³é¢‘",
                            visible=True,
                            interactive=False
                        )
                    
                    with gr.Column():
                        gr.Markdown("### ğŸ–¼ï¸ æå–çš„å›¾åƒ")
                        extracted_image_display = gr.Image(
                            label="ä»è§†é¢‘æå–çš„æœ€åä¸€å¸§",
                            type="pil",
                            visible=True,
                            interactive=False
                        )
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š å¤„ç†ä¿¡æ¯")
                processing_info = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    lines=8,
                    interactive=False,
                    value="ç­‰å¾…å¤„ç†..."
                )
        
        
        # äº‹ä»¶ç»‘å®š
        load_btn.click(
            fn=processor.load_model,
            outputs=model_status
        )
        
        def handle_process_standard(text_input, image_input, audio_input, video_input, system_prompt, max_tokens, 
                                   extract_video_audio, extract_video_frame, enable_streaming):
            """æ ‡å‡†å¤„ç†å‡½æ•°"""
            if enable_streaming:
                # å¦‚æœå¯ç”¨æµå¼ï¼Œç»™å‡ºæç¤º
                return "ğŸ“¡ æµå¼æ¨¡å¼ï¼šè¯·ç‚¹å‡»ä¸‹é¢çš„æµå¼å¤„ç†æŒ‰é’®", "", None, None
            else:
                # ä½¿ç”¨æ ‡å‡†å¤„ç†
                result = processor.process_multimodal(
                    text_input, image_input, audio_input, video_input,
                    system_prompt, max_tokens, extract_video_audio, extract_video_frame, False
                )
                return result[0], result[1], result[2], result[3]  # status, text, audio, image
        
        def handle_process_streaming(text_input, image_input, audio_input, video_input, system_prompt, max_tokens, 
                                   extract_video_audio, extract_video_frame):
            """æµå¼å¤„ç†å‡½æ•°"""
            for status, text, audio, image in processor.process_multimodal_streaming(
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame
            ):
                yield status, text, audio, image
        
        # æ ‡å‡†å¤„ç†æŒ‰é’®
        process_btn.click(
            fn=handle_process_standard,
            inputs=[
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame, enable_streaming
            ],
            outputs=[processing_info, output_text, extracted_audio_display, extracted_image_display]
        )
        
        # æµå¼æŒ‰é’®å·²åœ¨ä¸Šé¢å®šä¹‰
        
        stream_btn.click(
            fn=handle_process_streaming,
            inputs=[
                text_input, image_input, audio_input, video_input,
                system_prompt, max_tokens, extract_video_audio, extract_video_frame
            ],
            outputs=[processing_info, output_text, extracted_audio_display, extracted_image_display]
        )
        
        # æ ¹æ®æµå¼å¼€å…³æ§åˆ¶æŒ‰é’®æ˜¾ç¤º
        def update_buttons(enable_streaming):
            if enable_streaming:
                return gr.update(value="ğŸš€ æ ‡å‡†å¤„ç†"), gr.update(visible=True)
            else:
                return gr.update(value="ğŸš€ å¼€å§‹å¤„ç†"), gr.update(visible=False)
        
        enable_streaming.change(
            fn=update_buttons,
            inputs=[enable_streaming],
            outputs=[process_btn, stream_btn]
        )
        
        def clear_all():
            return "", None, None, None, "", "ç­‰å¾…å¤„ç†...", None, None, False
        
        clear_btn.click(
            fn=clear_all,
            outputs=[text_input, image_input, audio_input, video_input, output_text, processing_info, extracted_audio_display, extracted_image_display, enable_streaming]
        )
    
    return demo


if __name__ == "__main__":
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        debug=True,
        show_error=True
    )