#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ˜¾å­˜å’Œå†…å­˜ç®¡ç†æ¨¡å—
æ”¯æŒåŠ¨æ€é…ç½®å“ªäº›æ¨¡å—åŠ è½½åˆ°æ˜¾å­˜ï¼Œå“ªäº›ä¿æŒåœ¨CPU
"""

import torch
import psutil
import gc
import logging
import json
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import os

logger = logging.getLogger(__name__)

@dataclass
class MemoryConfig:
    """å†…å­˜é…ç½®ç±»"""
    # GPUå†…å­˜é…ç½®
    gpu_memory_limit_gb: float = 4.0           # GPUå†…å­˜é™åˆ¶(GB)
    gpu_memory_reserve_gb: float = 1.0         # GPUé¢„ç•™å†…å­˜(GB)
    enable_gpu_memory_growth: bool = True      # å¯ç”¨GPUå†…å­˜å¢é•¿
    
    # CPUå†…å­˜é…ç½®
    cpu_memory_limit_gb: float = 8.0           # CPUå†…å­˜é™åˆ¶(GB)
    enable_cpu_offload: bool = True            # å¯ç”¨CPUå¸è½½
    
    # æ¨¡å—åŠ è½½é…ç½®
    modules_on_gpu: List[str] = None           # å¼ºåˆ¶åŠ è½½åˆ°GPUçš„æ¨¡å—
    modules_on_cpu: List[str] = None           # å¼ºåˆ¶åŠ è½½åˆ°CPUçš„æ¨¡å—
    auto_manage_modules: bool = True           # è‡ªåŠ¨ç®¡ç†æ¨¡å—ä½ç½®
    
    # ä¼˜åŒ–é…ç½®
    use_gradient_checkpointing: bool = True    # æ¢¯åº¦æ£€æŸ¥ç‚¹
    use_half_precision: bool = True            # åŠç²¾åº¦
    clear_cache_frequency: int = 5             # ç¼“å­˜æ¸…ç†é¢‘ç‡
    
    def __post_init__(self):
        if self.modules_on_gpu is None:
            self.modules_on_gpu = [
                "thinker.model.embed_tokens",
                "thinker.model.layers",
                "thinker.lm_head"
            ]
        
        if self.modules_on_cpu is None:
            self.modules_on_cpu = [
                "talker",
                "token2wav"
            ]

@dataclass
class ModuleMemoryInfo:
    """æ¨¡å—å†…å­˜ä¿¡æ¯"""
    name: str
    parameters: int
    memory_mb: float
    device: str
    dtype: str
    requires_grad: bool
    is_active: bool = True

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: MemoryConfig):
        self.config = config
        self.module_info: Dict[str, ModuleMemoryInfo] = {}
        self.operation_count = 0
        self.setup_memory_monitoring()
    
    def setup_memory_monitoring(self):
        """è®¾ç½®å†…å­˜ç›‘æ§"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    def get_system_memory_info(self) -> Dict[str, float]:
        """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯"""
        info = {}
        
        # CPUå†…å­˜
        memory = psutil.virtual_memory()
        info.update({
            'cpu_total_gb': memory.total / 1024**3,
            'cpu_used_gb': memory.used / 1024**3,
            'cpu_available_gb': memory.available / 1024**3,
            'cpu_percent': memory.percent
        })
        
        # GPUå†…å­˜
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                total = props.total_memory / 1024**3
                
                info.update({
                    f'gpu_{i}_name': props.name,
                    f'gpu_{i}_total_gb': total,
                    f'gpu_{i}_allocated_gb': allocated,
                    f'gpu_{i}_reserved_gb': reserved,
                    f'gpu_{i}_free_gb': total - reserved,
                    f'gpu_{i}_utilization': (allocated / total) * 100 if total > 0 else 0
                })
        
        return info
    
    def analyze_model_memory_usage(self, model) -> Dict[str, ModuleMemoryInfo]:
        """åˆ†ææ¨¡å‹å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        module_info = {}
        
        def get_module_info(name: str, module: torch.nn.Module) -> ModuleMemoryInfo:
            # è®¡ç®—å‚æ•°æ•°é‡
            params = sum(p.numel() for p in module.parameters())
            
            # ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆå­—èŠ‚ï¼‰
            param_memory = 0
            device = "unknown"
            dtype = "unknown"
            requires_grad = False
            
            for param in module.parameters():
                param_memory += param.numel() * param.element_size()
                device = str(param.device)
                dtype = str(param.dtype)
                requires_grad = param.requires_grad
                break  # å‡è®¾æ¨¡å—å†…å‚æ•°è®¾å¤‡ä¸€è‡´
            
            memory_mb = param_memory / 1024**2
            
            return ModuleMemoryInfo(
                name=name,
                parameters=params,
                memory_mb=memory_mb,
                device=device,
                dtype=dtype,
                requires_grad=requires_grad
            )
        
        # åˆ†æä¸»è¦æ¨¡å—
        try:
            if hasattr(model, 'thinker'):
                # Thinkeræ¨¡å—
                if hasattr(model.thinker, 'model'):
                    module_info['thinker.model'] = get_module_info('thinker.model', model.thinker.model)
                
                if hasattr(model.thinker, 'lm_head'):
                    module_info['thinker.lm_head'] = get_module_info('thinker.lm_head', model.thinker.lm_head)
                
                if hasattr(model.thinker, 'visual'):
                    module_info['thinker.visual'] = get_module_info('thinker.visual', model.thinker.visual)
                
                if hasattr(model.thinker, 'audio_tower'):
                    module_info['thinker.audio_tower'] = get_module_info('thinker.audio_tower', model.thinker.audio_tower)
            
            # Talkeræ¨¡å—
            if hasattr(model, 'talker'):
                module_info['talker'] = get_module_info('talker', model.talker)
            
            # Token2Wavæ¨¡å—
            if hasattr(model, 'token2wav'):
                module_info['token2wav'] = get_module_info('token2wav', model.token2wav)
            
        except Exception as e:
            logger.error(f"æ¨¡å‹å†…å­˜åˆ†æå¤±è´¥: {e}")
        
        self.module_info = module_info
        return module_info
    
    def apply_memory_optimization(self, model) -> Dict[str, Any]:
        """åº”ç”¨å†…å­˜ä¼˜åŒ–ç­–ç•¥"""
        optimization_results = {
            'optimizations_applied': [],
            'memory_before': self.get_system_memory_info(),
            'module_moves': []
        }
        
        try:
            # 1. å¼ºåˆ¶GPUæ¨¡å—
            for module_name in self.config.modules_on_gpu:
                if self._move_module_to_device(model, module_name, 'cuda'):
                    optimization_results['module_moves'].append(f"{module_name} -> GPU")
                    optimization_results['optimizations_applied'].append(f"moved_{module_name}_to_gpu")
            
            # 2. å¼ºåˆ¶CPUæ¨¡å—
            for module_name in self.config.modules_on_cpu:
                if self._move_module_to_device(model, module_name, 'cpu'):
                    optimization_results['module_moves'].append(f"{module_name} -> CPU")
                    optimization_results['optimizations_applied'].append(f"moved_{module_name}_to_cpu")
            
            # 3. è‡ªåŠ¨ç®¡ç†æ¨¡å—ï¼ˆæ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µï¼‰
            if self.config.auto_manage_modules:
                auto_moves = self._auto_manage_modules(model)
                optimization_results['module_moves'].extend(auto_moves)
                optimization_results['optimizations_applied'].extend([f"auto_managed_{len(auto_moves)}_modules"])
            
            # 4. åº”ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            if self.config.use_gradient_checkpointing:
                try:
                    model.gradient_checkpointing_enable()
                    optimization_results['optimizations_applied'].append("gradient_checkpointing")
                except:
                    logger.warning("æ¢¯åº¦æ£€æŸ¥ç‚¹è®¾ç½®å¤±è´¥")
            
            # 5. æ¸…ç†å†…å­˜
            self.clear_memory()
            optimization_results['optimizations_applied'].append("memory_cleanup")
            
            # 6. è®°å½•ä¼˜åŒ–åå†…å­˜
            optimization_results['memory_after'] = self.get_system_memory_info()
            
            logger.info(f"å†…å­˜ä¼˜åŒ–å®Œæˆ: {len(optimization_results['optimizations_applied'])}é¡¹ä¼˜åŒ–")
            
        except Exception as e:
            logger.error(f"å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
            optimization_results['error'] = str(e)
        
        return optimization_results
    
    def _move_module_to_device(self, model, module_path: str, device: str) -> bool:
        """ç§»åŠ¨æ¨¡å—åˆ°æŒ‡å®šè®¾å¤‡"""
        try:
            # è§£ææ¨¡å—è·¯å¾„
            module = model
            for attr in module_path.split('.'):
                if hasattr(module, attr):
                    module = getattr(module, attr)
                else:
                    logger.warning(f"æ¨¡å—è·¯å¾„ä¸å­˜åœ¨: {module_path}")
                    return False
            
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            module.to(device)
            logger.debug(f"æ¨¡å— {module_path} å·²ç§»åŠ¨åˆ° {device}")
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å—ç§»åŠ¨å¤±è´¥ {module_path} -> {device}: {e}")
            return False
    
    def _auto_manage_modules(self, model) -> List[str]:
        """è‡ªåŠ¨ç®¡ç†æ¨¡å—ä½ç½®"""
        moves = []
        
        try:
            # è·å–å½“å‰GPUå†…å­˜ä½¿ç”¨
            if not torch.cuda.is_available():
                return moves
            
            current_memory = torch.cuda.memory_allocated() / 1024**3
            memory_limit = self.config.gpu_memory_limit_gb - self.config.gpu_memory_reserve_gb
            
            if current_memory > memory_limit:
                # å†…å­˜ä¸è¶³ï¼Œå°†éå…³é”®æ¨¡å—ç§»åˆ°CPU
                non_critical_modules = [
                    "thinker.visual",
                    "thinker.audio_tower",
                    "token2wav"
                ]
                
                for module_name in non_critical_modules:
                    if self._move_module_to_device(model, module_name, 'cpu'):
                        moves.append(f"{module_name} -> CPU (è‡ªåŠ¨)")
                        
                        # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
                        new_memory = torch.cuda.memory_allocated() / 1024**3
                        if new_memory <= memory_limit:
                            break
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ¨¡å—ç®¡ç†å¤±è´¥: {e}")
        
        return moves
    
    def clear_memory(self):
        """æ¸…ç†å†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def monitor_memory_during_inference(self, operation_name: str = "inference") -> Dict[str, float]:
        """æ¨ç†æœŸé—´ç›‘æ§å†…å­˜"""
        self.operation_count += 1
        
        # å®šæœŸæ¸…ç†ç¼“å­˜
        if self.operation_count % self.config.clear_cache_frequency == 0:
            self.clear_memory()
            logger.debug(f"å®šæœŸå†…å­˜æ¸…ç†å®Œæˆ (æ“ä½œ #{self.operation_count})")
        
        # è·å–å†…å­˜ä¿¡æ¯
        memory_info = self.get_system_memory_info()
        
        # æ£€æŸ¥å†…å­˜è­¦å‘Š
        if torch.cuda.is_available():
            gpu_util = memory_info.get('gpu_0_utilization', 0)
            if gpu_util > 90:
                logger.warning(f"GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {gpu_util:.1f}%")
        
        cpu_percent = memory_info.get('cpu_percent', 0)
        if cpu_percent > 85:
            logger.warning(f"CPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent:.1f}%")
        
        return memory_info
    
    def get_optimization_recommendations(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        memory_info = self.get_system_memory_info()
        
        # GPUå†…å­˜å»ºè®®
        if torch.cuda.is_available():
            gpu_util = memory_info.get('gpu_0_utilization', 0)
            if gpu_util > 80:
                recommendations.append("GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¯ç”¨CPUå¸è½½")
                recommendations.append("è€ƒè™‘å‡å°‘batch_sizeæˆ–é™ä½æ¨¡å‹ç²¾åº¦")
            elif gpu_util < 50:
                recommendations.append("GPUå†…å­˜å……è¶³ï¼Œå¯ä»¥æé«˜æ¨¡å‹ç²¾åº¦æˆ–å¢åŠ batch_size")
        
        # CPUå†…å­˜å»ºè®®
        cpu_percent = memory_info.get('cpu_percent', 0)
        if cpu_percent > 80:
            recommendations.append("CPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å‡å°‘CPUå¸è½½çš„æ¨¡å—")
        
        # æ ¹æ®æ¨¡å—ä½¿ç”¨æƒ…å†µç»™å‡ºå»ºè®®
        if self.module_info:
            total_gpu_memory = sum(
                info.memory_mb for info in self.module_info.values() 
                if 'cuda' in info.device
            )
            if total_gpu_memory > self.config.gpu_memory_limit_gb * 1024:
                recommendations.append("æ¨¡å‹æ€»å†…å­˜è¶…è¿‡é™åˆ¶ï¼Œå»ºè®®å°†æ›´å¤šæ¨¡å—ç§»åˆ°CPU")
        
        return recommendations

class DynamicMemoryManager:
    """åŠ¨æ€å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: MemoryConfig):
        self.config = config
        self.memory_manager = MemoryManager(config)
        self.module_usage_history: Dict[str, int] = {}
        self.last_optimization_memory = 0
    
    def smart_module_placement(self, model) -> Dict[str, str]:
        """æ™ºèƒ½æ¨¡å—æ”¾ç½®ç­–ç•¥"""
        placement_plan = {}
        
        try:
            # åˆ†ææ¨¡å‹
            module_info = self.memory_manager.analyze_model_memory_usage(model)
            
            # æŒ‰å†…å­˜ä½¿ç”¨æ’åº
            sorted_modules = sorted(
                module_info.items(),
                key=lambda x: x[1].memory_mb,
                reverse=True
            )
            
            # è®¡ç®—å¯ç”¨GPUå†…å­˜
            memory_info = self.memory_manager.get_system_memory_info()
            available_gpu_memory = (
                self.config.gpu_memory_limit_gb - 
                self.config.gpu_memory_reserve_gb
            ) * 1024  # è½¬æ¢ä¸ºMB
            
            used_gpu_memory = 0
            
            # ä¼˜å…ˆçº§æ’åºï¼ˆæ ¸å¿ƒæ¨¡å—ä¼˜å…ˆGPUï¼‰
            core_modules = [
                "thinker.model.embed_tokens",
                "thinker.model.layers", 
                "thinker.lm_head"
            ]
            
            # é¦–å…ˆæ”¾ç½®æ ¸å¿ƒæ¨¡å—
            for module_name, info in sorted_modules:
                if any(core in module_name for core in core_modules):
                    if used_gpu_memory + info.memory_mb <= available_gpu_memory:
                        placement_plan[module_name] = "cuda"
                        used_gpu_memory += info.memory_mb
                    else:
                        placement_plan[module_name] = "cpu"
                        logger.warning(f"æ ¸å¿ƒæ¨¡å— {module_name} è¢«æ”¾ç½®åˆ°CPUï¼ˆå†…å­˜ä¸è¶³ï¼‰")
            
            # ç„¶åæ”¾ç½®å…¶ä»–æ¨¡å—
            for module_name, info in sorted_modules:
                if module_name not in placement_plan:
                    if used_gpu_memory + info.memory_mb <= available_gpu_memory:
                        placement_plan[module_name] = "cuda"
                        used_gpu_memory += info.memory_mb
                    else:
                        placement_plan[module_name] = "cpu"
            
            logger.info(f"æ™ºèƒ½æ”¾ç½®è®¡åˆ’: GPU {used_gpu_memory:.1f}MB / {available_gpu_memory:.1f}MB")
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ¨¡å—æ”¾ç½®å¤±è´¥: {e}")
        
        return placement_plan
    
    def adaptive_optimization(self, model, current_memory_usage: float) -> bool:
        """è‡ªé€‚åº”ä¼˜åŒ–"""
        try:
            # å¦‚æœå†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼ï¼Œè§¦å‘ä¼˜åŒ–
            memory_threshold = self.config.gpu_memory_limit_gb * 0.9
            
            if current_memory_usage > memory_threshold:
                logger.info(f"å†…å­˜ä½¿ç”¨è¿‡é«˜ ({current_memory_usage:.2f}GB)ï¼Œè§¦å‘è‡ªé€‚åº”ä¼˜åŒ–")
                
                # æ‰¾åˆ°æœ€å°‘ä½¿ç”¨çš„éæ ¸å¿ƒæ¨¡å—ç§»åˆ°CPU
                candidates = [
                    "thinker.visual",
                    "thinker.audio_tower",
                    "token2wav"
                ]
                
                for candidate in candidates:
                    if self.memory_manager._move_module_to_device(model, candidate, 'cpu'):
                        logger.info(f"è‡ªé€‚åº”ä¼˜åŒ–: ç§»åŠ¨ {candidate} åˆ°CPU")
                        
                        # æ¸…ç†å†…å­˜å¹¶æ£€æŸ¥
                        self.memory_manager.clear_memory()
                        new_usage = torch.cuda.memory_allocated() / 1024**3
                        
                        if new_usage <= memory_threshold:
                            logger.info(f"è‡ªé€‚åº”ä¼˜åŒ–æˆåŠŸ: {current_memory_usage:.2f}GB -> {new_usage:.2f}GB")
                            return True
                
                logger.warning("è‡ªé€‚åº”ä¼˜åŒ–æ— æ³•å……åˆ†é‡Šæ”¾å†…å­˜")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"è‡ªé€‚åº”ä¼˜åŒ–å¤±è´¥: {e}")
            return False

class ConfigurableMemoryLoader:
    """å¯é…ç½®çš„å†…å­˜åŠ è½½å™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = self.load_config(config_path)
        self.memory_manager = MemoryManager(self.config)
        self.dynamic_manager = DynamicMemoryManager(self.config)
    
    def load_config(self, config_path: Optional[str]) -> MemoryConfig:
        """åŠ è½½å†…å­˜é…ç½®"""
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_dict = json.load(f)
                return MemoryConfig(**config_dict)
            except Exception as e:
                logger.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤é…ç½®
        return MemoryConfig()
    
    def save_config(self, config_path: str):
        """ä¿å­˜å†…å­˜é…ç½®"""
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.config), f, indent=2, ensure_ascii=False)
            logger.info(f"é…ç½®å·²ä¿å­˜: {config_path}")
        except Exception as e:
            logger.error(f"é…ç½®ä¿å­˜å¤±è´¥: {e}")
    
    def load_model_with_memory_config(self, model_class, model_path: str, **model_kwargs):
        """ä½¿ç”¨å†…å­˜é…ç½®åŠ è½½æ¨¡å‹"""
        try:
            logger.info("å¼€å§‹åŠ è½½æ¨¡å‹å¹¶åº”ç”¨å†…å­˜é…ç½®")
            
            # è·å–æ™ºèƒ½è®¾å¤‡æ˜ å°„
            device_map = self.get_intelligent_device_map()
            
            # åˆå¹¶é…ç½®
            final_kwargs = {
                "device_map": device_map,
                "torch_dtype": torch.float16 if self.config.use_half_precision else torch.float32,
                **model_kwargs
            }
            
            # åŠ è½½æ¨¡å‹
            model = model_class.from_pretrained(model_path, **final_kwargs)
            
            # åº”ç”¨å†…å­˜ä¼˜åŒ–
            optimization_results = self.memory_manager.apply_memory_optimization(model)
            
            logger.info("æ¨¡å‹åŠ è½½å’Œå†…å­˜ä¼˜åŒ–å®Œæˆ")
            self._print_memory_summary()
            
            return model, optimization_results
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_intelligent_device_map(self) -> Dict[str, str]:
        """è·å–æ™ºèƒ½è®¾å¤‡æ˜ å°„"""
        device_map = {}
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§å’Œå†…å­˜
        if not torch.cuda.is_available():
            logger.warning("CUDAä¸å¯ç”¨ï¼Œæ‰€æœ‰æ¨¡å—å°†åŠ è½½åˆ°CPU")
            return "cpu"
        
        memory_info = self.memory_manager.get_system_memory_info()
        available_gpu_memory = memory_info.get('gpu_0_free_gb', 0)
        
        if available_gpu_memory < 2.0:
            logger.warning("GPUå†…å­˜ä¸è¶³ï¼Œå¯ç”¨æ¿€è¿›çš„CPUå¸è½½")
            device_map.update({
                "thinker.model": "cuda",
                "thinker.lm_head": "cuda",
                "thinker.visual": "cpu",
                "thinker.audio_tower": "cpu",
                "talker": "cpu",
                "token2wav": "cpu"
            })
        elif available_gpu_memory < 4.0:
            logger.info("GPUå†…å­˜æœ‰é™ï¼Œå¯ç”¨é€‚åº¦çš„CPUå¸è½½")
            device_map.update({
                "thinker.model": "cuda",
                "thinker.lm_head": "cuda", 
                "thinker.visual": "cuda",
                "thinker.audio_tower": "cpu",
                "talker": "cpu",
                "token2wav": "cpu"
            })
        else:
            logger.info("GPUå†…å­˜å……è¶³ï¼Œä¸»è¦æ¨¡å—åŠ è½½åˆ°GPU")
            device_map.update({
                "thinker.model": "cuda",
                "thinker.lm_head": "cuda",
                "thinker.visual": "cuda", 
                "thinker.audio_tower": "cuda",
                "talker": "cpu",      # talkerä»ä¿æŒåœ¨CPUï¼ˆæ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼‰
                "token2wav": "cpu"
            })
        
        return device_map
    
    def _print_memory_summary(self):
        """æ‰“å°å†…å­˜æ‘˜è¦"""
        try:
            memory_info = self.memory_manager.get_system_memory_info()
            
            print("\n" + "="*60)
            print("ğŸ“Š å†…å­˜ä½¿ç”¨æ‘˜è¦")
            print("="*60)
            
            # CPUå†…å­˜
            print(f"ğŸ’» CPUå†…å­˜: {memory_info['cpu_used_gb']:.2f}GB / {memory_info['cpu_total_gb']:.2f}GB ({memory_info['cpu_percent']:.1f}%)")
            
            # GPUå†…å­˜
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    gpu_name = memory_info.get(f'gpu_{i}_name', f'GPU {i}')
                    gpu_used = memory_info.get(f'gpu_{i}_allocated_gb', 0)
                    gpu_total = memory_info.get(f'gpu_{i}_total_gb', 0)
                    gpu_util = memory_info.get(f'gpu_{i}_utilization', 0)
                    
                    print(f"ğŸ® {gpu_name}: {gpu_used:.2f}GB / {gpu_total:.2f}GB ({gpu_util:.1f}%)")
            
            # ä¼˜åŒ–å»ºè®®
            recommendations = self.memory_manager.get_optimization_recommendations()
            if recommendations:
                print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for rec in recommendations:
                    print(f"  - {rec}")
            
            print("="*60)
            
        except Exception as e:
            logger.error(f"å†…å­˜æ‘˜è¦æ‰“å°å¤±è´¥: {e}")

# é¢„è®¾å†…å­˜é…ç½®
class MemoryPresets:
    """å†…å­˜é…ç½®é¢„è®¾"""
    
    @staticmethod
    def get_preset(name: str) -> MemoryConfig:
        """è·å–é¢„è®¾é…ç½®"""
        presets = {
            'ultra_low_vram': MemoryConfig(
                gpu_memory_limit_gb=2.0,
                gpu_memory_reserve_gb=0.5,
                modules_on_gpu=["thinker.model.layers"],
                modules_on_cpu=[
                    "thinker.visual",
                    "thinker.audio_tower", 
                    "talker",
                    "token2wav"
                ],
                auto_manage_modules=True,
                use_gradient_checkpointing=True,
                use_half_precision=True
            ),
            
            'low_vram': MemoryConfig(
                gpu_memory_limit_gb=4.0,
                gpu_memory_reserve_gb=1.0,
                modules_on_gpu=[
                    "thinker.model",
                    "thinker.lm_head"
                ],
                modules_on_cpu=[
                    "talker",
                    "token2wav"
                ],
                auto_manage_modules=True,
                use_gradient_checkpointing=True,
                use_half_precision=True
            ),
            
            'balanced': MemoryConfig(
                gpu_memory_limit_gb=6.0,
                gpu_memory_reserve_gb=1.5,
                modules_on_gpu=[
                    "thinker.model",
                    "thinker.lm_head",
                    "thinker.visual"
                ],
                modules_on_cpu=[
                    "talker",
                    "token2wav"
                ],
                auto_manage_modules=True,
                use_gradient_checkpointing=False,
                use_half_precision=True
            ),
            
            'high_vram': MemoryConfig(
                gpu_memory_limit_gb=12.0,
                gpu_memory_reserve_gb=2.0,
                modules_on_gpu=[
                    "thinker",
                    "thinker.visual",
                    "thinker.audio_tower"
                ],
                modules_on_cpu=[],
                auto_manage_modules=False,
                use_gradient_checkpointing=False,
                use_half_precision=False
            )
        }
        
        if name not in presets:
            raise ValueError(f"æœªçŸ¥é¢„è®¾: {name}. å¯ç”¨é¢„è®¾: {list(presets.keys())}")
        
        return presets[name]
    
    @staticmethod
    def list_presets() -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é¢„è®¾"""
        return ['ultra_low_vram', 'low_vram', 'balanced', 'high_vram']
    
    @staticmethod
    def detect_optimal_preset() -> str:
        """æ£€æµ‹æœ€ä¼˜é¢„è®¾"""
        try:
            if not torch.cuda.is_available():
                return 'ultra_low_vram'
            
            # æ£€æµ‹GPUå†…å­˜
            props = torch.cuda.get_device_properties(0)
            total_memory_gb = props.total_memory / 1024**3
            
            if total_memory_gb < 4:
                return 'ultra_low_vram'
            elif total_memory_gb < 6:
                return 'low_vram'
            elif total_memory_gb < 10:
                return 'balanced'
            else:
                return 'high_vram'
                
        except Exception:
            return 'ultra_low_vram'

def create_memory_config_file(preset_name: str, output_path: str):
    """åˆ›å»ºå†…å­˜é…ç½®æ–‡ä»¶"""
    try:
        config = MemoryPresets.get_preset(preset_name)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(config), f, indent=2, ensure_ascii=False)
        
        logger.info(f"å†…å­˜é…ç½®æ–‡ä»¶å·²åˆ›å»º: {output_path}")
        
    except Exception as e:
        logger.error(f"é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}")

def auto_configure_memory() -> MemoryConfig:
    """è‡ªåŠ¨é…ç½®å†…å­˜"""
    optimal_preset = MemoryPresets.detect_optimal_preset()
    logger.info(f"æ£€æµ‹åˆ°æœ€ä¼˜é¢„è®¾: {optimal_preset}")
    return MemoryPresets.get_preset(optimal_preset)