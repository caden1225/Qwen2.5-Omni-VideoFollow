#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å•ä¸ªé¢„è®¾é…ç½®çš„å¢å¼ºç‰ˆè§†é¢‘å¤„ç†å™¨
é¿å…å†…å­˜é—®é¢˜ï¼Œä¸“æ³¨äºåŠŸèƒ½éªŒè¯
"""

import torch
import gc
import os
import sys
import time
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from enhanced_video_processor import (
    EnhancedVideoProcessor,
    EnhancedVideoOptimizationPresets
)

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniConfig

# ä»ç¯å¢ƒå˜é‡åŠ è½½æ¨¡å‹è·¯å¾„
MODEL_PATH = os.getenv('MODEL_PATH', "/home/caden/workplace/models/Qwen2.5-Omni-3B")

def print_gpu_memory_usage(stage=""):
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        print(f"[{stage}] GPUå†…å­˜ - å·²åˆ†é…: {allocated:.2f}GB, å·²é¢„ç•™: {reserved:.2f}GB")

def load_model():
    """åŠ è½½æ¨¡å‹"""
    print("=== åŠ è½½æ¨¡å‹ ===")
    
    try:
        config = Qwen2_5OmniConfig.from_pretrained(MODEL_PATH)
        config.enable_audio_output = False
        
        device_map = {
            "thinker.model": "cuda",
            "thinker.lm_head": "cuda",
            "thinker.visual": "cuda",
            "thinker.audio_tower": "cuda",
        }
        
        max_memory = {0: "8GB", "cpu": "16GB"}
        
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        print_gpu_memory_usage("åŠ è½½å‰")
        
        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            config=config,
            device_map=device_map,
            max_memory=max_memory,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        
        processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)
        
        print_gpu_memory_usage("æ¨¡å‹åŠ è½½å")
        print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_single_preset(video_path: str, preset_name: str, model, processor):
    """æµ‹è¯•å•ä¸ªé¢„è®¾é…ç½®"""
    print(f"\n{'='*60}")
    print(f"ğŸ¬ æµ‹è¯•é¢„è®¾: {preset_name}")
    print(f"{'='*60}")
    
    try:
        # è·å–é¢„è®¾é…ç½®
        video_config, separation_config = EnhancedVideoOptimizationPresets.get_separation_preset(preset_name)
        
        print(f"ğŸ“‹ é…ç½®è¯¦æƒ…:")
        print(f"  - éŸ³é¢‘æå–: {separation_config.extract_audio}")
        print(f"  - å¸§æå–: {separation_config.extract_frames}")
        print(f"  - å¸§æå–æ–¹æ³•: {separation_config.frame_extraction_method}")
        print(f"  - å…³é”®å¸§æ•°é‡: {separation_config.num_keyframes}")
        print(f"  - è§†é¢‘å¤„ç†: {separation_config.video_processing}")
        
        # åˆ›å»ºå¢å¼ºç‰ˆå¤„ç†å™¨
        processor_enhanced = EnhancedVideoProcessor(video_config, separation_config)
        
        # åˆ›å»ºå¯¹è¯
        conversation = [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªè§†é¢‘åˆ†æåŠ©æ‰‹ï¼Œè¯·æ ¹æ®å¾—åˆ°çš„ä¿¡æ¯å®Œæˆä»»åŠ¡ã€‚"}
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": video_path},
                    {"type": "text", "text": "å®Œæˆè§†é¢‘ä¸­çš„æŒ‡ä»¤ã€‚"},
                ],
            },
        ]
        
        # ä½¿ç”¨åˆ†ç¦»å¤„ç†æ–¹å¼å¤„ç†è§†é¢‘
        print(f"\nğŸ”„ å¼€å§‹åˆ†ç¦»å¤„ç†è§†é¢‘...")
        start_time = time.time()
        
        success, results, media_data = processor_enhanced.process_video_with_separation(video_path, conversation)
        
        if not success:
            print("âŒ è§†é¢‘åˆ†ç¦»å¤„ç†å¤±è´¥")
            return False
        
        total_time = time.time() - start_time
        
        print(f"\nâœ… è§†é¢‘åˆ†ç¦»å¤„ç†æˆåŠŸï¼")
        print(f"ğŸ“Š å¤„ç†ç»“æœ:")
        print(f"  - éŸ³é¢‘æå–: {'âœ… æˆåŠŸ' if results['audio_extraction']['success'] else 'âŒ å¤±è´¥'}")
        print(f"  - å¸§æå–: {'âœ… æˆåŠŸ' if results['frame_extraction']['success'] else 'âŒ å¤±è´¥'}")
        print(f"  - è§†é¢‘å¤„ç†: {'âœ… æˆåŠŸ' if results['video_processing']['success'] else 'âŒ å¤±è´¥'}")
        print(f"  - åˆ†ç¦»å¤„ç†æ—¶é—´: {results['processing_time']:.2f}ç§’")
        print(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # å¦‚æœæˆåŠŸæå–äº†éŸ³é¢‘å’Œå¸§ï¼Œæµ‹è¯•å›¾åƒè¾“å…¥
        if results['audio_extraction']['success'] and results['frame_extraction']['success']:
            print(f"\nğŸ§ª æµ‹è¯•å›¾åƒè¾“å…¥å¤„ç†...")
            
            try:
                # ä½¿ç”¨æœ€åä¸€å¸§å›¾ç‰‡
                last_frame_path = results['frame_extraction']['paths'][-1]
                print(f"  ğŸ“¸ ä½¿ç”¨æœ€åä¸€å¸§å›¾ç‰‡: {os.path.basename(last_frame_path)}")
                
                # åˆ›å»ºå›¾åƒè¾“å…¥å¯¹è¯
                image_conversation = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": last_frame_path},
                            {"type": "text", "text": "è¿™æ˜¯è§†é¢‘çš„æœ€åä¸€å¸§ï¼Œè¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"}
                        ],
                    }
                ]
                
                # åº”ç”¨èŠå¤©æ¨¡æ¿
                text = processor.apply_chat_template(image_conversation, add_generation_prompt=True)
                print(f"âœ… å›¾åƒè¾“å…¥æ¨¡æ¿åº”ç”¨æˆåŠŸï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
                
                # å¤„ç†è¾“å…¥
                try:
                    from PIL import Image
                    
                    # åŠ è½½æœ€åä¸€å¸§å›¾åƒ
                    image = Image.open(last_frame_path)
                    
                    # å¤„ç†è¾“å…¥
                    inputs = processor(
                        text=[text],
                        images=[image],
                        videos=None,
                        padding=True,
                        return_tensors="pt",
                    )
                    
                    print(f"âœ… å›¾åƒè¾“å…¥å¤„ç†æˆåŠŸ")
                    print(f"ğŸ“Š è¾“å…¥å¼ é‡ä¿¡æ¯:")
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            print(f"  - {key}: {value.shape}, {value.dtype}")
                        else:
                            print(f"  - {key}: {type(value)}")
                    
                    # å¦‚æœæœ‰æ¨¡å‹ï¼Œå°è¯•ç”Ÿæˆå›å¤
                    if model is not None:
                        print(f"\nğŸ”„ ç”ŸæˆAIå›å¤...")
                        generation_start = time.time()
                        
                        # ç¡®ä¿å¼ é‡ç±»å‹æ­£ç¡®
                        for key, value in inputs.items():
                            if isinstance(value, torch.Tensor):
                                if key in ["input_ids", "image_grid_thw", "video_grid_thw"]:
                                    inputs[key] = value.to(model.device).long()
                                elif "visual" in key:
                                    inputs[key] = value.to(model.device).to(model.dtype)
                                else:
                                    inputs[key] = value.to(model.device).to(model.dtype)
                        
                        with torch.no_grad():
                            generated_ids = model.generate(
                                **inputs,
                                max_new_tokens=256,
                                do_sample=False,
                                temperature=0.0,
                                top_p=1.0,
                            )
                        
                        generation_time = time.time() - generation_start
                        
                        # è§£ç è¾“å‡º
                        generated_ids = [
                            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
                        ]
                        response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                        
                        print(f"\nğŸ‰ æ¨ç†æˆåŠŸï¼")
                        print(f"ğŸ¤– AIå›å¤:")
                        print(f"{response}")
                        print(f"\nâ±ï¸ ç”Ÿæˆè€—æ—¶: {generation_time:.2f}ç§’")
                    
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒè¾“å…¥å¤„ç†å¤±è´¥: {e}")
                    print(f"ğŸ“ è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºprocessoréœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼")
                
            except Exception as e:
                print(f"âŒ å›¾åƒè¾“å…¥æµ‹è¯•å¤±è´¥: {e}")
                print(f"ğŸ“ è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºprocessoréœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼")
        
        print_gpu_memory_usage("åˆ†ç¦»å¤„ç†å")
        
        # æ¸…ç†
        processor_enhanced.cleanup()
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å•ä¸ªé¢„è®¾é…ç½®æµ‹è¯•")
    print("="*80)
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    video_path = "./math.mp4"
    
    if not os.path.exists(video_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    file_size = os.path.getsize(video_path) / (1024 * 1024)
    print(f"ğŸ“ æµ‹è¯•è§†é¢‘: {video_path} ({file_size:.2f} MB)")
    
    try:
        # åŠ è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        print(f"\n{'='*60}")
        print("ğŸ”„ åŠ è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•å›¾åƒè¾“å…¥ï¼‰")
        print(f"{'='*60}")
        
        model, processor = load_model()
        
        if model is None or processor is None:
            print("âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†è·³è¿‡å›¾åƒè¾“å…¥æµ‹è¯•")
            model, processor = None, None
        
        # æµ‹è¯•balanced_separationé¢„è®¾ï¼ˆæ¨èé…ç½®ï¼‰
        preset_name = "balanced_separation"
        print(f"\nğŸ¯ æµ‹è¯•æ¨èé¢„è®¾: {preset_name}")
        
        success = test_single_preset(video_path, preset_name, model, processor)
        
        if success:
            print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸï¼")
            print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“:")
            print(f"  - é¢„è®¾é…ç½®: {preset_name}")
            print(f"  - éŸ³è½¨å’Œè§†é¢‘åˆ†ç¦»å¤„ç†: âœ… æˆåŠŸ")
            print(f"  - æœ€åä¸€å¸§å›¾åƒåˆ†æ: âœ… æˆåŠŸ")
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥")
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
