#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•éŸ³è½¨å’Œè§†é¢‘åˆ†ç¦»å¤„ç†åŠŸèƒ½
ä½¿ç”¨math.mp4æ–‡ä»¶è¿›è¡Œæµ‹è¯•ï¼Œæå–éŸ³é¢‘å’Œæœ€åä¸€å¸§å›¾åƒ
"""

import torch
import gc
import os
import sys
import time
import cv2
import numpy as np
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniConfig

# ä»ç¯å¢ƒå˜é‡åŠ è½½æ¨¡å‹è·¯å¾„
MODEL_PATH = os.getenv('MODEL_PATH', "/home/caden/workplace/models/Qwen2.5-Omni-3B")

def print_gpu_memory_usage(stage=""):
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        print(f"[{stage}] GPUå†…å­˜ - å·²åˆ†é…: {allocated:.2f}GB, å·²é¢„ç•™: {reserved:.2f}GB")

def load_model():
    """åŠ è½½æ¨¡å‹"""
    print("=== åŠ è½½æ¨¡å‹ ===")
    
    try:
        config = Qwen2_5OmniConfig.from_pretrained(MODEL_PATH)
        config.enable_audio_output = False
        
        device_map = {
            "thinker.model": "cuda",
            "thinker.lm_head": "cuda",
            "thinker.visual": "cuda",
            "thinker.audio_tower": "cuda",
        }
        
        max_memory = {0: "8GB", "cpu": "16GB"}
        
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        print_gpu_memory_usage("åŠ è½½å‰")
        
        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            config=config,
            device_map=device_map,
            max_memory=max_memory,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        
        processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)
        
        print_gpu_memory_usage("æ¨¡å‹åŠ è½½å")
        print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def extract_audio_from_video(video_path: str, output_audio_path: str = None):
    """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
    print(f"ğŸµ ä»è§†é¢‘ä¸­æå–éŸ³é¢‘: {os.path.basename(video_path)}")
    
    if output_audio_path is None:
        output_audio_path = video_path.replace('.mp4', '_audio.wav')
    
    try:
        import cv2
        
        # æ‰“å¼€è§†é¢‘æ–‡ä»¶
        cap = cv2.VideoCapture(video_path)
        
        # è·å–éŸ³é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"  - è§†é¢‘ä¿¡æ¯:")
        print(f"    FPS: {fps:.2f}")
        print(f"    ï¸æ€»å¸§æ•°: {total_frames}")
        print(f"    â±ï¸æ—¶é•¿: {duration:.2f}ç§’")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éŸ³é¢‘è½¨é“
        has_audio = False
        try:
            # å°è¯•è¯»å–éŸ³é¢‘
            import librosa
            audio, sr = librosa.load(video_path, sr=None)
            has_audio = True
            print(f"  - éŸ³é¢‘ä¿¡æ¯:")
            print(f"    ğŸµ é‡‡æ ·ç‡: {sr} Hz")
            print(f"    ğŸ”Š éŸ³é¢‘é•¿åº¦: {len(audio)/sr:.2f}ç§’")
            print(f"    ğŸ“Š éŸ³é¢‘å½¢çŠ¶: {audio.shape}")
            
            # ä¿å­˜éŸ³é¢‘ï¼ˆä½¿ç”¨soundfileï¼Œå› ä¸ºlibrosa.outputå·²è¢«ç§»é™¤ï¼‰
            import soundfile as sf
            sf.write(output_audio_path, audio, sr)
            print(f"  - âœ… éŸ³é¢‘å·²ä¿å­˜åˆ°: {output_audio_path}")
            
        except Exception as e:
            print(f"  - âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
            has_audio = False
        
        cap.release()
        return has_audio, output_audio_path
        
    except Exception as e:
        print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
        return False, None

def extract_last_frame_from_video(video_path: str, output_image_path: str = None):
    """ä»è§†é¢‘ä¸­æå–æœ€åä¸€å¸§ä½œä¸ºå›¾åƒ"""
    print(f"ğŸ–¼ï¸ ä»è§†é¢‘ä¸­æå–æœ€åä¸€å¸§: {os.path.basename(video_path)}")
    
    if output_image_path is None:
        output_image_path = video_path.replace('.mp4', '_last_frame.jpg')
    
    try:
        import cv2
        
        # æ‰“å¼€è§†é¢‘æ–‡ä»¶
        cap = cv2.VideoCapture(video_path)
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"  - è§†é¢‘ä¿¡æ¯:")
        print(f"    ğŸ“ åˆ†è¾¨ç‡: {width}x{height}")
        print(f"    ğŸ¬ æ€»å¸§æ•°: {total_frames}")
        print(f"    â±ï¸ æ—¶é•¿: {total_frames/fps:.2f}ç§’")
        
        if total_frames > 0:
            # è·³è½¬åˆ°æœ€åä¸€å¸§
            cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)
            
            # è¯»å–æœ€åä¸€å¸§
            ret, frame = cap.read()
            
            if ret:
                # è½¬æ¢BGRåˆ°RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # ä¿å­˜å›¾åƒ
                cv2.imwrite(output_image_path, frame)
                print(f"  - âœ… æœ€åä¸€å¸§å·²ä¿å­˜åˆ°: {output_image_path}")
                print(f"  - ğŸ“Š å›¾åƒå½¢çŠ¶: {frame_rgb.shape}")
                
                cap.release()
                return True, output_image_path, frame_rgb
            else:
                print("  - âŒ æ— æ³•è¯»å–æœ€åä¸€å¸§")
                cap.release()
                return False, None, None
        else:
            print("  - âŒ è§†é¢‘æ²¡æœ‰å¸§")
            cap.release()
            return False, None, None
            
    except Exception as e:
        print(f"âŒ å¸§æå–å¤±è´¥: {e}")
        return False, None, None

def test_processor_capabilities(processor, video_path: str, audio_path: str, image_path: str):
    """æµ‹è¯•processorå¤„ç†éŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒçš„èƒ½åŠ›"""
    print(f"\nğŸ§ª æµ‹è¯•Processorèƒ½åŠ›")
    print(f"{'='*60}")
    
    # æµ‹è¯•1: çº¯æ–‡æœ¬è¾“å…¥
    print(f"\nğŸ“ æµ‹è¯•1: çº¯æ–‡æœ¬è¾“å…¥")
    try:
        text_conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
                ],
            }
        ]
        
        text = processor.apply_chat_template(text_conversation, add_generation_prompt=True)
        print(f"  âœ… æ–‡æœ¬å¤„ç†æˆåŠŸï¼Œé•¿åº¦: {len(text)}")
        
    except Exception as e:
        print(f"  âŒ æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
    
    # æµ‹è¯•2: å›¾åƒè¾“å…¥
    print(f"\nğŸ–¼ï¸ æµ‹è¯•2: å›¾åƒè¾“å…¥")
    try:
        if os.path.exists(image_path):
            from PIL import Image
            image = Image.open(image_path)
            print(f"  âœ… å›¾åƒåŠ è½½æˆåŠŸ: {image.size}")
            
            image_conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚"}
                    ],
                }
            ]
            
            text = processor.apply_chat_template(image_conversation, add_generation_prompt=True)
            print(f"  âœ… å›¾åƒ+æ–‡æœ¬å¤„ç†æˆåŠŸï¼Œé•¿åº¦: {len(text)}")
            
        else:
            print(f"  âš ï¸ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            
    except Exception as e:
        print(f"  âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
    
    # æµ‹è¯•3: éŸ³é¢‘è¾“å…¥
    print(f"\nğŸµ æµ‹è¯•3: éŸ³é¢‘è¾“å…¥")
    try:
        if os.path.exists(audio_path):
            import librosa
            audio, sr = librosa.load(audio_path, sr=16000)
            print(f"  âœ… éŸ³é¢‘åŠ è½½æˆåŠŸ: é‡‡æ ·ç‡{sr}Hz, é•¿åº¦{len(audio)/sr:.2f}ç§’")
            
            audio_conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "audio", "audio": audio_path},
                        {"type": "text", "text": "è¯·æè¿°è¿™æ®µéŸ³é¢‘ã€‚"}
                    ],
                }
            ]
            
            text = processor.apply_chat_template(audio_conversation, add_generation_prompt=True)
            print(f"  âœ… éŸ³é¢‘+æ–‡æœ¬å¤„ç†æˆåŠŸï¼Œé•¿åº¦: {len(text)}")
            
        else:
            print(f"  âš ï¸ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            
    except Exception as e:
        print(f"  âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
    
    # æµ‹è¯•4: è§†é¢‘è¾“å…¥
    print(f"\nğŸ¬ æµ‹è¯•4: è§†é¢‘è¾“å…¥")
    try:
        if os.path.exists(video_path):
            video_conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "video", "video": video_path},
                        {"type": "text", "text": "è¯·æè¿°è¿™ä¸ªè§†é¢‘ã€‚"}
                    ],
                }
            ]
            
            text = processor.apply_chat_template(video_conversation, add_generation_prompt=True)
            print(f"  âœ… è§†é¢‘+æ–‡æœ¬å¤„ç†æˆåŠŸï¼Œé•¿åº¦: {len(text)}")
            
        else:
            print(f"  âš ï¸ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
    except Exception as e:
        print(f"  âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
    
    # æµ‹è¯•5: æ··åˆè¾“å…¥ï¼ˆå›¾åƒ+éŸ³é¢‘+æ–‡æœ¬ï¼‰
    print(f"\nğŸ”€ æµ‹è¯•5: æ··åˆè¾“å…¥ï¼ˆå›¾åƒ+éŸ³é¢‘+æ–‡æœ¬ï¼‰")
    try:
        if os.path.exists(image_path) and os.path.exists(audio_path):
            mixed_conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image_path},
                        {"type": "audio", "audio": audio_path},
                        {"type": "text", "text": "è¯·ç»“åˆå›¾åƒå’ŒéŸ³é¢‘å†…å®¹è¿›è¡Œæè¿°ã€‚"}
                    ],
                }
            ]
            
            text = processor.apply_chat_template(mixed_conversation, add_generation_prompt=True)
            print(f"  âœ… æ··åˆè¾“å…¥å¤„ç†æˆåŠŸï¼Œé•¿åº¦: {len(text)}")
            
        else:
            print(f"  âš ï¸ å›¾åƒæˆ–éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ··åˆè¾“å…¥æµ‹è¯•")
            
    except Exception as e:
        print(f"  âŒ æ··åˆè¾“å…¥å¤„ç†å¤±è´¥: {e}")

def test_audio_video_separation():
    """æµ‹è¯•éŸ³è½¨å’Œè§†é¢‘åˆ†ç¦»å¤„ç†"""
    print("ğŸš€ éŸ³è½¨å’Œè§†é¢‘åˆ†ç¦»å¤„ç†æµ‹è¯•")
    print("="*80)
    
    # è§†é¢‘æ–‡ä»¶è·¯å¾„
    video_path = "/home/caden/workplace/qwen2.5-Omni_inference/test_video.mp4"
    
    if not os.path.exists(video_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    print(f"ğŸ“ æµ‹è¯•è§†é¢‘: {video_path}")
    file_size = os.path.getsize(video_path) / (1024 * 1024)
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    
    # 1. æå–éŸ³é¢‘
    print(f"\n{'='*60}")
    has_audio, audio_path = extract_audio_from_video(video_path)
    
    # 2. æå–æœ€åä¸€å¸§å›¾åƒ
    print(f"\n{'='*60}")
    frame_success, image_path, last_frame = extract_last_frame_from_video(video_path)
    
    # 3. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    print(f"\n{'='*60}")
    model, processor = load_model()
    
    if model is None or processor is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return
    
    # 4. æµ‹è¯•processorèƒ½åŠ›
    test_processor_capabilities(processor, video_path, audio_path, image_path)
    
    # 5. æµ‹è¯•åˆ†ç¦»å¤„ç†çš„æ•ˆæœ
    print(f"\n{'='*60}")
    print("ğŸ§ª æµ‹è¯•åˆ†ç¦»å¤„ç†æ•ˆæœ")
    
    if has_audio and frame_success:
        print("âœ… æˆåŠŸæå–éŸ³é¢‘å’Œå›¾åƒ")
        
        # æµ‹è¯•éŸ³é¢‘+å›¾åƒ+æ–‡æœ¬çš„æ··åˆè¾“å…¥
        try:
            mixed_conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image_path},
                        {"type": "audio", "audio": audio_path},
                        {"type": "text", "text": "è¿™æ˜¯ä¸€ä¸ªæ•°å­¦æ•™å­¦è§†é¢‘çš„æœ€åä¸€å¸§å›¾åƒå’ŒéŸ³é¢‘ã€‚è¯·åˆ†æè¿™ä¸ªè§†é¢‘å¯èƒ½çš„å†…å®¹ã€‚"}
                    ],
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = processor.apply_chat_template(mixed_conversation, add_generation_prompt=True)
            print(f"âœ… æ··åˆè¾“å…¥æ¨¡æ¿åº”ç”¨æˆåŠŸ")
            print(f"ğŸ“ æ¨¡æ¿é•¿åº¦: {len(text)} å­—ç¬¦")
            
            # å¤„ç†è¾“å…¥
            inputs = processor(
                text=[text],
                images=None,  # è¿™é‡Œæˆ‘ä»¬åˆ†åˆ«å¤„ç†å›¾åƒå’ŒéŸ³é¢‘
                videos=None,  # ä¸ä½¿ç”¨è§†é¢‘è¾“å…¥
                padding=True,
                return_tensors="pt",
            )
            
            print(f"âœ… è¾“å…¥å¤„ç†æˆåŠŸ")
            print(f"ğŸ“Š è¾“å…¥å¼ é‡ä¿¡æ¯:")
            for key, value in inputs.items():
                if isinstance(value, torch.Tensor):
                    print(f"  - {key}: {value.shape}, {value.dtype}")
                else:
                    print(f"  - {key}: {type(value)}")
            
        except Exception as e:
            print(f"âŒ åˆ†ç¦»å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # æ¸…ç†
    print(f"\nğŸ§¹ æ¸…ç†èµ„æº...")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print_gpu_memory_usage("æ¸…ç†å")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print(f"  - éŸ³é¢‘æå–: {'âœ… æˆåŠŸ' if has_audio else 'âŒ å¤±è´¥'}")
    print(f"  - å›¾åƒæå–: {'âœ… æˆåŠŸ' if frame_success else 'âŒ å¤±è´¥'}")
    print(f"  - æ¨¡å‹åŠ è½½: {'âœ… æˆåŠŸ' if model is not None else 'âŒ å¤±è´¥'}")
    print(f"  - Processoræµ‹è¯•: {'âœ… å®Œæˆ' if model is not None else 'âŒ è·³è¿‡'}")

if __name__ == "__main__":
    test_audio_video_separation()
