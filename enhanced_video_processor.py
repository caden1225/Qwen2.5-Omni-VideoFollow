#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆè§†é¢‘å¤„ç†æ¨¡å—
æ”¯æŒéŸ³è½¨å’Œè§†é¢‘åˆ†ç¦»å¤„ç†ï¼Œæå–éŸ³é¢‘å’Œå…³é”®å¸§
"""

import torch
import gc
import os
import sys
import time
import json
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union
from dataclasses import dataclass
import librosa
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "qwen-omni-utils" / "src"))

from qwen_omni_utils import process_mm_info

@dataclass
class AudioVideoSeparationConfig:
    """éŸ³è§†é¢‘åˆ†ç¦»å¤„ç†é…ç½®"""
    # éŸ³é¢‘æå–é…ç½®
    extract_audio: bool = True              # æ˜¯å¦æå–éŸ³é¢‘
    audio_sample_rate: int = 16000         # éŸ³é¢‘é‡‡æ ·ç‡
    audio_format: str = "wav"              # éŸ³é¢‘æ ¼å¼
    audio_quality: str = "high"            # éŸ³é¢‘è´¨é‡ (low, medium, high)
    
    # å›¾åƒæå–é…ç½®
    extract_frames: bool = True             # æ˜¯å¦æå–å…³é”®å¸§
    frame_extraction_method: str = "last"  # å¸§æå–æ–¹æ³• (last, keyframes, uniform, custom)
    num_keyframes: int = 3                 # å…³é”®å¸§æ•°é‡
    frame_quality: int = 95                # JPEGè´¨é‡ (1-100)
    
    # è§†é¢‘å¤„ç†é…ç½®
    video_processing: bool = True           # æ˜¯å¦å¤„ç†è§†é¢‘
    video_compression: bool = True          # æ˜¯å¦å‹ç¼©è§†é¢‘
    video_quality: str = "medium"          # è§†é¢‘è´¨é‡ (low, medium, high)
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "./extracted_media"  # è¾“å‡ºç›®å½•
    save_intermediate: bool = True         # æ˜¯å¦ä¿å­˜ä¸­é—´æ–‡ä»¶
    cleanup_after_processing: bool = False # å¤„ç†åæ˜¯å¦æ¸…ç†ä¸­é—´æ–‡ä»¶

@dataclass
class VideoOptimizationConfig:
    """è§†é¢‘ä¼˜åŒ–é…ç½®ç±»"""
    # å¸§æ•°æ§åˆ¶
    nframes: int = 4                    # ç›´æ¥æŒ‡å®šå¸§æ•°
    fps: Optional[float] = None         # å¸§ç‡ï¼ˆä¸nframesäº’æ–¥ï¼‰
    min_frames: int = 2                 # æœ€å°å¸§æ•°
    max_frames: int = 16                # æœ€å¤§å¸§æ•°
    
    # åˆ†è¾¨ç‡æ§åˆ¶
    resized_height: int = 112           # ç›®æ ‡é«˜åº¦
    resized_width: int = 112            # ç›®æ ‡å®½åº¦
    
    # æ—¶é—´æ§åˆ¶
    video_start: float = 0.0            # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    video_end: Optional[float] = None   # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    
    # åƒç´ é™åˆ¶
    min_pixels: Optional[int] = None    # æœ€å°åƒç´ æ•°
    max_pixels: Optional[int] = None    # æœ€å¤§åƒç´ æ•°
    
    # å†…å­˜ä¼˜åŒ–
    use_half_precision: bool = True     # æ˜¯å¦ä½¿ç”¨float16
    enable_audio: bool = False          # æ˜¯å¦å¯ç”¨éŸ³é¢‘å¤„ç†

class EnhancedVideoProcessor:
    """å¢å¼ºç‰ˆè§†é¢‘å¤„ç†å™¨ï¼Œæ”¯æŒéŸ³è§†é¢‘åˆ†ç¦»"""
    
    def __init__(self, 
                 video_config: VideoOptimizationConfig,
                 separation_config: AudioVideoSeparationConfig):
        self.video_config = video_config
        self.separation_config = separation_config
        self.setup_environment()
        self.setup_output_directory()
    
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        # è®¾ç½®è§†é¢‘åƒç´ é™åˆ¶
        if self.video_config.max_pixels:
            os.environ['VIDEO_MAX_PIXELS'] = str(self.video_config.max_pixels)
        
        # è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    def setup_output_directory(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        output_dir = Path(self.separation_config.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (output_dir / "audio").mkdir(exist_ok=True)
        (output_dir / "frames").mkdir(exist_ok=True)
        (output_dir / "video").mkdir(exist_ok=True)
        
        print(f"ğŸ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {output_dir.absolute()}")
    
    def get_video_info(self, video_path: str) -> Dict[str, Any]:
        """è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯"""
        print(f"ğŸ“Š åˆ†æè§†é¢‘ä¿¡æ¯: {os.path.basename(video_path)}")
        
        try:
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise ValueError("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            
            # åŸºæœ¬ä¿¡æ¯
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            duration = total_frames / fps if fps > 0 else 0
            
            # æ£€æŸ¥éŸ³é¢‘è½¨é“
            has_audio = False
            try:
                audio, sr = librosa.load(video_path, sr=None)
                has_audio = True
                audio_duration = len(audio) / sr
            except:
                has_audio = False
                audio_duration = 0
            
            cap.release()
            
            info = {
                'file_path': video_path,
                'file_size_mb': os.path.getsize(video_path) / (1024 * 1024),
                'fps': fps,
                'total_frames': total_frames,
                'width': width,
                'height': height,
                'duration': duration,
                'has_audio': has_audio,
                'audio_duration': audio_duration,
                'resolution': f"{width}x{height}",
                'aspect_ratio': width / height if height > 0 else 0
            }
            
            print(f"  âœ… è§†é¢‘ä¿¡æ¯è·å–æˆåŠŸ:")
            print(f"    ğŸ“ åˆ†è¾¨ç‡: {info['resolution']}")
            print(f"    ğŸ¬ æ€»å¸§æ•°: {total_frames}")
            print(f"    â±ï¸ æ—¶é•¿: {duration:.2f}ç§’")
            print(f"    ğŸµ éŸ³é¢‘: {'æœ‰' if has_audio else 'æ— '}")
            if has_audio:
                print(f"    ğŸ”Š éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’")
            
            return info
            
        except Exception as e:
            print(f"âŒ è§†é¢‘ä¿¡æ¯è·å–å¤±è´¥: {e}")
            return {}
    
    def extract_audio_from_video(self, video_path: str) -> Tuple[bool, Optional[str], Optional[np.ndarray]]:
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
        if not self.separation_config.extract_audio:
            print("âš ï¸ éŸ³é¢‘æå–å·²ç¦ç”¨")
            return False, None, None
        
        print(f"ğŸµ æå–éŸ³é¢‘: {os.path.basename(video_path)}")
        
        try:
            # ä½¿ç”¨librosaæå–éŸ³é¢‘
            audio, sr = librosa.load(video_path, sr=self.separation_config.audio_sample_rate)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            video_name = Path(video_path).stem
            output_path = Path(self.separation_config.output_dir) / "audio" / f"{video_name}_audio.{self.separation_config.audio_format}"
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆä½¿ç”¨soundfileï¼Œå› ä¸ºlibrosa.outputå·²è¢«ç§»é™¤ï¼‰
            import soundfile as sf
            sf.write(str(output_path), audio, sr)
            
            print(f"  âœ… éŸ³é¢‘æå–æˆåŠŸ:")
            print(f"    ğŸµ é‡‡æ ·ç‡: {sr} Hz")
            print(f"    ğŸ”Š éŸ³é¢‘é•¿åº¦: {len(audio)/sr:.2f}ç§’")
            print(f"    ğŸ“Š éŸ³é¢‘å½¢çŠ¶: {audio.shape}")
            print(f"    ğŸ’¾ ä¿å­˜è·¯å¾„: {output_path}")
            
            return True, str(output_path), audio
            
        except Exception as e:
            print(f"  âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
            return False, None, None
    
    def extract_frames_from_video(self, video_path: str) -> Tuple[bool, List[str], List[np.ndarray]]:
        """ä»è§†é¢‘ä¸­æå–å…³é”®å¸§"""
        if not self.separation_config.extract_frames:
            print("âš ï¸ å¸§æå–å·²ç¦ç”¨")
            return False, [], []
        
        print(f"ğŸ–¼ï¸ æå–å…³é”®å¸§: {os.path.basename(video_path)}")
        
        try:
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise ValueError("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            if total_frames == 0:
                raise ValueError("è§†é¢‘æ²¡æœ‰æœ‰æ•ˆå¸§")
            
            extracted_frames = []
            frame_paths = []
            video_name = Path(video_path).stem
            
            if self.separation_config.frame_extraction_method == "last":
                # æå–æœ€åä¸€å¸§
                frame_indices = [total_frames - 1]
                
            elif self.separation_config.frame_extraction_method == "keyframes":
                # æå–å…³é”®å¸§ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
                num_frames = min(self.separation_config.num_keyframes, total_frames)
                frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
                
            elif self.separation_config.frame_extraction_method == "uniform":
                # å‡åŒ€æå–å¸§
                num_frames = min(self.separation_config.num_keyframes, total_frames)
                frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
                
            else:
                # è‡ªå®šä¹‰å¸§ç´¢å¼•
                frame_indices = [0, total_frames // 2, total_frames - 1]
            
            print(f"  ğŸ“Š æå–ç­–ç•¥: {self.separation_config.frame_extraction_method}")
            print(f"  ğŸ¯ ç›®æ ‡å¸§æ•°: {len(frame_indices)}")
            
            for i, frame_idx in enumerate(frame_indices):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                
                if ret:
                    # è½¬æ¢BGRåˆ°RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                    timestamp = frame_idx / fps if fps > 0 else 0
                    output_path = Path(self.separation_config.output_dir) / "frames" / f"{video_name}_frame_{i:02d}_{timestamp:.1f}s.jpg"
                    
                    # ä¿å­˜å›¾åƒ
                    cv2.imwrite(str(output_path), frame, 
                               [cv2.IMWRITE_JPEG_QUALITY, self.separation_config.frame_quality])
                    
                    extracted_frames.append(frame_rgb)
                    frame_paths.append(str(output_path))
                    
                    print(f"    âœ… å¸§ {i+1}: æ—¶é—´ {timestamp:.1f}s, ä¿å­˜åˆ° {output_path.name}")
                else:
                    print(f"    âš ï¸ å¸§ {i+1}: è¯»å–å¤±è´¥")
            
            cap.release()
            
            if extracted_frames:
                print(f"  ğŸ‰ å¸§æå–å®Œæˆ: {len(extracted_frames)}/{len(frame_indices)} å¸§æˆåŠŸ")
                return True, frame_paths, extracted_frames
            else:
                print(f"  âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•å¸§")
                return False, [], []
                
        except Exception as e:
            print(f"  âŒ å¸§æå–å¤±è´¥: {e}")
            return False, [], []
    
    def process_video_with_separation(self, video_path: str, conversation: list) -> Tuple[bool, Dict[str, Any], Dict[str, Any]]:
        """ä½¿ç”¨åˆ†ç¦»å¤„ç†æ–¹å¼å¤„ç†è§†é¢‘"""
        print(f"\nğŸ¬ å¼€å§‹åˆ†ç¦»å¤„ç†è§†é¢‘: {os.path.basename(video_path)}")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        try:
            # 1. è·å–è§†é¢‘ä¿¡æ¯
            video_info = self.get_video_info(video_path)
            if not video_info:
                return False, {}, {}
            
            # 2. æå–éŸ³é¢‘
            audio_success, audio_path, audio_data = self.extract_audio_from_video(video_path)
            
            # 3. æå–å…³é”®å¸§
            frames_success, frame_paths, frame_data = self.extract_frames_from_video(video_path)
            
            # 4. å¤„ç†è§†é¢‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
            video_tensor = None
            if hasattr(self.separation_config, 'video_processing') and self.separation_config.video_processing:
                print(f"\nğŸ¬ å¤„ç†è§†é¢‘å¼ é‡...")
                try:
                    # ä½¿ç”¨åŸæœ‰çš„è§†é¢‘å¤„ç†é€»è¾‘
                    from qwen_omni_utils import process_mm_info
                    
                    # åˆ›å»ºè§†é¢‘å¤„ç†å‚æ•°
                    video_params = self._get_video_params()
                    
                    # å¤„ç†è§†é¢‘
                    video_conversation = self._add_video_to_conversation(conversation, video_path, video_params)
                    
                    # ä½¿ç”¨qwen_omni_utilså¤„ç†
                    audios, images, videos, video_kwargs = process_mm_info(
                        video_conversation, 
                        use_audio_in_video=False, 
                        return_video_kwargs=True
                    )
                    
                    if videos and len(videos) > 0:
                        video_tensor = videos[0]
                        print(f"  âœ… è§†é¢‘å¼ é‡å¤„ç†æˆåŠŸ: {video_tensor.shape}")
                    else:
                        print(f"  âš ï¸ è§†é¢‘å¼ é‡å¤„ç†å¤±è´¥")
                        
                except Exception as e:
                    print(f"  âŒ è§†é¢‘å¼ é‡å¤„ç†å¤±è´¥: {e}")
            else:
                print(f"\nğŸ¬ è§†é¢‘å¤„ç†å·²ç¦ç”¨ï¼ˆé…ç½®ä¸­video_processing=Falseï¼‰")
            
            # 5. æ”¶é›†å¤„ç†ç»“æœ
            processing_time = time.time() - start_time
            
            results = {
                'video_info': video_info,
                'audio_extraction': {
                    'success': audio_success,
                    'path': audio_path,
                    'data_shape': audio_data.shape if audio_data is not None else None
                },
                'frame_extraction': {
                    'success': frames_success,
                    'paths': frame_paths,
                    'count': len(frame_data),
                    'data_shapes': [frame.shape for frame in frame_data] if frame_data else []
                },
                'video_processing': {
                    'success': video_tensor is not None,
                    'tensor_shape': list(video_tensor.shape) if video_tensor is not None else None
                },
                'processing_time': processing_time,
                'total_time': time.time() - start_time
            }
            
            # 6. æ‰“å°å¤„ç†æ€»ç»“
            self._print_processing_summary(results)
            
            return True, results, {'video_tensor': video_tensor, 'audio_data': audio_data, 'frame_data': frame_data}
            
        except Exception as e:
            print(f"âŒ åˆ†ç¦»å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, {}, {}
    
    def _get_video_params(self) -> Dict[str, Any]:
        """è·å–è§†é¢‘å¤„ç†å‚æ•°"""
        params = {}
        
        # å¸§æ•°æ§åˆ¶
        if self.video_config.nframes is not None:
            params['nframes'] = self.video_config.nframes
        elif self.video_config.fps is not None:
            params['fps'] = self.video_config.fps
            params['min_frames'] = self.video_config.min_frames
            params['max_frames'] = self.video_config.max_frames
        
        # åˆ†è¾¨ç‡æ§åˆ¶
        params['resized_height'] = self.video_config.resized_height
        params['resized_width'] = self.video_config.resized_width
        
        # æ—¶é—´æ§åˆ¶
        params['video_start'] = self.video_config.video_start
        params['video_end'] = self.video_config.video_end
        
        # åƒç´ é™åˆ¶
        if self.video_config.max_pixels:
            params['max_pixels'] = self.video_config.max_pixels
        
        return params
    
    def _add_video_to_conversation(self, conversation: list, video_path: str, video_params: Dict[str, Any]) -> list:
        """å°†è§†é¢‘æ·»åŠ åˆ°å¯¹è¯ä¸­"""
        # æ·±æ‹·è´å¯¹è¯
        video_conversation = []
        for turn in conversation:
            new_turn = {'role': turn['role'], 'content': []}
            for content in turn['content']:
                if content['type'] == 'video':
                    # æ›¿æ¢è§†é¢‘å†…å®¹
                    new_content = {'type': 'video', 'video': video_path, **video_params}
                    new_turn['content'].append(new_content)
                else:
                    new_turn['content'].append(content.copy())
            video_conversation.append(new_turn)
        
        return video_conversation
    
    def _print_processing_summary(self, results: Dict[str, Any]):
        """æ‰“å°å¤„ç†æ€»ç»“"""
        print(f"\nğŸ“Š åˆ†ç¦»å¤„ç†æ€»ç»“")
        print(f"{'='*60}")
        
        video_info = results['video_info']
        audio_extraction = results['audio_extraction']
        frame_extraction = results['frame_extraction']
        video_processing = results['video_processing']
        
        print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯:")
        print(f"  - æ–‡ä»¶å¤§å°: {video_info['file_size_mb']:.2f} MB")
        print(f"  - åˆ†è¾¨ç‡: {video_info['resolution']}")
        print(f"  - æ—¶é•¿: {video_info['duration']:.2f}ç§’")
        print(f"  - éŸ³é¢‘: {'æœ‰' if video_info['has_audio'] else 'æ— '}")
        
        print(f"\nğŸµ éŸ³é¢‘æå–:")
        print(f"  - çŠ¶æ€: {'âœ… æˆåŠŸ' if audio_extraction['success'] else 'âŒ å¤±è´¥'}")
        if audio_extraction['success']:
            print(f"  - è·¯å¾„: {audio_extraction['path']}")
            print(f"  - æ•°æ®å½¢çŠ¶: {audio_extraction['data_shape']}")
        
        print(f"\nğŸ–¼ï¸ å¸§æå–:")
        print(f"  - çŠ¶æ€: {'âœ… æˆåŠŸ' if frame_extraction['success'] else 'âŒ å¤±è´¥'}")
        if frame_extraction['success']:
            print(f"  - æå–å¸§æ•°: {frame_extraction['count']}")
            print(f"  - ä¿å­˜è·¯å¾„: {', '.join([Path(p).name for p in frame_extraction['paths']])}")
        
        print(f"\nğŸ¬ è§†é¢‘å¤„ç†:")
        print(f"  - çŠ¶æ€: {'âœ… æˆåŠŸ' if video_processing['success'] else 'âŒ å¤±è´¥'}")
        if video_processing['success']:
            print(f"  - å¼ é‡å½¢çŠ¶: {video_processing['tensor_shape']}")
        
        print(f"\nâ±ï¸ å¤„ç†æ—¶é—´:")
        print(f"  - åˆ†ç¦»å¤„ç†: {results['processing_time']:.2f}ç§’")
        print(f"  - æ€»è€—æ—¶: {results['total_time']:.2f}ç§’")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æ¸…ç†ä¸­é—´æ–‡ä»¶
        if self.separation_config.cleanup_after_processing:
            print("ğŸ§¹ æ¸…ç†ä¸­é—´æ–‡ä»¶...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘

class EnhancedVideoOptimizationPresets:
    """å¢å¼ºç‰ˆè§†é¢‘ä¼˜åŒ–é¢„è®¾é…ç½®"""
    
    @staticmethod
    def get_separation_preset(name: str) -> Tuple[VideoOptimizationConfig, AudioVideoSeparationConfig]:
        """è·å–åˆ†ç¦»å¤„ç†é¢„è®¾é…ç½®"""
        presets = {
            'audio_focus': (
                VideoOptimizationConfig(
                    nframes=2,
                    resized_height=112,
                    resized_width=112,
                    video_start=0.0,
                    video_end=2.0,
                    max_pixels=64 * 28 * 28,
                    use_half_precision=True,
                    enable_audio=False
                ),
                AudioVideoSeparationConfig(
                    extract_audio=True,
                    extract_frames=True,
                    frame_extraction_method="keyframes",
                    num_keyframes=2,
                    video_processing=False,  # ä¸å¤„ç†è§†é¢‘ï¼Œåªæå–éŸ³é¢‘å’Œå¸§
                    save_intermediate=True
                )
            ),
            
            'frame_focus': (
                VideoOptimizationConfig(
                    nframes=4,
                    resized_height=168,
                    resized_width=168,
                    video_start=0.0,
                    video_end=3.0,
                    max_pixels=128 * 28 * 28,
                    use_half_precision=True,
                    enable_audio=False
                ),
                AudioVideoSeparationConfig(
                    extract_audio=True,
                    extract_frames=True,
                    frame_extraction_method="uniform",
                    num_keyframes=5,
                    video_processing=True,
                    save_intermediate=True
                )
            ),
            
            'balanced_separation': (
                VideoOptimizationConfig(
                    nframes=6,
                    resized_height=168,
                    resized_width=168,
                    video_start=0.0,
                    video_end=4.0,
                    max_pixels=256 * 28 * 28,
                    use_half_precision=True,
                    enable_audio=False
                ),
                AudioVideoSeparationConfig(
                    extract_audio=True,
                    extract_frames=True,
                    frame_extraction_method="keyframes",
                    num_keyframes=3,
                    video_processing=True,
                    save_intermediate=True
                )
            ),
            
            'full_extraction': (
                VideoOptimizationConfig(
                    nframes=8,
                    resized_height=224,
                    resized_width=224,
                    video_start=0.0,
                    video_end=5.0,
                    max_pixels=512 * 28 * 28,
                    use_half_precision=True,
                    enable_audio=False
                ),
                AudioVideoSeparationConfig(
                    extract_audio=True,
                    extract_frames=True,
                    frame_extraction_method="uniform",
                    num_keyframes=8,
                    video_processing=True,
                    save_intermediate=True
                )
            )
        }
        
        if name not in presets:
            raise ValueError(f"æœªçŸ¥çš„é¢„è®¾é…ç½®: {name}")
        
        return presets[name]
    
    @staticmethod
    def list_separation_presets() -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ†ç¦»å¤„ç†é¢„è®¾"""
        return ['audio_focus', 'frame_focus', 'balanced_separation', 'full_extraction']

def load_config_from_file(config_path: str) -> Tuple[VideoOptimizationConfig, AudioVideoSeparationConfig]:
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        video_config = VideoOptimizationConfig(**config_data.get('video', {}))
        separation_config = AudioVideoSeparationConfig(**config_data.get('separation', {}))
        
        return video_config, separation_config
        
    except Exception as e:
        print(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤é…ç½®
        return VideoOptimizationConfig(), AudioVideoSeparationConfig()

def save_config_to_file(video_config: VideoOptimizationConfig, 
                        separation_config: AudioVideoSeparationConfig, 
                        config_path: str):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    try:
        config_data = {
            'video': {
                'nframes': video_config.nframes,
                'resized_height': video_config.resized_height,
                'resized_width': video_config.resized_width,
                'video_start': video_config.video_start,
                'video_end': video_config.video_end,
                'max_pixels': video_config.max_pixels,
                'use_half_precision': video_config.use_half_precision,
                'enable_audio': video_config.enable_audio
            },
            'separation': {
                'extract_audio': separation_config.extract_audio,
                'audio_sample_rate': separation_config.audio_sample_rate,
                'audio_format': separation_config.audio_format,
                'extract_frames': separation_config.extract_frames,
                'frame_extraction_method': separation_config.frame_extraction_method,
                'num_keyframes': separation_config.num_keyframes,
                'video_processing': separation_config.video_processing,
                'output_dir': separation_config.output_dir,
                'save_intermediate': separation_config.save_intermediate
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
        
    except Exception as e:
        print(f"âŒ é…ç½®ä¿å­˜å¤±è´¥: {e}")
