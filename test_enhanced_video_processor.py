#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¢å¼ºç‰ˆè§†é¢‘å¤„ç†æ¨¡å—
æµ‹è¯•éŸ³è½¨å’Œè§†é¢‘åˆ†ç¦»å¤„ç†åŠŸèƒ½
"""

import torch
import gc
import os
import sys
import time
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from enhanced_video_processor import (
    EnhancedVideoProcessor,
    VideoOptimizationConfig,
    AudioVideoSeparationConfig,
    EnhancedVideoOptimizationPresets,
    load_config_from_file,
    save_config_to_file
)

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, Qwen2_5OmniConfig

# ä»ç¯å¢ƒå˜é‡åŠ è½½æ¨¡å‹è·¯å¾„
MODEL_PATH = os.getenv('MODEL_PATH', "/home/caden/workplace/models/Qwen2.5-Omni-3B")

def print_gpu_memory_usage(stage=""):
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        print(f"[{stage}] GPUå†…å­˜ - å·²åˆ†é…: {allocated:.2f}GB, å·²é¢„ç•™: {reserved:.2f}GB")

def load_model():
    """åŠ è½½æ¨¡å‹"""
    print("=== åŠ è½½æ¨¡å‹ ===")
    
    try:
        config = Qwen2_5OmniConfig.from_pretrained(MODEL_PATH)
        config.enable_audio_output = False
        
        device_map = {
            "thinker.model": "cuda",
            "thinker.lm_head": "cuda",
            "thinker.visual": "cuda",
            "thinker.audio_tower": "cuda",
        }
        
        max_memory = {0: "8GB", "cpu": "16GB"}
        
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        print_gpu_memory_usage("åŠ è½½å‰")
        
        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            config=config,
            device_map=device_map,
            max_memory=max_memory,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        
        processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)
        
        print_gpu_memory_usage("æ¨¡å‹åŠ è½½å")
        print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_enhanced_video_processor(video_path: str, preset_name: str, model, processor):
    """æµ‹è¯•å¢å¼ºç‰ˆè§†é¢‘å¤„ç†å™¨"""
    print(f"\n{'='*60}")
    print(f"ğŸ¬ æµ‹è¯•å¢å¼ºç‰ˆè§†é¢‘å¤„ç†å™¨: {os.path.basename(video_path)}")
    print(f"âš™ï¸ ä½¿ç”¨é¢„è®¾: {preset_name}")
    print(f"{'='*60}")
    
    try:
        # è·å–é¢„è®¾é…ç½®
        video_config, separation_config = EnhancedVideoOptimizationPresets.get_separation_preset(preset_name)
        
        print(f"ğŸ“‹ è§†é¢‘é…ç½®:")
        print(f"  - å¸§æ•°: {video_config.nframes}")
        print(f"  - åˆ†è¾¨ç‡: {video_config.resized_width}x{video_config.resized_height}")
        print(f"  - æ—¶é—´èŒƒå›´: {video_config.video_start}s - {video_config.video_end}s")
        print(f"  - æœ€å¤§åƒç´ : {video_config.max_pixels:,}")
        print(f"  - åŠç²¾åº¦: {video_config.use_half_precision}")
        
        print(f"\nğŸ“‹ åˆ†ç¦»é…ç½®:")
        print(f"  - éŸ³é¢‘æå–: {separation_config.extract_audio}")
        print(f"  - å¸§æå–: {separation_config.extract_frames}")
        print(f"  - å¸§æå–æ–¹æ³•: {separation_config.frame_extraction_method}")
        print(f"  - å…³é”®å¸§æ•°é‡: {separation_config.num_keyframes}")
        print(f"  - è§†é¢‘å¤„ç†: {separation_config.video_processing}")
        print(f"  - è¾“å‡ºç›®å½•: {separation_config.output_dir}")
        
        # åˆ›å»ºå¢å¼ºç‰ˆå¤„ç†å™¨
        processor_enhanced = EnhancedVideoProcessor(video_config, separation_config)
        
        # åˆ›å»ºå¯¹è¯
        conversation = [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªè§†é¢‘åˆ†æåŠ©æ‰‹ï¼Œ è¯·æ ¹æ®å¾—åˆ°çš„ä¿¡æ¯å®Œæˆä»»åŠ¡ã€‚"}
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": video_path},
                    {"type": "text", "text": "å®Œæˆè§†é¢‘ä¸­çš„æŒ‡ä»¤ã€‚"},
                ],
            },
        ]
        
        # ä½¿ç”¨åˆ†ç¦»å¤„ç†æ–¹å¼å¤„ç†è§†é¢‘
        print(f"\nğŸ”„ å¼€å§‹åˆ†ç¦»å¤„ç†è§†é¢‘...")
        start_time = time.time()
        
        success, results, media_data = processor_enhanced.process_video_with_separation(video_path, conversation)
        
        if not success:
            print("âŒ è§†é¢‘åˆ†ç¦»å¤„ç†å¤±è´¥")
            return False, {}
        
        total_time = time.time() - start_time
        
        print(f"\nâœ… è§†é¢‘åˆ†ç¦»å¤„ç†æˆåŠŸï¼")
        print(f"ğŸ“Š å¤„ç†ç»“æœ:")
        print(f"  - éŸ³é¢‘æå–: {'âœ… æˆåŠŸ' if results['audio_extraction']['success'] else 'âŒ å¤±è´¥'}")
        print(f"  - å¸§æå–: {'âœ… æˆåŠŸ' if results['frame_extraction']['success'] else 'âŒ å¤±è´¥'}")
        print(f"  - è§†é¢‘å¤„ç†: {'âœ… æˆåŠŸ' if results['video_processing']['success'] else 'âŒ å¤±è´¥'}")
        print(f"  - åˆ†ç¦»å¤„ç†æ—¶é—´: {results['processing_time']:.2f}ç§’")
        print(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # å¦‚æœæˆåŠŸæå–äº†éŸ³é¢‘å’Œå¸§ï¼Œæµ‹è¯•æ··åˆè¾“å…¥
        if results['audio_extraction']['success'] and results['frame_extraction']['success']:
            print(f"\nğŸ§ª æµ‹è¯•å›¾åƒè¾“å…¥å¤„ç†...")
            
            try:
                # åˆ›å»ºå›¾åƒè¾“å…¥å¯¹è¯ï¼ˆä½¿ç”¨æœ€åä¸€å¸§å›¾ç‰‡ï¼‰
                last_frame_path = results['frame_extraction']['paths'][-1]  # ä½¿ç”¨æœ€åä¸€å¸§
                print(f"  ğŸ“¸ ä½¿ç”¨æœ€åä¸€å¸§å›¾ç‰‡: {os.path.basename(last_frame_path)}")
                
                image_conversation = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": last_frame_path},
                            {"type": "text", "text": "è¿™æ˜¯è§†é¢‘çš„æœ€åä¸€å¸§ï¼Œè¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"}
                        ],
                    }
                ]
                
                # åº”ç”¨èŠå¤©æ¨¡æ¿
                text = processor.apply_chat_template(image_conversation, add_generation_prompt=True)
                print(f"âœ… å›¾åƒè¾“å…¥æ¨¡æ¿åº”ç”¨æˆåŠŸï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
                
                # å¤„ç†è¾“å…¥
                try:
                    from PIL import Image
                    
                    # åŠ è½½æœ€åä¸€å¸§å›¾åƒ
                    image = Image.open(last_frame_path)
                    
                    # å¤„ç†è¾“å…¥
                    inputs = processor(
                        text=[text],
                        images=[image],
                        videos=None,
                        padding=True,
                        return_tensors="pt",
                    )
                    
                    print(f"âœ… å›¾åƒè¾“å…¥å¤„ç†æˆåŠŸ")
                    print(f"ğŸ“Š è¾“å…¥å¼ é‡ä¿¡æ¯:")
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            print(f"  - {key}: {value.shape}, {value.dtype}")
                        else:
                            print(f"  - {key}: {type(value)}")
                    
                    # å¦‚æœæœ‰æ¨¡å‹ï¼Œå°è¯•ç”Ÿæˆå›å¤
                    if model is not None:
                        print(f"\nğŸ”„ ç”ŸæˆAIå›å¤...")
                        generation_start = time.time()
                        
                        # ç¡®ä¿å¼ é‡ç±»å‹æ­£ç¡®
                        for key, value in inputs.items():
                            if isinstance(value, torch.Tensor):
                                if key in ["input_ids", "image_grid_thw", "video_grid_thw"]:
                                    inputs[key] = value.to(model.device).long()
                                elif "visual" in key:
                                    inputs[key] = value.to(model.device).to(model.dtype)
                                else:
                                    inputs[key] = value.to(model.device).to(model.dtype)
                        
                        with torch.no_grad():
                            generated_ids = model.generate(
                                **inputs,
                                max_new_tokens=256,
                                do_sample=False,
                                temperature=0.0,
                                top_p=1.0,
                            )
                        
                        generation_time = time.time() - generation_start
                        
                        # è§£ç è¾“å‡º
                        generated_ids = [
                            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
                        ]
                        response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                        
                        print(f"\nğŸ‰ æ¨ç†æˆåŠŸï¼")
                        print(f"ğŸ¤– AIå›å¤:")
                        print(f"{response}")
                        print(f"\nâ±ï¸ ç”Ÿæˆè€—æ—¶: {generation_time:.2f}ç§’")
                    
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒè¾“å…¥å¤„ç†å¤±è´¥: {e}")
                    print(f"ğŸ“ è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºprocessoréœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼")
                
            except Exception as e:
                print(f"âŒ å›¾åƒè¾“å…¥æµ‹è¯•å¤±è´¥: {e}")
                print(f"ğŸ“ è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºprocessoréœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼")
        
        print_gpu_memory_usage("åˆ†ç¦»å¤„ç†å")
        
        # æ¸…ç†
        processor_enhanced.cleanup()
        
        return True, results
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, {}

def test_all_separation_presets(video_path: str, model, processor):
    """æµ‹è¯•æ‰€æœ‰åˆ†ç¦»å¤„ç†é¢„è®¾é…ç½®"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª å…¨é¢æµ‹è¯•åˆ†ç¦»å¤„ç†é¢„è®¾: {os.path.basename(video_path)}")
    print(f"{'='*80}")
    
    # è·å–æ‰€æœ‰é¢„è®¾é…ç½®
    presets = EnhancedVideoOptimizationPresets.list_separation_presets()
    
    results = {}
    
    for preset_name in presets:
        print(f"\n{'='*60}")
        print(f"ğŸ¯ æµ‹è¯•é¢„è®¾: {preset_name}")
        print(f"{'='*60}")
        
        success, info = test_enhanced_video_processor(video_path, preset_name, model, processor)
        
        if success:
            results[preset_name] = {
                'success': True,
                'info': info
            }
            print(f"âœ… {preset_name} é¢„è®¾æµ‹è¯•æˆåŠŸ")
        else:
            results[preset_name] = {
                'success': False,
                'info': {}
            }
            print(f"âŒ {preset_name} é¢„è®¾æµ‹è¯•å¤±è´¥")
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        torch.cuda.empty_cache()
        print_gpu_memory_usage("æ¸…ç†å")
        
        # ç­‰å¾…ä¸€ä¸‹å†æµ‹è¯•ä¸‹ä¸€ä¸ª
        time.sleep(2)
    
    return results

def print_separation_summary_report(video_path: str, results: dict):
    """æ‰“å°åˆ†ç¦»å¤„ç†æµ‹è¯•æ€»ç»“æŠ¥å‘Š"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š åˆ†ç¦»å¤„ç†æµ‹è¯•æ€»ç»“æŠ¥å‘Š: {os.path.basename(video_path)}")
    print(f"{'='*80}")
    
    successful_presets = []
    failed_presets = []
    
    for preset_name, result in results.items():
        if result['success']:
            successful_presets.append(preset_name)
        else:
            failed_presets.append(preset_name)
    
    print(f"âœ… æˆåŠŸçš„é¢„è®¾é…ç½® ({len(successful_presets)}/{len(results)}):")
    for preset in successful_presets:
        info = results[preset]['info']
        print(f"  - {preset}:")
        print(f"    ğŸµ éŸ³é¢‘æå–: {'âœ…' if info['audio_extraction']['success'] else 'âŒ'}")
        print(f"    ğŸ–¼ï¸ å¸§æå–: {'âœ…' if info['frame_extraction']['success'] else 'âŒ'}")
        print(f"    ğŸ¬ è§†é¢‘å¤„ç†: {'âœ…' if info['video_processing']['success'] else 'âŒ'}")
        print(f"    â±ï¸ å¤„ç†æ—¶é—´: {info['processing_time']:.2f}ç§’")
    
    if failed_presets:
        print(f"\nâŒ å¤±è´¥çš„é¢„è®¾é…ç½® ({len(failed_presets)}):")
        for preset in failed_presets:
            print(f"  - {preset}")
    
    # æ¨èæœ€ä½³é…ç½®
    if successful_presets:
        best_preset = min(successful_presets, 
                         key=lambda x: results[x]['info']['processing_time'])
        best_info = results[best_preset]['info']
        print(f"\nğŸ† æ¨èé…ç½®: {best_preset}")
        print(f"  - å¤„ç†æ—¶é—´æœ€çŸ­: {best_info['processing_time']:.2f}ç§’")
        print(f"  - éŸ³é¢‘æå–: {'âœ…' if best_info['audio_extraction']['success'] else 'âŒ'}")
        print(f"  - å¸§æå–: {'âœ…' if best_info['frame_extraction']['success'] else 'âŒ'}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¢å¼ºç‰ˆè§†é¢‘å¤„ç†å™¨åˆ†ç¦»å¤„ç†æµ‹è¯•")
    print("="*80)
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    video_path = "./math.mp4"
    
    if not os.path.exists(video_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    file_size = os.path.getsize(video_path) / (1024 * 1024)
    print(f"ğŸ“ æµ‹è¯•è§†é¢‘: {video_path} ({file_size:.2f} MB)")
    
    try:
        # åŠ è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        print(f"\n{'='*60}")
        print("ğŸ”„ åŠ è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•æ··åˆè¾“å…¥ï¼‰")
        print(f"{'='*60}")
        
        model, processor = load_model()
        
        if model is None or processor is None:
            print("âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†è·³è¿‡æ··åˆè¾“å…¥æµ‹è¯•")
            model, processor = None, None
        
        # æµ‹è¯•æ‰€æœ‰åˆ†ç¦»å¤„ç†é¢„è®¾
        results = test_all_separation_presets(video_path, model, processor)
        
        # æ‰“å°æ€»ç»“æŠ¥å‘Š
        print_separation_summary_report(video_path, results)
        
        print(f"\nğŸ‰ åˆ†ç¦»å¤„ç†æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"  - æµ‹è¯•è§†é¢‘: {os.path.basename(video_path)}")
        print(f"  - å¯ç”¨é¢„è®¾é…ç½®: {len(EnhancedVideoOptimizationPresets.list_separation_presets())}")
        print(f"  - æˆåŠŸå®ç°éŸ³è½¨å’Œè§†é¢‘åˆ†ç¦»å¤„ç†ï¼")
        print(f"  - æå–çš„åª’ä½“æ–‡ä»¶ä¿å­˜åœ¨: ./extracted_media/ ç›®å½•")
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
